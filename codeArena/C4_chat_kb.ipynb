{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level approach\n",
    "- Parse messages, translate messages along with any replies into this format \"timestamp;author;message;reply_to_author;reply_to_message\"\n",
    "- Prompt LLM to generate questions and answers with timestamp and answer source author\n",
    "- To handle duplicates and use the latest:\n",
    "    - Add all the questions to the embeddings index\n",
    "    - For each quesion look for highly similar questions in the index, with threshold of 0.9\n",
    "    - For the similar questions, choose the one with the latest timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install beautifulsoup4 lxml\n",
    "%pip install matplotlib openai plotly pandas scipy scikit-learn python-dotenv langchain tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY') or getpass('Enter your OpenAI API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "formatted_msg_blocks = []\n",
    "\n",
    "with open('./Code4rena_-_Main_-_questions.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "message_groups = soup.find_all('div', class_='chatlog__message-group')\n",
    "\n",
    "parsed_messages = []\n",
    "\n",
    "for message_group in message_groups:\n",
    "    author = message_group.find('span', class_='chatlog__author')\n",
    "    if author:\n",
    "        author = author.text\n",
    "    else:\n",
    "        author = ''\n",
    "    timestamp = message_group.find('span', class_='chatlog__timestamp')\n",
    "    if timestamp:\n",
    "        timestamp = timestamp.text\n",
    "    else:\n",
    "        timestamp = ''\n",
    "    markdown_preserves = message_group.find_all('span', class_='chatlog__markdown-preserve')\n",
    "    message = '\\n'.join([mp.text for mp in markdown_preserves])\n",
    "    reply_to_author = message_group.find('div', class_='chatlog__reply-author')\n",
    "    if reply_to_author:\n",
    "        reply_to_author = reply_to_author.text\n",
    "    else:\n",
    "        reply_to_author = ''\n",
    "    reply_to_message = message_group.find('span', class_='chatlog__reply-link')\n",
    "    if reply_to_message:\n",
    "        reply_to_message = reply_to_message.text\n",
    "    else:\n",
    "        reply_to_message = ''\n",
    "    parsed_messages.append({'author': author, 'timestamp': timestamp, 'message': message, 'reply_to_author': reply_to_author, 'reply_to_message': reply_to_message})\n",
    "\n",
    "len(parsed_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the last N messages for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_messages = parsed_messages[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate message blocks using sliding window with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message_blocks = []\n",
    "step = 10\n",
    "lookout_range = 5\n",
    "for i in range(0, len(latest_messages), step):    \n",
    "    before = latest_messages[i-lookout_range:i]\n",
    "    main_block = latest_messages[i:i+step]\n",
    "    after = latest_messages[i+1:i+lookout_range]\n",
    "\n",
    "    message_block = []\n",
    "    message_block.extend(before)\n",
    "    message_block.extend(main_block)\n",
    "    message_block.extend(after)\n",
    "\n",
    "    message_blocks.append(message_block)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format messages for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_msg_blocks = []\n",
    "for mb in message_blocks:\n",
    "    lines = \"\"\n",
    "    \n",
    "    for m in mb:\n",
    "        timestamp = m['timestamp']\n",
    "        author = m['author']\n",
    "        message = m['message'].replace('\"', \"\").replace(\"'\", \"\")\n",
    "        reply_to_author = m['reply_to_author']\n",
    "        reply_to_message = m['reply_to_message'].replace('\"', \"\").replace(\"'\", \"\")\n",
    "        formatted_message = f\"{timestamp};{author};{message};{reply_to_author};{reply_to_message}\"\n",
    "        lines += formatted_message + \"---\"\n",
    "        \n",
    "    formatted_msg_blocks.append(lines)\n",
    "len(formatted_msg_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an intelligent analyst capable of looking at chat messages and generating questions and answers from it to create an FAQ.\n",
    "\n",
    "- For your task, you have been given chat messages from an organization called Code4rena (a.k.a C4) that specializes in crowd sourced smart contract audits.\n",
    "- You are given chat messages below, each message is formatted as timestamp;author;message;reply_to_author;reply_to_message and separated by \"---\"\n",
    "- To generate questions and answers, think step-by-step, first base it on the reply to author and reply to message, if they are not available, then solely based it on the messages before and after.\n",
    "- **DO NOT** use any follow-on questions as an answer to previous question.\n",
    "- If a message seems like a casual conversation and unrelated to the general subject, skip it.\n",
    "- If a question does not have a helpful answer, feel free to skip it.\n",
    "- Rephrase the questions and answers to be professional, suitable enough to be used in a FAQ.\n",
    "- Use the message timestamp from the author as the timestamp for the question and answer.\n",
    "- Do not mention any thing about the particular chat or author in the answer, it should be generic enough to be used in a FAQ.\n",
    "- Any links mentioned in the messages are very important, please include them in the answer.\n",
    "- Identify the true source author that contributed to the answer from the messages\n",
    "- Output the results as a JSON list with fields \"timestamp\", \"question\", \"answer\", \"answer_source_author\"\n",
    "- **DO NOT** make up questions and answers, only use the chat messages as the source of truth.\n",
    "\n",
    "## Eample:\n",
    "### Chat messages:\n",
    "06/28/2023 5:39 PM;DadeKuma;thats old, it doesnt work like that anymore;lsaudit; according to that .cvs file, Low issues are ranked by uniquess too ---06/28/2023 5:40 PM;lsaudit;so if all As get the same award, no matter how many Low findings there are - why should auditors bother to put more than one Low findins in QA?\\nif one Low finding is enough to be scored as A ?\\nOr maybe Ill rephrase my question. Lets assume that there are only three QA reports. 1st reports issues: A, B, C, D. 2nd: B, C, D, E; 3rd: F. Can only one report be choosen for a final report?\\nOr the report will merge: A, B, C, D, E, F. So 1st report will get bonus for A uniquness, and 3rd report, would get bonus for reporting F issue?;;---06/28/2023 5:47 PM;ðŸ¦™ liveactionllama | C4;The info here might be helpful:\\nhttps://docs.code4rena.com/awarding/judging-criteria#qa-reports-low-non-critical\\nhttps://docs.code4rena.com/awarding/incentive-model-and-awards#qa-and-gas-optimization-reports\\n\\nJudges look at both quantity and quality when judging QA reports. If a wardens QA submission only had 1 item, it would be pretty unlikely to receive a high grade. Especially if other wardens QA submissions within that audit contained many high quality items in comparison.;lsaudit; so if all As get the same award, no matter how many Low findings there are - why should auditors bother to put more than one Low findins in QA?\n",
    "\n",
    "### JSON result:\n",
    "{{\n",
    "\"timestamp\": \"06/28/2023 5:40 PM\",\n",
    "\"question\": \"Why should auditors bother to put more than one Low findings in QA if all As get the same award, no matter how many Low findings there are?\",\n",
    "\"answer\": \"Judges look at both quantity and quality when judging QA reports. If a warden's QA submission only had 1 item, it would be pretty unlikely to receive a high grade. Especially if other wardens' QA submissions within that audit contained many high-quality items in comparison. More information can be found at https://docs.code4rena.com/awarding/judging-criteria#qa-reports-low-non-critical and https://docs.code4rena.com/awarding/incentive-model-and-awards#qa-and-gas-optimization-reports.\",\n",
    "\"answer_source_author\": \"ðŸ¦™ liveactionllama | C4\",\n",
    "}}\n",
    "\n",
    "## Chat messages:\n",
    "{chat_messages}\n",
    "\n",
    "## JSON result:\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_messages\"],\n",
    "    template=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "NUM_MESSAGE_BLOCKS = 10\n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "qa_list = []\n",
    "total_tokens = 0\n",
    "total_cost = 0\n",
    "with get_openai_callback() as cb:\n",
    "    for l in formatted_msg_blocks[:NUM_MESSAGE_BLOCKS]:\n",
    "        result = chain.run(chat_messages=l)\n",
    "        print(result)\n",
    "        qa_list.extend(json.loads(result))\n",
    "        total_tokens += cb.total_tokens\n",
    "        total_cost += cb.total_cost\n",
    "\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Total cost: {total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Langchain Document objects from the resultant questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "docs = []\n",
    "for i, qa in enumerate(qa_list):\n",
    "    question = qa['question']\n",
    "    doc = Document(page_content=question, metadata={\n",
    "        'ques_id': i,\n",
    "        'timestamp': qa['timestamp'],\n",
    "        'epoch_time': int(datetime.strptime(qa['timestamp'], '%m/%d/%Y %I:%M %p').timestamp()),\n",
    "        'answer': qa['answer'],\n",
    "        'answer_source_author': qa['answer_source_author']\n",
    "    })\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Documents to the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"questions\"\n",
    "\n",
    "chroma = chromadb.Client()\n",
    "try:\n",
    "    collection = chroma.get_collection(collection_name)\n",
    "    if collection:\n",
    "        chroma.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ques_db = Chroma(collection_name=collection_name, embedding_function=embeddings)\n",
    "ques_db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for latest questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_question_ids = []\n",
    "final_qa_docs = []\n",
    "\n",
    "for d in docs:\n",
    "    q = d.page_content\n",
    "    ques_id = d.metadata['ques_id']\n",
    "\n",
    "    if ques_id in skip_question_ids:\n",
    "        continue\n",
    "\n",
    "    results = ques_db.similarity_search_with_relevance_scores(q, k=4, score_threshold=0.9)\n",
    "    latest_question = d\n",
    "    for r in results:\n",
    "        skip_question_ids.append(r[0].metadata['ques_id'])\n",
    "        ques_id = r[0].metadata['ques_id']\n",
    "        epoch_time = r[0].metadata['epoch_time']\n",
    "        if epoch_time > latest_question.metadata['epoch_time']:\n",
    "            latest_question = r[0]\n",
    "    final_qa_docs.append(latest_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create markdown file with the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_store = []\n",
    "for d in final_qa_docs:\n",
    "    qa_to_store.append({\n",
    "        'question': d.page_content,\n",
    "        'answer': d.metadata['answer'],\n",
    "        'timestamp': d.metadata['timestamp'],\n",
    "        'answer_source_author': d.metadata['answer_source_author']\n",
    "    })\n",
    "\n",
    "with open('./output/faq.json', 'w') as f:\n",
    "    json.dump(qa_to_store, f, indent=4)\n",
    "\n",
    "with open('./output/faq.md', 'w') as f:\n",
    "    for i, qa in enumerate(final_qa_docs):\n",
    "        question = qa.page_content\n",
    "        answer = qa.metadata['answer']\n",
    "        author = qa.metadata['answer_source_author']\n",
    "        timestamp = qa.metadata['timestamp']\n",
    "        f.write(f\"#### {i+1}. {question}\\n\")\n",
    "        f.write(f\"{answer}\\n\\n\")\n",
    "        f.write(f\"*Source Author: {author}*\\n\\n\")\n",
    "        f.write(f\"*Source Timestamp: {timestamp}*\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-cookie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
