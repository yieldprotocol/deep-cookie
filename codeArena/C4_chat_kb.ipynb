{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ from Code4rena's Discord chat messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "- The goal of this notebook is to create an FAQ from the chat messages in the Code4rena's (C4) Discord \"questions\" channel.\n",
    "- The purpose of this FAQ is to primarily allow C4 to identify gaps in documentation and accordingly update it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level approach\n",
    "- Parse chat messages in the HTML export into a structured format (timestamp, author, message, reply_to_author, reply_to_message)\n",
    "- Create overlapping groups of messages and add each group to the LLM prompt to generate questions and answers with timestamp and answer source author\n",
    "- To handle highly-similar questions and only keep the latest:\n",
    "    - Add all the questions to the embeddings index\n",
    "    - For each quesion look for highly similar questions in the index, with threshold of 0.9\n",
    "    - For the similar questions, choose the one with the latest timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (4.12.2)\n",
      "Requirement already satisfied: lxml in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (4.9.3)\n",
      "Requirement already satisfied: matplotlib in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: openai in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: plotly in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (5.17.0)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (2.1.1)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.1.2-cp310-cp310-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (1.11.2)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.11.3-cp310-cp310-macosx_10_9_x86_64.whl (37.3 MB)\n",
      "Requirement already satisfied: scikit-learn in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (1.3.1)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp310-cp310-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: python-dotenv in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (0.0.302)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.0.324-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tiktoken in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: chromadb in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (0.4.13)\n",
      "Collecting chromadb\n",
      "  Downloading chromadb-0.4.15-py3-none-any.whl (479 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m479.8/479.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (4.66.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (1.26.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: aiohttp in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Collecting langsmith<0.1.0,>=0.0.52\n",
      "  Downloading langsmith-0.0.52-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (0.6.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (2.0.21)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (2.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: anyio<4.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (3.7.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from tiktoken) (2023.8.8)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: chroma-hnswlib==0.7.3 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (4.8.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (1.16.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.13.3)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.20.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (3.3.0)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.103.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.23.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.20.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (1.58.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: importlib-resources in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (6.1.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from chromadb) (3.0.2)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.20.0-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (3.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from anyio<4.0->langchain) (1.1.3)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from anyio<4.0->langchain) (3.4)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.27.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.6.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting urllib3<2.0,>=1.24.2\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: flatbuffers in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (4.24.3)\n",
      "Requirement already satisfied: coloredlogs in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
      "Collecting opentelemetry-proto==1.20.0\n",
      "  Downloading opentelemetry_proto-1.20.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-common==1.20.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.20.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.60.0)\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
      "Collecting opentelemetry-semantic-conventions==0.41b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.41b0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.15.0-cp310-cp310-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
      "Installing collected packages: wrapt, urllib3, scipy, rsa, plotly, opentelemetry-semantic-conventions, opentelemetry-proto, oauthlib, cachetools, scikit-learn, pandas, opentelemetry-exporter-otlp-proto-common, google-auth, deprecated, requests-oauthlib, opentelemetry-api, langsmith, opentelemetry-sdk, langchain, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.5\n",
      "    Uninstalling urllib3-2.0.5:\n",
      "      Successfully uninstalled urllib3-2.0.5\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.2\n",
      "    Uninstalling scipy-1.11.2:\n",
      "      Successfully uninstalled scipy-1.11.2\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.17.0\n",
      "    Uninstalling plotly-5.17.0:\n",
      "      Successfully uninstalled plotly-5.17.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.1\n",
      "    Uninstalling scikit-learn-1.3.1:\n",
      "      Successfully uninstalled scikit-learn-1.3.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.1\n",
      "    Uninstalling pandas-2.1.1:\n",
      "      Successfully uninstalled pandas-2.1.1\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.40\n",
      "    Uninstalling langsmith-0.0.40:\n",
      "      Successfully uninstalled langsmith-0.0.40\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.0.302\n",
      "    Uninstalling langchain-0.0.302:\n",
      "      Successfully uninstalled langchain-0.0.302\n",
      "  Attempting uninstall: chromadb\n",
      "    Found existing installation: chromadb 0.4.13\n",
      "    Uninstalling chromadb-0.4.13:\n",
      "      Successfully uninstalled chromadb-0.4.13\n",
      "Successfully installed cachetools-5.3.2 chromadb-0.4.15 deprecated-1.2.14 google-auth-2.23.3 kubernetes-28.1.0 langchain-0.0.324 langsmith-0.0.52 oauthlib-3.2.2 opentelemetry-api-1.20.0 opentelemetry-exporter-otlp-proto-common-1.20.0 opentelemetry-exporter-otlp-proto-grpc-1.20.0 opentelemetry-proto-1.20.0 opentelemetry-sdk-1.20.0 opentelemetry-semantic-conventions-0.41b0 pandas-2.1.2 plotly-5.18.0 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.3.2 scipy-1.11.3 urllib3-1.26.18 wrapt-1.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U beautifulsoup4 lxml matplotlib openai plotly pandas scipy scikit-learn python-dotenv langchain tiktoken chromadb tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.environ.get('OPENAI_API_KEY') or getpass('Enter your OpenAI API key: ')\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "formatted_msg_blocks = []\n",
    "\n",
    "with open('./Code4rena_-_Main_-_questions.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse chat messages in the HTML export into a structured format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "message_groups = soup.find_all('div', class_='chatlog__message-group')\n",
    "\n",
    "parsed_messages = []\n",
    "\n",
    "for message_group in message_groups:\n",
    "    author = message_group.find('span', class_='chatlog__author')\n",
    "    if author:\n",
    "        author = author.text\n",
    "    else:\n",
    "        author = ''\n",
    "    timestamp = message_group.find('span', class_='chatlog__timestamp')\n",
    "    if timestamp:\n",
    "        timestamp = timestamp.text\n",
    "    else:\n",
    "        timestamp = ''\n",
    "    markdown_preserves = message_group.find_all('span', class_='chatlog__markdown-preserve')\n",
    "    message = '\\n'.join([mp.text for mp in markdown_preserves])\n",
    "    reply_to_author = message_group.find('div', class_='chatlog__reply-author')\n",
    "    if reply_to_author:\n",
    "        reply_to_author = reply_to_author.text\n",
    "    else:\n",
    "        reply_to_author = ''\n",
    "    reply_to_message = message_group.find('span', class_='chatlog__reply-link')\n",
    "    if reply_to_message:\n",
    "        reply_to_message = reply_to_message.text\n",
    "    else:\n",
    "        reply_to_message = ''\n",
    "    parsed_messages.append({'author': author, 'timestamp': timestamp, 'message': message, 'reply_to_author': reply_to_author, 'reply_to_message': reply_to_message})\n",
    "\n",
    "len(parsed_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the last N messages for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_MESSAGES = len(parsed_messages) # Change this limit to preferred last N messages\n",
    "\n",
    "latest_messages = parsed_messages[-NUM_MESSAGES:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate message blocks/groups using sliding window with overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message_blocks = []\n",
    "step = 10\n",
    "lookout_range = 3\n",
    "for i in range(0, len(latest_messages), step):    \n",
    "    before = latest_messages[i-lookout_range:i]\n",
    "    main_block = latest_messages[i:i+step]\n",
    "    after = latest_messages[i+1:i+lookout_range]\n",
    "\n",
    "    message_block = []\n",
    "    message_block.extend(before)\n",
    "    message_block.extend(main_block)\n",
    "    message_block.extend(after)\n",
    "\n",
    "    message_blocks.append(message_block)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format messages for prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_msg_blocks = []\n",
    "for mb in message_blocks:\n",
    "    lines = \"\"\n",
    "    \n",
    "    for m in mb:\n",
    "        timestamp = m['timestamp']\n",
    "        author = m['author']\n",
    "        message = m['message'].replace('\"', \"\").replace(\"'\", \"\")\n",
    "        reply_to_author = m['reply_to_author']\n",
    "        reply_to_message = m['reply_to_message'].replace('\"', \"\").replace(\"'\", \"\")\n",
    "        formatted_message = f\"{timestamp};{author};{message};{reply_to_author};{reply_to_message}\"\n",
    "        lines += formatted_message + \"---\"\n",
    "        \n",
    "    formatted_msg_blocks.append(lines)\n",
    "len(formatted_msg_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the prompt with detailed instructions and a single example (one-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an intelligent analyst capable of looking at chat messages and generating questions and answers from it to create an FAQ.\n",
    "\n",
    "- For your task, you have been given chat messages from an organization called Code4rena (a.k.a C4) that specializes in crowd sourced smart contract audits.\n",
    "- You are given chat messages below, each message is formatted as timestamp;author;message;reply_to_author;reply_to_message and separated by \"---\"\n",
    "- To generate questions and answers, think step-by-step, first base it on the reply to author and reply to message, if they are not available, then solely based it on the messages before and after.\n",
    "- **DO NOT** use any follow-on questions as an answer to previous question.\n",
    "- If a message seems like a casual conversation and unrelated to the general subject, skip it.\n",
    "- If a question does not have a helpful answer, feel free to skip it.\n",
    "- Rephrase the questions and answers to be professional, suitable enough to be used in a FAQ.\n",
    "- Use the message timestamp from the author as the timestamp for the question and answer.\n",
    "- Do not mention any thing about the particular chat or author in the answer, it should be generic enough to be used in a FAQ.\n",
    "- Any links mentioned in the messages are very important, please include them in the answer.\n",
    "- Identify the true source author that contributed to the answer from the messages\n",
    "- Output the results as a JSON list with fields \"timestamp\", \"question\", \"answer\", \"answer_source_author\"\n",
    "- **DO NOT** make up questions and answers, only use the chat messages as the source of truth.\n",
    "\n",
    "## Eample:\n",
    "### Chat messages:\n",
    "06/28/2023 5:39 PM;DadeKuma;thats old, it doesnt work like that anymore;lsaudit; according to that .cvs file, Low issues are ranked by uniquess too ---06/28/2023 5:40 PM;lsaudit;so if all As get the same award, no matter how many Low findings there are - why should auditors bother to put more than one Low findins in QA?\\nif one Low finding is enough to be scored as A ?\\nOr maybe Ill rephrase my question. Lets assume that there are only three QA reports. 1st reports issues: A, B, C, D. 2nd: B, C, D, E; 3rd: F. Can only one report be choosen for a final report?\\nOr the report will merge: A, B, C, D, E, F. So 1st report will get bonus for A uniquness, and 3rd report, would get bonus for reporting F issue?;;---06/28/2023 5:47 PM;🦙 liveactionllama | C4;The info here might be helpful:\\nhttps://docs.code4rena.com/awarding/judging-criteria#qa-reports-low-non-critical\\nhttps://docs.code4rena.com/awarding/incentive-model-and-awards#qa-and-gas-optimization-reports\\n\\nJudges look at both quantity and quality when judging QA reports. If a wardens QA submission only had 1 item, it would be pretty unlikely to receive a high grade. Especially if other wardens QA submissions within that audit contained many high quality items in comparison.;lsaudit; so if all As get the same award, no matter how many Low findings there are - why should auditors bother to put more than one Low findins in QA?\n",
    "\n",
    "### JSON result:\n",
    "{{\n",
    "\"timestamp\": \"06/28/2023 5:40 PM\",\n",
    "\"question\": \"Why should auditors put more than one Low findings in QA if all As get the same award, no matter how many Low findings there are?\",\n",
    "\"answer\": \"Judges look at both quantity and quality when judging QA reports. If a warden's QA submission only had 1 item, it would be pretty unlikely to receive a high grade. Especially if other wardens' QA submissions within that audit contained many high-quality items in comparison. More information can be found at https://docs.code4rena.com/awarding/judging-criteria#qa-reports-low-non-critical and https://docs.code4rena.com/awarding/incentive-model-and-awards#qa-and-gas-optimization-reports.\",\n",
    "\"answer_source_author\": \"🦙 liveactionllama | C4\"\n",
    "}}\n",
    "\n",
    "## Chat messages:\n",
    "{chat_messages}\n",
    "\n",
    "## JSON result:\"\"\"\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_messages\"],\n",
    "    template=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the LLM for each message block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_RESULTS_DIR = './output/llm_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02/01/2022 10:00 PM;0xleastwood;to account for the increase in judging fee\\nit was a beast of a contest, so there was a lot for the judge to handle;wildmolasses; hi! why was the malt prize pool changed? ---02/02/2022 12:53 AM;wildmolasses;gotcha, did i miss an announcement?;0xleastwood; it was a beast of a contest, so there was a lot for the judge to handle ---02/02/2022 12:53 AM;0xleastwood;I think this was raised a couple of weeks ago\\nnot sure if there was an announcement;wildmolasses; gotcha, did i miss an announcement? ---02/02/2022 12:55 AM;🧦 sockdrawer | C4;Discussed in the wardens channel. Our backlog came to a head based on overwhelming levels of issues on some contests and limited judge availability and we needed to ramp up our offers for judging comp for a window of time in order to just clear out the seriously lagging contests in the backlog\\nThankfully we had some judges do some SERIOUS overtime and we’re starting to get caught up. The recent process improvements and tool enhancements we’ve made should help us get ahead and stay ahead, too \\nFurther context for those curious: Some contests this fall (including malt) have had 4-8x as many issues as judges were handling this summer for roughly the same take. Our judges have been heroic the last few months in the way they’ve met this challenge.;wildmolasses; gotcha, did i miss an announcement? ---02/02/2022 1:27 AM;wildmolasses;thanks @🧦 sockdrawer | C4!;;---02/02/2022 5:06 AM;AlekseyNB🤖;Hello  people✌️  ;;---02/02/2022 9:13 AM;SaiKyatPauk;hello sir;;---02/02/2022 5:48 PM;Papo;hi all;;---02/02/2022 8:18 PM;Pedroais;When will the new submissions mechanism be implemented ?\\nIs it already active for the contest starting today ?;;---02/02/2022 8:21 PM;Dravee;From my understanding: yes, the new contests that are starting will be that way\\nQuick question: I saw that GitHub was down for a little in the past hour (server error). Do you use a queue system for submissions? While I did receive the mails, Im not so sure what happens when GitHub is down;Pedroais; When will the new submissions mechanism be implemented ? ---02/02/2022 8:30 PM;dzhawsh;I was wondering the same -- well, not whether theres a queue (theres not), but what would happen. Its a POST to GitHub to create an issue.. I can check a repo, but not 100% sure what to look for;Dravee; Quick question: I saw that GitHub was down for a little in the past hour (server error). Do you use a queue system for submissions? While I did receive the mails, Im not so sure what happens when GitHub is down ---02/02/2022 8:34 PM;🧦 sockdrawer | C4;I think in cases when GitHub has failed to take in issues in the past, it has actually rejected submissions via the API for a window of time, which does bubble up as a failed submission (or has in the past at least)\\nTo @dzhawshs point, it would be good if anyone has something they submitted during the window of the GitHub outage that maybe you could DM him and he could check whether we received or not;;---02/02/2022 8:45 PM;Dravee;FYI, I submitted 2 findings around 3 minutes before and 3 minutes after a GitHub server-error page. Seems like theres no impact;;---02/02/2022 1:27 AM;wildmolasses;thanks @🧦 sockdrawer | C4!;;---02/02/2022 5:06 AM;AlekseyNB🤖;Hello  people✌️  ;;---'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_msg_blocks[73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for blocks 74 to 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/567 [01:08<5:24:20, 34.44s/it]"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from tqdm import tqdm\n",
    "\n",
    "START_BLOCK_INDEX = 74 # start from preferred block\n",
    "END_BLOCK_INDEX = len(formatted_msg_blocks) - 1 # end at preferred block\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "total_tokens = 0\n",
    "total_cost = 0\n",
    "last_processed_block_index = 0\n",
    "\n",
    "if not os.path.exists(LLM_RESULTS_DIR):\n",
    "    os.makedirs(LLM_RESULTS_DIR)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    print(f\"Running for blocks {START_BLOCK_INDEX} to {END_BLOCK_INDEX}\")\n",
    "    for i in tqdm(range(START_BLOCK_INDEX, END_BLOCK_INDEX + 1)):\n",
    "        block = formatted_msg_blocks[i]\n",
    "        result = chain.run(chat_messages=block)\n",
    "        #print(result)\n",
    "        json_obj = json.loads(result)\n",
    "        if isinstance(json_obj, dict):\n",
    "            json_obj = [json_obj]\n",
    "        total_tokens += cb.total_tokens\n",
    "        total_cost += cb.total_cost\n",
    "        last_processed_block_index = i\n",
    "        with open(f'{LLM_RESULTS_DIR}/faq-{i}.json', 'w') as f:\n",
    "            json.dump(json_obj, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last processed message block index: 72\n",
      "Total tokens: 6005212\n",
      "Total cost: 206.23014\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last processed message block index: {last_processed_block_index}\")\n",
    "print(f\"Total tokens: {total_tokens}\")\n",
    "print(f\"Total cost: {total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the LLM result JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'timestamp': '02/16/2021 9:26 PM', 'question': 'Should we create a page for the contest and list or link to wardens, judges, and sponsors? Should we also have a form for people to fill out when joining as a warden, including links to their socials, bio, avi, etc.?', 'answer': 'Yes, these are all good ideas.', 'answer_source_author': 'zscole'}, {'timestamp': '02/16/2021 9:27 PM', 'question': 'Should we start a channel specific to the website?', 'answer': 'Yes, we can add a channel here. Also, feel free to submit PRs with any ideas to the GitHub. The website was put together quickly and any help is appreciated.', 'answer_source_author': 'zscole'}, {'timestamp': '02/16/2021 11:17 PM', 'question': 'When will we have access to the codebase?', 'answer': 'Access to the codebase will be available on February 17 @ 1400 UTC (9AM EST), which is a little less than 13 hours from now.', 'answer_source_author': 'zscole'}, {'timestamp': '02/17/2021 3:32 PM', 'question': 'Are the smart contracts from the real world (i.e., will they be used in practice) or are they only made for the purpose of this competition?', 'answer': 'Yes, they are real smart contracts that will be deployed after being audited. There are no fake world contracts here.', 'answer_source_author': 'zscole'}, {'timestamp': '02/16/2021 9:26 PM', 'question': 'Should we create a page for the contest and list or link to wardens, judges, and sponsors? Should we also have a form for people to fill out when joining as a warden, including links to their socials, bio, avi, etc.?', 'answer': 'Yes, these are all good ideas.', 'answer_source_author': 'zscole'}, {'timestamp': '02/16/2021 9:27 PM', 'question': 'Should we start a channel specific to the website for more detailed discussions?', 'answer': 'Yes, we can add a channel here. Also, feel free to submit PRs with any ideas to the GitHub. We just put the website together really quickly.', 'answer_source_author': 'zscole'}, {'timestamp': '02/16/2021 11:17 PM', 'question': 'When will we have access to the codebase?', 'answer': 'Access to the codebase will be available on February 17 @ 1400 UTC (9AM EST), which is a little less than 13 hours from now.', 'answer_source_author': 'zscole'}, {'timestamp': '02/17/2021 3:32 PM', 'question': 'Are the smart contracts used in the competition from the real world (i.e., will they be used in practice) or are they only made for the purpose of this competition?', 'answer': 'Yes, they are real smart contracts that will be deployed after being audited. There are no fake world contracts here.', 'answer_source_author': 'zscole'}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "json_files = [f for f in os.listdir(LLM_RESULTS_DIR)]\n",
    "\n",
    "qa_list = []\n",
    "\n",
    "for file in json_files:\n",
    "    with open(os.path.join(LLM_RESULTS_DIR, file), 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        qa_list.extend(json_data)\n",
    "\n",
    "print(qa_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Langchain Document objects from the resultant questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "docs = []\n",
    "for i, qa in enumerate(qa_list):\n",
    "    question = qa['question']\n",
    "    doc = Document(page_content=question, metadata={\n",
    "        'ques_id': i,\n",
    "        'timestamp': qa['timestamp'],\n",
    "        'epoch_time': int(datetime.strptime(qa['timestamp'], '%m/%d/%Y %I:%M %p').timestamp()),\n",
    "        'answer': qa['answer'],\n",
    "        'answer_source_author': qa['answer_source_author']\n",
    "    })\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Documents to the vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2ceb3632-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb36dc-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb3722-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb3754-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb3786-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb37b8-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb37ea-744d-11ee-a814-367dda1ae1c5',\n",
       " '2ceb381c-744d-11ee-a814-367dda1ae1c5']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"questions\"\n",
    "\n",
    "chroma = chromadb.Client()\n",
    "try:\n",
    "    collection = chroma.get_collection(collection_name)\n",
    "    if collection:\n",
    "        chroma.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "ques_db = Chroma(collection_name=collection_name, embedding_function=embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "ques_db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for latest questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sagarshah/.pyenv/versions/3.10.13/envs/deep-cookie/lib/python3.10/site-packages/langchain/schema/vectorstore.py:257: UserWarning: Relevance scores must be between 0 and 1, got [(Document(page_content='Should we start a channel specific to the website?', metadata={'answer': 'Yes, we can add a channel here. Also, feel free to submit PRs with any ideas to the GitHub. The website was put together quickly and any help is appreciated.', 'answer_source_author': 'zscole', 'epoch_time': 1613532420, 'ques_id': 1, 'timestamp': '02/16/2021 9:27 PM'}), 1.0000011920928955), (Document(page_content='Should we start a channel specific to the website for more detailed discussions?', metadata={'answer': 'Yes, we can add a channel here. Also, feel free to submit PRs with any ideas to the GitHub. We just put the website together really quickly.', 'answer_source_author': 'zscole', 'epoch_time': 1613532420, 'ques_id': 5, 'timestamp': '02/16/2021 9:27 PM'}), 0.9740092158317566), (Document(page_content='Should we create a page for the contest and list or link to wardens, judges, and sponsors? Should we also have a form for people to fill out when joining as a warden, including links to their socials, bio, avi, etc.?', metadata={'answer': 'Yes, these are all good ideas.', 'answer_source_author': 'zscole', 'epoch_time': 1613532360, 'ques_id': 4, 'timestamp': '02/16/2021 9:26 PM'}), 0.8140446543693542), (Document(page_content='Should we create a page for the contest and list or link to wardens, judges, and sponsors? Should we also have a form for people to fill out when joining as a warden, including links to their socials, bio, avi, etc.?', metadata={'answer': 'Yes, these are all good ideas.', 'answer_source_author': 'zscole', 'epoch_time': 1613532360, 'ques_id': 0, 'timestamp': '02/16/2021 9:26 PM'}), 0.8140219449996948)]\n",
      "  query: input text\n"
     ]
    }
   ],
   "source": [
    "skip_question_ids = []\n",
    "final_qa_docs = []\n",
    "\n",
    "for d in docs:\n",
    "    q = d.page_content\n",
    "    ques_id = d.metadata['ques_id']\n",
    "\n",
    "    if ques_id in skip_question_ids:\n",
    "        continue\n",
    "\n",
    "    results = ques_db.similarity_search_with_relevance_scores(q, k=4, score_threshold=0.9, )\n",
    "    latest_question = d\n",
    "    for r in results:\n",
    "        skip_question_ids.append(r[0].metadata['ques_id'])\n",
    "        ques_id = r[0].metadata['ques_id']\n",
    "        epoch_time = r[0].metadata['epoch_time']\n",
    "        if epoch_time > latest_question.metadata['epoch_time']:\n",
    "            latest_question = r[0]\n",
    "    final_qa_docs.append(latest_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create markdown file with the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_store = []\n",
    "for d in final_qa_docs:\n",
    "    qa_to_store.append({\n",
    "        'question': d.page_content,\n",
    "        'answer': d.metadata['answer'],\n",
    "        'timestamp': d.metadata['timestamp'],\n",
    "        'answer_source_author': d.metadata['answer_source_author']\n",
    "    })\n",
    "\n",
    "with open('./output/faq.json', 'w') as f:\n",
    "    json.dump(qa_to_store, f, indent=4)\n",
    "\n",
    "with open('./output/faq.md', 'w') as f:\n",
    "    for i, qa in enumerate(final_qa_docs):\n",
    "        question = qa.page_content\n",
    "        answer = qa.metadata['answer']\n",
    "        author = qa.metadata['answer_source_author']\n",
    "        timestamp = qa.metadata['timestamp']\n",
    "        f.write(f\"#### {i+1}. {question}\\n\")\n",
    "        f.write(f\"{answer}\\n\\n\")\n",
    "        f.write(f\"*Answer Source Author: {author}*\\n\\n\")\n",
    "        f.write(f\"*Source Timestamp: {timestamp}*\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-cookie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
