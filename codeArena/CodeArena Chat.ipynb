{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44354e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code4rena_-_Main_-_questions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d03702d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: lxml in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (4.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from beautifulsoup4) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82826cd2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (1.7.4)\n",
      "Requirement already satisfied: matplotlib in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: openai in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (0.28.0)\n",
      "Requirement already satisfied: plotly in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (5.17.0)\n",
      "Requirement already satisfied: pandas in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (2.1.1)\n",
      "Requirement already satisfied: scipy in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (1.11.2)\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/db/0d/1f6d2cd52c886707b00ddb7ed2504cbf10903a60a7bebcd71f0f77d53505/scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.1.1 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Downloading scikit_learn-1.3.1-cp311-cp311-macosx_12_0_arm64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.3.1 threadpoolctl-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu matplotlib openai plotly pandas scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5576b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from getpass import getpass\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bace3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import faiss  \n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71255862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "# setting up an OpenAI template on the run\n",
    "OPENAI_API_KEY = getpass()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b28588e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2ad189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada636ef",
   "metadata": {},
   "source": [
    "### Load the Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae84613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "chat_lines = []\n",
    "\n",
    "# Load the HTML content from the file\n",
    "with open('Code4rena_-_Main_-_questions.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "\n",
    "# Parse the HTML using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3b7b272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = []\n",
    "\n",
    "# Loop through each chat message block\n",
    "for message_block in soup.find_all('div', class_='chatlog__message-group'):\n",
    "    username_elem = message_block.find('span', class_='chatlog__author')\n",
    "    reply_to = message_block.find('span', class_='chatlog__author')\n",
    "    message_elem = message_block.find('div', class_='chatlog__content chatlog__markdown')\n",
    "\n",
    "    # If both username and message elements are found, append to the chat_data\n",
    "    if username_elem and message_elem:\n",
    "        username = username_elem.get_text().strip()\n",
    "        message = message_elem.get_text().strip()\n",
    "        chat_data.append((username, message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d93a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_data = []\n",
    "# Loop through each chat message block\n",
    "for message_block in soup.find_all('div', class_='chatlog__message-group'):\n",
    "    username_elem = message_block.find('span', class_='chatlog__author')\n",
    "    message_elem = message_block.find('div', class_='chatlog__content chatlog__markdown')\n",
    "    reply_elem = message_block.find('div', class_='chatlog__reply-content')\n",
    "\n",
    "    # If both username and message elements are found, append to the chat_data\n",
    "    if username_elem and message_elem:\n",
    "        username = username_elem.get_text().strip()\n",
    "        message = message_elem.get_text().strip()\n",
    "\n",
    "        # If the message has a reply, prepend it to the primary message\n",
    "        if reply_elem:\n",
    "            reply_content = reply_elem.get_text().strip()\n",
    "            message = f\"(Reply to: {reply_content}) {message}\"\n",
    "        \n",
    "        chat_data.append((username, message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "42b80752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6372"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "19a878ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Е sockdrawer | C4',\n",
       "  '(Reply to: Question for C4 team: Is there any reason not to release all the unverified submissions a few days after contest ends, before judging?\\n\\nI ask because one of the best things about this process is learning from what others found, and itd be great to do that while protocol still fresh on my mind.) In the works. Several moving pieces involved here. https://forum.code4rena.com/t/rfc-certified-wardens-rulebook-scout-role-contest-qa-and-mitigation-review-services/123'),\n",
       " ('Dravee',\n",
       "  'Just to make it clear: are high risk findings still game if they are out of scope? Does it depend on the contest?'),\n",
       " ('Е sockdrawer | C4',\n",
       "  '(Reply to: Just to make it clear: are high risk findings still game if they are out of scope? Does it depend on the contest?) Definitely depends on the contest and the judge I think. Id say you should make a case to the judge in your submission if you think it should be considered.'),\n",
       " ('p_crypt0',\n",
       "  'Any useful resources/tips for a beginner? I would appreciate some direction if anyone is willing to give'),\n",
       " ('abhinavmir / evmsecurity.org',\n",
       "  \"Can't send a text on #chat - any clue what's up?\"),\n",
       " ('Е sockdrawer | C4',\n",
       "  \"(Reply to: Can't send a text on #chat - any clue what's up?) The main chat is locked to just contributors at the moment to reduce spam and offtopic stuff. If youd like to join as a warden, you can do that in #i-want-to-be-a-warden\"),\n",
       " ('Е sockdrawer | C4',\n",
       "  '(Reply to: Any useful resources/tips for a beginner? I would appreciate some direction if anyone is willing to give) Check out @cmichels great post https://cmichel.io/how-to-become-a-smart-contract-auditor/'),\n",
       " ('abhinavmir / evmsecurity.org',\n",
       "  \"(Reply to: The main chat is locked to just contributors at the moment to reduce spam and offtopic stuff. If youd like to join as a warden, you can do that in #i-want-to-be-a-warden) Sent! I'll try and audit the upcoming JPEGD contracts. Seems fun! Also, great work @cmichel !\"),\n",
       " ('p_crypt0',\n",
       "  \"(Reply to: Check out @cmichels great post https://cmichel.io/how-to-become-a-smart-contract-auditor/) Thank you so much! I'm hoping to get started shortly and spend the next couple of weeks studying\"),\n",
       " ('M2-DEMOS',\n",
       "  'Hello!  I was curious to see if you and your team would like to have a quick chat about how Governor DAO Proof of Existence Token can solve your Sybil resistance issues you might have?'),\n",
       " ('Dravee',\n",
       "  \"Mmmhhh I can't prove that this is true anymore for keccak expressions on recent solidity versions: https://github.com/ethereum/solidity/issues/9232 . \\nIt's a gas optimization that's been existing for a while (and we can even see it on recently audited projects, like Axelar on solidity 0.8.9: // AUDIT: constants should be literal and their derivation should be in comments)\\nWhen did this optimization become obsolete? (edited)\"),\n",
       " ('TomFrenchBlockchain',\n",
       "  \"(Reply to: Mmmhhh I can't prove that this is true anymore for keccak expressions on recent solidity versions: https://github.com/ethereum/solidity/issues/9232 . \\nIt's a gas optimization that's been existing for a while (and we can even see it on recently audited projects, like Axelar on solidity 0.8.9: // AUDIT: constants should be literal and their derivation should be in comments)\\nWhen did this optimization become obsolete?  (edited)) Sorry, I think I'm responsible for this piece of misinformation entering the C4 hivemind and it's been amplified by wardens pulling findings from previous contest reports ever since.\\n\\nYou can see that this was fixed in 0.6.12 in this changelog: https://github.com/ethereum/solidity/blob/develop/Changelog.md#0612-2020-07-22\"),\n",
       " ('Dravee',\n",
       "  \"(Reply to: Realistically this finding should be rejected on all contests as it's very unlikely that someone will be using <0.6.12) Wow thanks for making it clear!\"),\n",
       " ('Kathleen_O',\n",
       "  'Hi everyone! Im Kathleen from IdleDAO (Idle.finance). Could someone please point me to a treasury manager?'),\n",
       " ('Е sockdrawer | C4',\n",
       "  '(Reply to: Hi everyone! Im Kathleen from IdleDAO (Idle.finance). Could someone please point me to a treasury manager?) @ eric (ninek) | C4 is probably the closest thing to that?'),\n",
       " ('100proof',\n",
       "  'Is the Total column on the Leaderboard the number of audits done by that particular person?'),\n",
       " ('Е sockdrawer | C4',\n",
       "  '(Reply to: Is the Total column on the Leaderboard the number of audits done by that particular person?) total number of valid findings of all severity levels'),\n",
       " ('100proof',\n",
       "  \"I see that High, Med, and Gas are titles. Is there any particular reason why Low is not there (seeing as you've got a Total column)?\"),\n",
       " ('Е sockdrawer | C4',\n",
       "  \"(Reply to: I see that High, Med, and Gas are titles. Is there any particular reason why Low is not there (seeing as you've got a Total column)?) low and non-critical issues are now grouped together as a single report by each warden.\"),\n",
       " ('100proof',\n",
       "  'That seems fair enough! Thank you so much for your prompt responses.')]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_data[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf73a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "- CodeArena deals with smart contracts that are intended to be used in real-world applications, not just for the sake of competition. \n",
    "- Smart contracts are set to be deployed after they are audited.\n",
    "- Teams do not intentionally hide issues in the smart contracts.\n",
    "- All smart contracts can be expected to have issues. \n",
    "- You can read how to disclose issues at  https://medium.com/@scott_lew_is/slingshot-finance-sponsors-20-000-usdc-guaranteed-distribution-bounty-pool-for-code-432n4s-first-789514a8dc99\n",
    "- CodeArena is involved in auditing real-world smart contracts, places importance on transparency and proper reporting, and has partnerships and guidelines to ensure a structured approach to their activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Smart contracts in the context of the discussion are from the \"real world\" and will be used in practice.\n",
    "These smart contracts will be deployed after being audited.\n",
    "The mentioned smart contracts are not just made for the purpose of competition.\n",
    "There is an expectation that the team should not hide issues in the smart contracts on purpose.\n",
    "There are guidelines on how to report issues related to these smart contracts.\n",
    "There is a medium article that provides more information about the subject.\n",
    "There's a GitHub link pointing to a submission policy for CodeArena contests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09b98885",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4146194856.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[92], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    - A leaderboard of the best contestants will be manually updated until a system is built to track it automatically.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Long Prompt:\n",
    "\n",
    "- A leaderboard of the best contestants will be manually updated until a system is built to track it automatically.\n",
    "- Participants can review and submit their findings on the last day and still be rewarded if they find a good exploit.\n",
    "- All submissions will be made available after the contest ends, once the possible exploits have been patched.\n",
    "- The focus of the contest is on the smart contracts, but suggestions for other relevant areas are open.\n",
    "- The submission policy allows for submissions up to 3 hours prior to the contest stop time.\n",
    "- A suggestion is made to allow submissions anytime prior to the contest end time and only accept the first or last entry from a person/team.\n",
    "- Documentation will be updated to reflect the suggestion regarding code submissions for proof-of-concepts (POCs).\n",
    "- Each POC is approximately 50 lines of code.\n",
    "- Github usernames will be added to the submissions.\n",
    "- There is a discussion on how to approach the potential misbehavior of the owner of the contract.\n",
    "- A trust model description is suggested to clarify the behavior and roles of the owner and other roles involved.\n",
    "- Mitigation measures can be created considering potential social engineering attacks on the owner.\n",
    "- A compromised or malicious owner is considered out-of-scope for the contest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827dc33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "- There is a suggestion to have a leaderboard of the best contestants after the results of the contest.\n",
    "- Participants can review and send their submissions on the last day and still be rewarded if they find a good exploit.\n",
    "- The smart contract review contest focuses on smart contracts, but other relevant suggestions are open.\n",
    "- The submission policy allows submissions up to 3 hours prior to the contest stop time, but the latest submission time is not mentioned.\n",
    "- A suggestion is made to allow submissions any time prior to the contest end time, with a policy to accept only the first or last entry from a person/team.\n",
    "- The suggestion to submit code that runs proof of concept (poc) for each bug can be done by adding a zip file or sharing a private GitHub repository.\n",
    "- There is a specific GitHub link provided for sharing vulnerability discovery poc: https://github.com/code-423n4/code-contests/tree/main/contests/01-slingshot#sharing-vulnerabilitydiscovery-poc\n",
    "- There is a discussion about considering the potential impact of misbehavior of the contract owner and the need for a trust model description for involved roles.\n",
    "- Mitigation measures can be created to address social engineering attacks on the contract owner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31915b",
   "metadata": {},
   "source": [
    "### Load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f761ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4-0613\")\n",
    "model.temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdecd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d426b6ba",
   "metadata": {},
   "source": [
    "### Construct the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f97556c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = '''\n",
    "We are building a knowledge base using unstructured information from a chat room. \n",
    "Specifically, we are using information from the questions channel of the Discord\n",
    "of CodeArena (C4), a company that performs audits of smart contracts. Your task \n",
    "is to review an excerpt from the chat and identify all information in the chat\n",
    "that gives information about CodeArena, it's activities, user questions and concerns, etc.\n",
    "\n",
    "Please carefully review the chat log and write whole sentences that carefully describe \n",
    "information you found in the chat. Please keep extracting information until you've extracted \n",
    "all the information from the chat. \n",
    "\n",
    "Chat:\n",
    "('Cheetah',\n",
    "  \"Are those smart contracts from the 'real world' (i.e. will be used in practice) or only made for the purpose of this competition?\"),\n",
    " ('shinobi',\n",
    "  '@Cheetah they are real smart contracts that will be deployed after being audited'),\n",
    " ('zscole', 'yes, they are smart contracts from the \"real world\" (edited)'),\n",
    " ('Cheetah',\n",
    "  'thanks for the clarification, so theoretically there should be no issues hidden by the team on purpose'),\n",
    " ('Matthias | ChainSecurity', 'Hehe, would be the first without issues'),\n",
    " ('shinobi', 'No, but there are guidelines on how to report those'),\n",
    " ('zscole', \"no, there shouldn't be.\"),\n",
    " ('Matthias | ChainSecurity', 'how to disclose issues?'),\n",
    " ('shinobi',\n",
    "  'You can read More https://medium.com/@scott_lew_is/slingshot-finance-sponsors-20-000-usdc-guaranteed-distribution-bounty-pool-for-code-432n4s-first-789514a8dc99'),\n",
    " ('zscole',\n",
    "  'https://github.com/code-423n4/code-contests/blob/main/SUBMISSION_POLICY.md'),\n",
    "\n",
    "Remember, please provide a bulleted list of facts from this chat. \n",
    "Do not mention or reference any chat usernames or individuals. \n",
    "Only extract factual information without adding additional context or interpretation. \n",
    "When a fact relates to a link, ALWAYS include the link. If possible, try to figure out \n",
    "a specific thing when a general noun is used (for example use \"blue finance contest\" rather \n",
    "than \"contest\" if the context supports it). \n",
    "'''\n",
    "\n",
    "assistant1 = '''\n",
    "- Smart contracts in the context of the discussion are from the \"real world\" and will be used in practice.\n",
    "- Smart contracts will be deployed after being audited.\n",
    "- Smart contracts are not just made for the purpose of competition.\n",
    "- There is an expectation that teams should not hide issues in the smart contracts on purpose.\n",
    "- Guidelines on how to report issues related to smart contracts can be found at https://github.com/code-423n4/code-contests/blob/main/SUBMISSION_POLICY.md\n",
    "- More information on the slingshot finance competition can be found at the following medium article: https://medium.com/@scott_lew_is/slingshot-finance-sponsors-20-000-usdc-guaranteed-distribution-bounty-pool-for-code-432n4s-first-789514a8dc99\n",
    "'''\n",
    "\n",
    "user2 = '''\n",
    "Great, now review this excerpt:\n",
    "{}\n",
    "\n",
    "Remember, please provide a bulleted list of facts from this chat. Do not mention or reference any chat usernames or individuals. Only extract factual information without adding additional context or interpretation. When a fact relates to a link, ALWAYS include the link. If possible, try to figure out a specific thing when a general noun is used (for example use \"blue finance contest\" rather than \"contest\" if the context supports it). \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f89621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "excerpt = chat_data[30:60]\n",
    "prompt = user2.format(excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29fb15b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nGreat, now review this excerpt:\\n[(\\'shinobi\\', \\'You found a critical one already\\'), (\\'zscole\\', \\'i need to make it more clear, but reports should be submitted at the end of the contest period\\'), (\\'Thunder\\', \\'what happens then if 2 participants submit the same bug at the end of the contest?\\'), (\\'zscole\\', \\'https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md#duplicate-submissions\\'), (\\'Thunder\\', \\'so no hurry guyz, see you in 4 dayz\\'), (\\'zscole\\', \\'are you participating as a warden?\\'), (\\'Thunder\\', \\'idk just looking around for now\\'), (\\'zscole\\', \"sounds good. let me know if you\\'d like to be added to the warden role.\"), (\\'Luke\\', \\'Are there any well recommended resources on Solidity? Background is in vulnerability analysis but have not touched smart contracts etc so would probably need to do some deep diving before I can be of use\\'), (\\'zscole\\', \\'https://solidity-by-example.org/0.6\\\\nhttps://docs.soliditylang.org/en/v0.7.5/ (edited)\\'), (\\'zscole\\', \\'i would also recommend checking out the #team-formation channel and joining someone in there\\'), (\\'pdizzy\\', \\'how are you guys choosing \"judges\" and how do they show what their decision on a bounty is?\\'), (\\'scott_L\\', \\'@pdizzy based on experience/reputation. someone we think will do a good job. zak ( @zscole ) is judging the first contest.\\\\n\\\\nwe are going to publish the results after the contest concludes. because all of the pool is paid out, regardless of how many bugs are found, there is not an incentive for the judge to \"downgrade\" bugs or deny people bounty shares they have earned.\\'), (\\'zscole\\', \\'https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md\\'), (\\'WillieBeamin\\', \\'Hi. Is there a \"Getting Started\" or something similar that shows how we run the slingshot code as it executes in the overall system?\\'), (\\'scott_L\\', \\'@zscole ^\\'), (\\'zscole\\', \\'https://github.com/code-423n4/code-contests/tree/main/contests/01-slingshot#how-it-works\\'), (\\'WillieBeamin\\', \\'So we need to slingshot backend running locally, and thats it?\\'), (\\'zscole\\', \\'the contracts can be compiled and function independently of the back end (edited)\\'), (\\'WillieBeamin\\', \\'cool, thanks\\'), (\\'0xDolus\\', \\'Happy to do a Loom video in the AM to show how to get the environment set up if that would be helpful\\'), (\\'zscole\\', \\'that would be great!\\'), (\\'0xDolus\\', \\'\\'), (\\'Thunder\\', \\'can we get a countdown timer so we do not miss the submission deadline?\\'), (\\'zscole\\', \"sure i\\'ll see how we can go about implementing something like that\"), (\\'Е sockdrawer | C4\\', \\'@zscole lmk when the deadline is and Ill add one to the site.\\'), (\\'shinobi\\', \\'Hey man ! Yeah for sure, would be great to read your opinion, Ill see read and comment on it\\'), (\\'Е sockdrawer | C4\\', \\'@zscole @shinobi what do you think about requesting links and preferred avatars from competing wardens and adding them to the home page along with the countdown?\\'), (\\'zscole\\', \\'stop time is Feb 21, 2359 UTC\\'), (\\'Е sockdrawer | C4\\', \\'Basically: heres whos judging and whos competing in this contest so far... also you can still join them\\')]\\n\\nRemember, please provide a bulleted list of facts from this chat. Do not mention or reference any chat usernames or individuals. Only extract factual information without adding additional context or interpretation. When a fact relates to a link, ALWAYS include the link. If possible, try to figure out a specific thing when a general noun is used (for example use \"blue finance contest\" rather than \"contest\" if the context supports it). \\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c60c9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = model.predict_messages([HumanMessage(content=user1),\n",
    "                                  AIMessage(content=assistant1),\n",
    "                                  HumanMessage(content=prompt)\n",
    "                                 ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5facd784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Reports of found issues should be submitted at the end of the contest period.\n",
      "- There is a policy on how to handle duplicate submissions at the end of the contest, which can be found at https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md#duplicate-submissions\n",
      "- Participants can be added to the \"warden\" role.\n",
      "- There are recommended resources on Solidity at https://solidity-by-example.org/0.6 and https://docs.soliditylang.org/en/v0.7.5/\n",
      "- There is a #team-formation channel where participants can join others.\n",
      "- Judges are chosen based on experience and reputation.\n",
      "- The results of the contest will be published after it concludes.\n",
      "- All of the pool is paid out, regardless of how many bugs are found, so there is no incentive for the judge to \"downgrade\" bugs or deny people bounty shares they have earned.\n",
      "- The judging criteria can be found at https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md\n",
      "- There is a \"Getting Started\" guide on how to run the slingshot code at https://github.com/code-423n4/code-contests/tree/main/contests/01-slingshot#how-it-works\n",
      "- The contracts can be compiled and function independently of the back end.\n",
      "- There is a plan to create a Loom video to show how to set up the environment.\n",
      "- There is a plan to implement a countdown timer for the submission deadline.\n",
      "- The stop time for the contest is Feb 21, 2359 UTC.\n",
      "- There is a suggestion to request links and preferred avatars from competing wardens and adding them to the home page along with the countdown.\n"
     ]
    }
   ],
   "source": [
    "print(facts.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d4bde2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reports of found issues should be submitted at the end of the contest period.', 'There is a policy on how to handle duplicate submissions at the end of the contest, which can be found at https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md#duplicate-submissions', 'Participants can be added to the \"warden\" role.', 'There are recommended resources on Solidity at https://solidity-by-example.org/0.6 and https://docs.soliditylang.org/en/v0.7.5/', 'There is a #team-formation channel where participants can join others.', 'Judges are chosen based on experience and reputation.', 'The results of the contest will be published after it concludes.', 'All of the pool is paid out, regardless of how many bugs are found, so there is no incentive for the judge to \"downgrade\" bugs or deny people bounty shares they have earned.', 'The judging criteria can be found at https://github.com/code-423n4/code-contests/blob/main/JUDGING_CRITERIA.md', 'There is a \"Getting Started\" guide on how to run the slingshot code at https://github.com/code-423n4/code-contests/tree/main/contests/01-slingshot#how-it-works', 'The contracts can be compiled and function independently of the back end.', 'There is a plan to create a Loom video to show how to set up the environment.', 'There is a plan to implement a countdown timer for the submission deadline.', 'The stop time for the contest is Feb 21, 2359 UTC.', 'There is a suggestion to request links and preferred avatars from competing wardens and adding them to the home page along with the countdown.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.strip(\"\\n\").strip(\" \") for x in facts.content.split(\"- \") if x != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe22f86",
   "metadata": {},
   "source": [
    "### Process the whole chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef0c16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_facts = []\n",
    "interval_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9709f063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interval_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d1916e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Interval: 0 - 30\n",
      "Current Interval: 25 - 55\n",
      "Current Interval: 50 - 80\n",
      "Current Interval: 75 - 105\n",
      "Current Interval: 100 - 130\n",
      "Current Interval: 125 - 155\n",
      "Current Interval: 150 - 180\n",
      "Current Interval: 175 - 205\n",
      "Current Interval: 200 - 230\n",
      "Current Interval: 225 - 255\n",
      "Current Interval: 250 - 280\n",
      "Current Interval: 275 - 305\n",
      "Current Interval: 300 - 330\n",
      "Current Interval: 325 - 355\n",
      "Current Interval: 350 - 380\n",
      "Current Interval: 375 - 405\n",
      "Current Interval: 400 - 430\n",
      "Current Interval: 425 - 455\n",
      "Current Interval: 450 - 480\n",
      "Current Interval: 475 - 505\n",
      "Current Interval: 500 - 530\n",
      "Current Interval: 525 - 555\n",
      "Current Interval: 550 - 580\n",
      "Current Interval: 575 - 605\n",
      "Current Interval: 600 - 630\n",
      "Current Interval: 625 - 655\n",
      "Current Interval: 650 - 680\n",
      "Current Interval: 675 - 705\n",
      "Current Interval: 700 - 730\n",
      "Current Interval: 725 - 755\n",
      "Current Interval: 750 - 780\n",
      "Current Interval: 775 - 805\n",
      "Current Interval: 800 - 830\n",
      "Current Interval: 825 - 855\n",
      "Current Interval: 850 - 880\n",
      "Current Interval: 875 - 905\n",
      "Current Interval: 900 - 930\n",
      "Current Interval: 925 - 955\n",
      "Current Interval: 950 - 980\n",
      "Current Interval: 975 - 1005\n",
      "Current Interval: 1000 - 1030\n",
      "Current Interval: 1025 - 1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Interval: 1050 - 1080\n",
      "Current Interval: 1075 - 1105\n",
      "Current Interval: 1100 - 1130\n",
      "Current Interval: 1125 - 1155\n",
      "Current Interval: 1150 - 1180\n",
      "Current Interval: 1175 - 1205\n",
      "Current Interval: 1200 - 1230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Interval: 1225 - 1255\n",
      "Current Interval: 1250 - 1280\n",
      "Current Interval: 1275 - 1305\n",
      "Current Interval: 1300 - 1330\n",
      "Current Interval: 1325 - 1355\n",
      "Current Interval: 1350 - 1380\n",
      "Current Interval: 1375 - 1405\n",
      "Current Interval: 1400 - 1430\n",
      "Current Interval: 1425 - 1455\n",
      "Current Interval: 1450 - 1480\n",
      "Current Interval: 1475 - 1505\n",
      "Current Interval: 1500 - 1530\n",
      "Current Interval: 1525 - 1555\n",
      "Current Interval: 1550 - 1580\n",
      "Current Interval: 1575 - 1605\n",
      "Current Interval: 1600 - 1630\n",
      "Current Interval: 1625 - 1655\n",
      "Current Interval: 1650 - 1680\n",
      "Current Interval: 1675 - 1705\n",
      "Current Interval: 1700 - 1730\n",
      "Current Interval: 1725 - 1755\n",
      "Current Interval: 1750 - 1780\n",
      "Current Interval: 1775 - 1805\n",
      "Current Interval: 1800 - 1830\n",
      "Current Interval: 1825 - 1855\n",
      "Current Interval: 1850 - 1880\n",
      "Current Interval: 1875 - 1905\n",
      "Current Interval: 1900 - 1930\n",
      "Current Interval: 1925 - 1955\n",
      "Current Interval: 1950 - 1980\n",
      "Current Interval: 1975 - 2005\n",
      "Current Interval: 2000 - 2030\n",
      "Current Interval: 2025 - 2055\n",
      "Current Interval: 2050 - 2080\n",
      "Current Interval: 2075 - 2105\n",
      "Current Interval: 2100 - 2130\n",
      "Current Interval: 2125 - 2155\n",
      "Current Interval: 2150 - 2180\n",
      "Current Interval: 2175 - 2205\n",
      "Current Interval: 2200 - 2230\n",
      "Current Interval: 2225 - 2255\n",
      "Current Interval: 2250 - 2280\n",
      "Current Interval: 2275 - 2305\n",
      "Current Interval: 2300 - 2330\n",
      "Current Interval: 2325 - 2355\n",
      "Current Interval: 2350 - 2380\n",
      "Current Interval: 2375 - 2405\n",
      "Current Interval: 2400 - 2430\n",
      "Current Interval: 2425 - 2455\n",
      "Current Interval: 2450 - 2480\n",
      "Current Interval: 2475 - 2505\n",
      "Current Interval: 2500 - 2530\n",
      "Current Interval: 2525 - 2555\n",
      "Current Interval: 2550 - 2580\n",
      "Current Interval: 2575 - 2605\n",
      "Current Interval: 2600 - 2630\n",
      "Current Interval: 2625 - 2655\n",
      "Current Interval: 2650 - 2680\n",
      "Current Interval: 2675 - 2705\n",
      "Current Interval: 2700 - 2730\n",
      "Current Interval: 2725 - 2755\n",
      "Current Interval: 2750 - 2780\n",
      "Current Interval: 2775 - 2805\n",
      "Current Interval: 2800 - 2830\n",
      "Current Interval: 2825 - 2855\n",
      "Current Interval: 2850 - 2880\n",
      "Current Interval: 2875 - 2905\n",
      "Current Interval: 2900 - 2930\n",
      "Current Interval: 2925 - 2955\n",
      "Current Interval: 2950 - 2980\n",
      "Current Interval: 2975 - 3005\n",
      "Current Interval: 3000 - 3030\n",
      "Current Interval: 3025 - 3055\n",
      "Current Interval: 3050 - 3080\n",
      "Current Interval: 3075 - 3105\n",
      "Current Interval: 3100 - 3130\n",
      "Current Interval: 3125 - 3155\n",
      "Current Interval: 3150 - 3180\n",
      "Current Interval: 3175 - 3205\n",
      "Current Interval: 3200 - 3230\n",
      "Current Interval: 3225 - 3255\n",
      "Current Interval: 3250 - 3280\n",
      "Current Interval: 3275 - 3305\n",
      "Current Interval: 3300 - 3330\n",
      "Current Interval: 3325 - 3355\n",
      "Current Interval: 3350 - 3380\n",
      "Current Interval: 3375 - 3405\n",
      "Current Interval: 3400 - 3430\n",
      "Current Interval: 3425 - 3455\n",
      "Current Interval: 3450 - 3480\n",
      "Current Interval: 3475 - 3505\n",
      "Current Interval: 3500 - 3530\n",
      "Current Interval: 3525 - 3555\n",
      "Current Interval: 3550 - 3580\n",
      "Current Interval: 3575 - 3605\n",
      "Current Interval: 3600 - 3630\n",
      "Current Interval: 3625 - 3655\n",
      "Current Interval: 3650 - 3680\n",
      "Current Interval: 3675 - 3705\n",
      "Current Interval: 3700 - 3730\n",
      "Current Interval: 3725 - 3755\n",
      "Current Interval: 3750 - 3780\n",
      "Current Interval: 3775 - 3805\n",
      "Current Interval: 3800 - 3830\n",
      "Current Interval: 3825 - 3855\n",
      "Current Interval: 3850 - 3880\n",
      "Current Interval: 3875 - 3905\n",
      "Current Interval: 3900 - 3930\n",
      "Current Interval: 3925 - 3955\n",
      "Current Interval: 3950 - 3980\n",
      "Current Interval: 3975 - 4005\n",
      "Current Interval: 4000 - 4030\n",
      "Current Interval: 4025 - 4055\n",
      "Current Interval: 4050 - 4080\n",
      "Current Interval: 4075 - 4105\n",
      "Current Interval: 4100 - 4130\n",
      "Current Interval: 4125 - 4155\n",
      "Current Interval: 4150 - 4180\n",
      "Current Interval: 4175 - 4205\n",
      "Current Interval: 4200 - 4230\n",
      "Current Interval: 4225 - 4255\n",
      "Current Interval: 4250 - 4280\n",
      "Current Interval: 4275 - 4305\n",
      "Current Interval: 4300 - 4330\n",
      "Current Interval: 4325 - 4355\n",
      "Current Interval: 4350 - 4380\n",
      "Current Interval: 4375 - 4405\n",
      "Current Interval: 4400 - 4430\n",
      "Current Interval: 4425 - 4455\n",
      "Current Interval: 4450 - 4480\n",
      "Current Interval: 4475 - 4505\n",
      "Current Interval: 4500 - 4530\n",
      "Current Interval: 4525 - 4555\n",
      "Current Interval: 4550 - 4580\n",
      "Current Interval: 4575 - 4605\n",
      "Current Interval: 4600 - 4630\n",
      "Current Interval: 4625 - 4655\n",
      "Current Interval: 4650 - 4680\n",
      "Current Interval: 4675 - 4705\n",
      "Current Interval: 4700 - 4730\n",
      "Current Interval: 4725 - 4755\n",
      "Current Interval: 4750 - 4780\n",
      "Current Interval: 4775 - 4805\n",
      "Current Interval: 4800 - 4830\n",
      "Current Interval: 4825 - 4855\n",
      "Current Interval: 4850 - 4880\n",
      "Current Interval: 4875 - 4905\n",
      "Current Interval: 4900 - 4930\n",
      "Current Interval: 4925 - 4955\n",
      "Current Interval: 4950 - 4980\n",
      "Current Interval: 4975 - 5005\n",
      "Current Interval: 5000 - 5030\n",
      "Current Interval: 5025 - 5055\n",
      "Current Interval: 5050 - 5080\n",
      "Current Interval: 5075 - 5105\n",
      "Current Interval: 5100 - 5130\n",
      "Current Interval: 5125 - 5155\n",
      "Current Interval: 5150 - 5180\n",
      "Current Interval: 5175 - 5205\n",
      "Current Interval: 5200 - 5230\n",
      "Current Interval: 5225 - 5255\n",
      "Current Interval: 5250 - 5280\n",
      "Current Interval: 5275 - 5305\n",
      "Current Interval: 5300 - 5330\n",
      "Current Interval: 5325 - 5355\n",
      "Current Interval: 5350 - 5380\n",
      "Current Interval: 5375 - 5405\n",
      "Current Interval: 5400 - 5430\n",
      "Current Interval: 5425 - 5455\n",
      "Current Interval: 5450 - 5480\n",
      "Current Interval: 5475 - 5505\n",
      "Current Interval: 5500 - 5530\n",
      "Current Interval: 5525 - 5555\n",
      "Current Interval: 5550 - 5580\n",
      "Current Interval: 5575 - 5605\n",
      "Current Interval: 5600 - 5630\n",
      "Current Interval: 5625 - 5655\n",
      "Current Interval: 5650 - 5680\n",
      "Current Interval: 5675 - 5705\n",
      "Current Interval: 5700 - 5730\n",
      "Current Interval: 5725 - 5755\n",
      "Current Interval: 5750 - 5780\n",
      "Current Interval: 5775 - 5805\n",
      "Current Interval: 5800 - 5830\n",
      "Current Interval: 5825 - 5855\n",
      "Current Interval: 5850 - 5880\n",
      "Current Interval: 5875 - 5905\n",
      "Current Interval: 5900 - 5930\n",
      "Current Interval: 5925 - 5955\n",
      "Current Interval: 5950 - 5980\n",
      "Current Interval: 5975 - 6005\n",
      "Current Interval: 6000 - 6030\n",
      "Current Interval: 6025 - 6055\n",
      "Current Interval: 6050 - 6080\n",
      "Current Interval: 6075 - 6105\n",
      "Current Interval: 6100 - 6130\n",
      "Current Interval: 6125 - 6155\n",
      "Current Interval: 6150 - 6180\n",
      "Current Interval: 6175 - 6205\n",
      "Current Interval: 6200 - 6230\n",
      "Current Interval: 6225 - 6255\n",
      "Current Interval: 6250 - 6280\n",
      "Current Interval: 6275 - 6305\n",
      "Current Interval: 6300 - 6330\n",
      "Current Interval: 6325 - 6355\n",
      "Current Interval: 6350 - 6380\n",
      "Current Interval: 6375 - 6405\n",
      "Current Interval: 6400 - 6430\n",
      "Current Interval: 6425 - 6455\n",
      "Current Interval: 6450 - 6480\n",
      "Current Interval: 6475 - 6505\n",
      "Current Interval: 6500 - 6530\n",
      "Current Interval: 6525 - 6555\n",
      "Current Interval: 6550 - 6580\n",
      "Current Interval: 6575 - 6605\n",
      "Current Interval: 6600 - 6630\n",
      "Current Interval: 6625 - 6655\n",
      "Current Interval: 6650 - 6680\n",
      "Current Interval: 6675 - 6705\n",
      "Current Interval: 6700 - 6730\n",
      "Current Interval: 6725 - 6755\n",
      "Current Interval: 6750 - 6780\n",
      "Current Interval: 6775 - 6805\n",
      "Current Interval: 6800 - 6830\n",
      "Current Interval: 6825 - 6855\n",
      "Current Interval: 6850 - 6880\n",
      "Current Interval: 6875 - 6905\n",
      "Current Interval: 6900 - 6930\n",
      "Current Interval: 6925 - 6955\n",
      "Current Interval: 6950 - 6980\n",
      "Current Interval: 6975 - 7005\n",
      "Current Interval: 7000 - 7030\n",
      "Current Interval: 7025 - 7055\n",
      "Current Interval: 7050 - 7080\n",
      "Current Interval: 7075 - 7105\n",
      "Current Interval: 7100 - 7130\n",
      "Current Interval: 7125 - 7155\n",
      "Current Interval: 7150 - 7180\n",
      "Current Interval: 7175 - 7205\n",
      "Current Interval: 7200 - 7230\n",
      "Current Interval: 7225 - 7255\n",
      "Current Interval: 7250 - 7280\n",
      "Current Interval: 7275 - 7305\n",
      "Current Interval: 7300 - 7330\n",
      "Current Interval: 7325 - 7355\n",
      "Current Interval: 7350 - 7380\n",
      "Current Interval: 7375 - 7405\n",
      "Current Interval: 7400 - 7430\n",
      "Current Interval: 7425 - 7455\n",
      "Current Interval: 7450 - 7480\n",
      "Current Interval: 7475 - 7505\n",
      "Current Interval: 7500 - 7530\n",
      "Current Interval: 7525 - 7555\n",
      "Current Interval: 7550 - 7580\n",
      "Current Interval: 7575 - 7605\n",
      "Current Interval: 7600 - 7630\n",
      "Current Interval: 7625 - 7655\n",
      "Current Interval: 7650 - 7680\n",
      "Current Interval: 7675 - 7705\n",
      "Current Interval: 7700 - 7730\n",
      "Current Interval: 7725 - 7755\n",
      "Current Interval: 7750 - 7780\n",
      "Current Interval: 7775 - 7805\n",
      "Current Interval: 7800 - 7830\n",
      "Current Interval: 7825 - 7855\n",
      "Current Interval: 7850 - 7880\n",
      "Current Interval: 7875 - 7905\n",
      "Current Interval: 7900 - 7930\n",
      "Current Interval: 7925 - 7955\n",
      "Current Interval: 7950 - 7980\n",
      "Current Interval: 7975 - 8005\n",
      "Current Interval: 8000 - 8030\n",
      "Current Interval: 8025 - 8055\n",
      "Current Interval: 8050 - 8080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Interval: 8075 - 8105\n",
      "Current Interval: 8100 - 8130\n",
      "Current Interval: 8125 - 8155\n",
      "Current Interval: 8150 - 8180\n",
      "Current Interval: 8175 - 8205\n",
      "Current Interval: 8200 - 8230\n",
      "Current Interval: 8225 - 8255\n",
      "Current Interval: 8250 - 8280\n",
      "Current Interval: 8275 - 8305\n",
      "Current Interval: 8300 - 8330\n",
      "Current Interval: 8325 - 8355\n",
      "Current Interval: 8350 - 8380\n",
      "Current Interval: 8375 - 8405\n",
      "Current Interval: 8400 - 8430\n",
      "Current Interval: 8425 - 8455\n",
      "Current Interval: 8450 - 8480\n",
      "Current Interval: 8475 - 8505\n",
      "Current Interval: 8500 - 8530\n",
      "Current Interval: 8525 - 8555\n",
      "Current Interval: 8550 - 8580\n",
      "Current Interval: 8575 - 8605\n",
      "Current Interval: 8600 - 8630\n",
      "Current Interval: 8625 - 8655\n",
      "Current Interval: 8650 - 8680\n",
      "Current Interval: 8675 - 8705\n",
      "Current Interval: 8700 - 8730\n",
      "Current Interval: 8725 - 8755\n",
      "Current Interval: 8750 - 8780\n",
      "Current Interval: 8775 - 8805\n",
      "Current Interval: 8800 - 8830\n",
      "Current Interval: 8825 - 8855\n",
      "Current Interval: 8850 - 8880\n",
      "Current Interval: 8875 - 8905\n",
      "Current Interval: 8900 - 8930\n",
      "Current Interval: 8925 - 8955\n",
      "Current Interval: 8950 - 8980\n",
      "Current Interval: 8975 - 9005\n",
      "Current Interval: 9000 - 9030\n",
      "Current Interval: 9025 - 9055\n",
      "Current Interval: 9050 - 9080\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m excerpt \u001b[38;5;241m=\u001b[39m chat_data[interval_start:end_interval]\n\u001b[1;32m     10\u001b[0m prompt \u001b[38;5;241m=\u001b[39m user2\u001b[38;5;241m.\u001b[39mformat(excerpt)\n\u001b[0;32m---> 11\u001b[0m facts \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_messages([HumanMessage(content\u001b[38;5;241m=\u001b[39muser1),\n\u001b[1;32m     12\u001b[0m                           AIMessage(content\u001b[38;5;241m=\u001b[39massistant1),\n\u001b[1;32m     13\u001b[0m                           HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[1;32m     14\u001b[0m                          ])  \n\u001b[1;32m     15\u001b[0m list_of_facts \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m facts\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m all_facts \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m list_of_facts\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:601\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(messages, stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 551\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    552\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    553\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    308\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    310\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    314\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 299\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    300\u001b[0m                 m,\n\u001b[1;32m    301\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    302\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    303\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    304\u001b[0m             )\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    447\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    346\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:276\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m _thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mrequest_timeout \u001b[38;5;28;01mif\u001b[39;00m request_timeout \u001b[38;5;28;01melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[38;5;241m=\u001b[39m_thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "length = 30\n",
    "overlap = 5\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        end_interval = interval_start + length\n",
    "        print(\"Current Interval:\", interval_start, \"-\", end_interval)\n",
    "        excerpt = chat_data[interval_start:end_interval]\n",
    "        prompt = user2.format(excerpt)\n",
    "        facts = model.predict_messages([HumanMessage(content=user1),\n",
    "                                  AIMessage(content=assistant1),\n",
    "                                  HumanMessage(content=prompt)\n",
    "                                 ])  \n",
    "        list_of_facts = [x.strip(\"\\n\").strip(\" \") for x in facts.content.split(\"- \") if x != '']\n",
    "        all_facts += list_of_facts\n",
    "        interval_start += (length - overlap)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ed90d59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3235"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "19f74573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['An audit on Basin was cancelled without any notice, leaving some users in the dark about the situation.',\n",
       " \"Becoming a certified warden, a part of the verification process, might need a passport or a certified copy of an individual's identity.\",\n",
       " 'Certification process details can be found at https://docs.code4rena.com/roles/certified-contributors.',\n",
       " 'To gain backstage access one may need to qualify and then request backstage access via a help desk request. More details about backstage access can be found at https://docs.code4rena.com/roles/certified-contributors/backstage-wardens and the help desk request page is https://code4rena.com/help.',\n",
       " 'To participate in Chainlink contests and be eligible for rewards, one must go through a KYC process before submitting.',\n",
       " 'Users can submit report without being certified, however certification is needed to receive rewards.',\n",
       " 'There are questions about how to embed code on reports.',\n",
       " 'Pancakeswap V2 and Uniswap V2 have different formulas for protocol fees, with PancakeSwap V2 utilizing 8/25 of the growth in the square root of K as its protocol fee, while Uniswap V2 employs a 5 basis point (0.05%) protocol fee. The code for PancakeSwap V2 can be found at https://bscscan.com/address/0xcA143Ce32Fe78f1f7019d7d551a6402fC5350c73#code.',\n",
       " 'There are concerns about the lack of feedback on bug submissions.',\n",
       " 'Participation in some contests can be done without being certified, but some contests require certification for payouts if any submissions are awarded.']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_facts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f6824cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1998521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert the list to a JSON formatted string\n",
    "json_string = json.dumps(all_facts)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\"./codearena/codearena-2.json\", \"w\") as file:\n",
    "    file.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1683cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON string from the file\n",
    "with open(\"./codearena/codearena-2.json\", \"r\") as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Convert the JSON formatted string back to a Python list\n",
    "all_facts = json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "986d3810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The experiment being discussed is an interesting variation of a bug-bounty, where it is time-limited and has a guaranteed pot that pays out.'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_facts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d889d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f6062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f034a1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7736b1",
   "metadata": {},
   "source": [
    "### Cluster the facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5e378e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_embeddings = []\n",
    "for fact in all_facts:\n",
    "    embed = get_embedding(fact)\n",
    "    facts_embeddings.append((fact, embed))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "204c91b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# open a file, where you want to store the data\n",
    "file = open('./codearena/embeddings-09_22.pickle', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(facts_embeddings, file)\n",
    "\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "00d95c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you stored the pickled data\n",
    "file = open('./codearena/embeddings-09_22.pickle', 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "48b87717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c6fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3dbd1364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Convert the data into a DataFrame\n",
    "labels, values = zip(*facts_embeddings)\n",
    "df = pd.DataFrame({'Label': labels, 'Values': values})\n",
    "df_values = pd.DataFrame(df['Values'].to_list())\n",
    "df = pd.concat([df[['Label']], df_values], axis=1)\n",
    "\n",
    "# Drop the 'Label' column to use only numeric columns for KMeans\n",
    "X = df.drop('Label', axis=1)\n",
    "\n",
    "# Define the KMeans model\n",
    "kmeans = KMeans(n_clusters=200)  # for demonstration, we're using 2 clusters\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster assignments for each row in the DataFrame\n",
    "df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5042e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ca603c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Cluster' column and get the 'Label' values\n",
    "grouped = df.groupby('Cluster')['Label'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3a65e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "918a9699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(group) for group in grouped]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64d39cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Certain files like \"FloatCapital_v0.sol\", \"Treasury_v0.sol\" and \"oracles/\" are not in the scope for the bounty program.',\n",
       " 'There is a guideline not to submit assumptions such as the owner may be compromised or centralized. The methods with the onlyowner/onlygovernance modifiers are strictly coming through the trustful bodies.',\n",
       " 'Participants who started doing contests since June are not eligible to receive any token airdrop. They would have needed to start in 2021.',\n",
       " 'The Code4rena staff are employees of a corporation hired by a DAO, so they cant sign on behalf of the DAO.',\n",
       " 'Rewards are distributed by the CodeArena team and cannot be withdrawn via a smart contract.',\n",
       " \"Leaderboard standing in CodeArena is not transferrable. Findings submitted under a user's current handle or username are not moved to another account.\",\n",
       " 'Listing any of the C4udit gas findings will void your report and count as 3 rejected reports.',\n",
       " 'Creating an alternate account and submitting the same issue from both accounts does not increase share, it decreases due to sybil protection.',\n",
       " 'Labels like \"bug\", \"grade-c\", and \"unsatisfactory\" on an issue indicate that it is not eligible for rewards.',\n",
       " 'Findings listed in the best bot-generated report are out of the contests scope, similar to the current Automated Findings.',\n",
       " \"Users cannot receive a reward for findings made with ChatGPT. If they wish to use AI in auditing, they're advised to enter the bot races instead.\",\n",
       " \"Regarding the bot races, the bots are considered a warden's intellectual property and are unlikely to be open sourced by CodeArena.\",\n",
       " 'The same issues reported by a bot should not be included in the report unless they build a more complex exploit.',\n",
       " 'For each contest, the Readme Page has a section titled \"Known Findings\" where automated findings not accepted in the contests are listed.',\n",
       " 'If a bot finds a high or medium finding, it only gets the bot pool reward based on the bot race rank. Bots can only gain more rewards by having more points and shifting the rank cutoffs, thus bumping others to lower ranks.',\n",
       " \"A user's submitted bug report that has been rejected can be found in Github's closed issues.\",\n",
       " 'QA reports that include QA bot findings from bot races but develop their explanation more and are more detailed are not eligible for QA report rewards.',\n",
       " 'If a participant submitted issues for a contest but did not make the award list, it is likely that their issues were rejected. Confirmation can be done by reviewing the available report.',\n",
       " 'Users may not submit different issues with different impacts or different attack scenarios if they all originate from the same root cause.']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped[175]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322abb1a",
   "metadata": {},
   "source": [
    "### Add Facts to Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "f6207b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_class = UseExisting()\n",
    "facts_db = FAISS.from_texts(all_facts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "47186640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Users are allowed to present their proofs of concept (PoC) in either code or plain English.', metadata={}),\n",
       " Document(page_content='Some participants are curious about the use of fuzzing tools like Echidna for auditing in contests.', metadata={}),\n",
       " Document(page_content=\"It's acceptable to submit a (very long) proof of concept (POC) using external platforms such as gist.\", metadata={}),\n",
       " Document(page_content=\"It's not necessary for a PoC to be exact code.\", metadata={}),\n",
       " Document(page_content='Users are curious about the use of fuzzing tools like Echidna for auditing in contests.', metadata={}),\n",
       " Document(page_content='There is a GitHub link that provides instructions on sharing vulnerability discovery PoCs: https://github.com/code-423n4/code-contests/tree/main/contests/01-slingshot#sharing-vulnerabilitydiscovery-poc', metadata={}),\n",
       " Document(page_content=\"There's a question on whether potential medium findings need to include Proof of Concept (POC).\", metadata={}),\n",
       " Document(page_content='It is suggested that auditors can create coded Proof-of-Concepts (POCs) to further explain their reported issues, but it will not have an effect on awards or the contest per C4 guidelines.', metadata={}),\n",
       " Document(page_content='While submitting an issue for any contest, it is beneficial to include a proof of concept and a case made for how an item can be exploited to avoid being marked as invalid.', metadata={}),\n",
       " Document(page_content='Users can submit code for proof of concepts (PoC) for each bug they find.', metadata={}),\n",
       " Document(page_content='Users can write Proof of Concept (PoC) in any language, as long as it demonstrates the vulnerability.', metadata={}),\n",
       " Document(page_content='Web applications might be in the scope of certain contests.', metadata={}),\n",
       " Document(page_content=\"The discussion includes a query about whether a bug report without Proof of Concept (PoC) would be accepted; the response suggests that without a PoC, a finding may be disregarded unless the issue is extremely obvious (such as a wrong parameter, typo, or code that doesn't compile).\", metadata={}),\n",
       " Document(page_content='If a user has written a Proof of Concept (POC) script for a vulnerability, they can include the link in the submission wherever relevant.', metadata={}),\n",
       " Document(page_content='It is acceptable to submit long proofs of concept (POC) using external platforms like Gist.', metadata={}),\n",
       " Document(page_content='Instructions on how to include a PoC are available at https://docs.code4rena.com/roles/wardens/submission-policy#how-to-include-a-proof-of-concept.', metadata={}),\n",
       " Document(page_content=\"Code4rena encourages participants to reach out to the sponsor team during the contest if they think they've found something and want to ask questions. Participants can also disclose a vulnerability directly to them, but they need to submit it via the contest submission form or it won't be eligible for awards.\", metadata={}),\n",
       " Document(page_content='Proof of Concepts (POCs) can be submitted by creating a public Github repository or by providing a diff of an existing sponsor-supplied test/contract.', metadata={}),\n",
       " Document(page_content='There was a question about whether citing similar findings from other contests is allowed to justify the severity and validity within submissions.', metadata={}),\n",
       " Document(page_content='A query about whether a POC (Proof of Concept) should fully show every step in code is raised but not answered in the excerpt.', metadata={})]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_db.similarity_search(\"Is it allowed to use arbitrary tools for PoC? Or must I use the framework which the contest project is set up with?\", k=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a07c0",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87fd515e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Obtaining dependency information for gensim from https://files.pythonhosted.org/packages/63/46/5feab9c524a380bfa9f9f1c0d065743280dca30b216ab4c7a231f22dbed7/gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from gensim) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from gensim) (1.11.2)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Obtaining dependency information for smart-open>=1.8.1 from https://files.pythonhosted.org/packages/fc/d9/d97f1db64b09278aba64e8c81b5d322d436132df5741c518f3823824fae0/smart_open-6.4.0-py3-none-any.whl.metadata\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /Users/allanniemerg/miniconda3/envs/ollama-test/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Downloading gensim-4.3.2-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, click, nltk, gensim\n",
      "Successfully installed click-8.1.7 gensim-4.3.2 nltk-3.8.1 smart-open-6.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3b04044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/allanniemerg/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/allanniemerg/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4f3208d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.049*\"email\" + 0.030*\"issue\" + 0.027*\"question\" + 0.020*\"user\" + 0.020*\"whether\"'), (1, '0.047*\"contest\" + 0.018*\"question\" + 0.016*\"participant\" + 0.014*\"testing\" + 0.012*\"bug\"'), (2, '0.026*\"report\" + 0.019*\"code\" + 0.013*\"issue\" + 0.011*\"project\" + 0.011*\"line\"'), (3, '0.020*\"pool\" + 0.017*\"question\" + 0.015*\"contract\" + 0.012*\"sherlock\" + 0.012*\"protocol\"'), (4, '0.075*\"contract\" + 0.052*\"smart\" + 0.017*\"team\" + 0.017*\"distributed\" + 0.016*\"award\"'), (5, '0.025*\"contest\" + 0.015*\"finding\" + 0.015*\"contract\" + 0.013*\"test\" + 0.013*\"submit\"'), (6, '0.029*\"issue\" + 0.025*\"process\" + 0.023*\"handle\" + 0.023*\"award\" + 0.018*\"finding\"'), (7, '0.038*\"report\" + 0.027*\"contest\" + 0.015*\"issue\" + 0.014*\"suggestion\" + 0.014*\"published\"'), (8, '0.061*\"contest\" + 0.012*\"gas\" + 0.011*\"warden\" + 0.010*\"website\" + 0.010*\"finding\"'), (9, '0.038*\"contest\" + 0.021*\"report\" + 0.018*\"address\" + 0.018*\"finding\" + 0.018*\"question\"'), (10, '0.023*\"contest\" + 0.013*\"link\" + 0.013*\"reality\" + 0.013*\"card\" + 0.012*\"protocol\"'), (11, '0.039*\"submission\" + 0.020*\"time\" + 0.017*\"sponsor\" + 0.017*\"gas\" + 0.016*\"contest\"'), (12, '0.027*\"contest\" + 0.015*\"code\" + 0.015*\"finding\" + 0.012*\"test\" + 0.012*\"submitted\"'), (13, '0.050*\"contest\" + 0.033*\"submission\" + 0.021*\"time\" + 0.017*\"issue\" + 0.012*\"github\"'), (14, '0.031*\"user\" + 0.023*\"address\" + 0.020*\"question\" + 0.017*\"submission\" + 0.014*\"specific\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download if you haven't\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load stopwords and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Data cleaning function\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Prepare data\n",
    "data_clean = [clean(doc).split() for doc in all_facts]  # assuming \"facts\" is your list of sentences\n",
    "\n",
    "# Create term dictionary\n",
    "dictionary = corpora.Dictionary(data_clean)\n",
    "\n",
    "# Create document-term matrix\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in data_clean]\n",
    "\n",
    "# Create LDA model\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Train LDA model (for example, with 5 topics)\n",
    "ldamodel = lda(doc_term_matrix, num_topics=15, id2word = dictionary, passes=50)\n",
    "\n",
    "# Print topics\n",
    "print(ldamodel.print_topics(num_topics=30, num_words=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d34ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=data_clean, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b8c06796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training the LDA model\n",
    "\n",
    "# Get the topic distribution for all documents\n",
    "document_topics = ldamodel.get_document_topics(doc_term_matrix, minimum_probability=0)\n",
    "\n",
    "# Filter documents that correlate strongly with a given topic (e.g., topic 0)\n",
    "topic_id = 0\n",
    "threshold = 0.5  # adjust this threshold as needed\n",
    "\n",
    "strongly_correlated_docs = []\n",
    "for doc_id, topics in enumerate(document_topics):\n",
    "    for topic, prob in topics:\n",
    "        if topic == topic_id and prob >= threshold:\n",
    "            strongly_correlated_docs.append((doc_id, prob))\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_docs = sorted(strongly_correlated_docs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the actual documents\n",
    "correlated_documents = [all_facts[doc_id] for doc_id, _ in sorted_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8bb91cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The maple-core repo has a test script set to use 100 fuzz runs, but for first time users, it is recommended to use 1 fuzz run and then increase to 10-100 fuzz runs after the first run.',\n",
       " 'It was clarified that when the report is out, the repo will be fully opened and participants will be able to see the discussion among sponsors and judges on the specific issue.',\n",
       " 'Auditors may need to manually check the differences between contracts, or they might be able to run a diff command on the two contracts.',\n",
       " 'A question was raised about whether wardens who report the same vulnerability but with different severities are given the same severity for award calculation.',\n",
       " 'Users are advised to switch to a different email address if they are experiencing issues with receiving emails.',\n",
       " 'Questions about the Vader protocol can be directed to a specific individual, and the latest updates have been posted at https://github.com/code-423n4/2021-04-vader.',\n",
       " 'A forum post that works through all the moving pieces in the opening constitution and delegation can be found at https://forum.code4rena.com/t/c4ip-1-2-3-4-5-constitution-dao-bootstrapping-reimbursements-token-sale/93.',\n",
       " 'If wardens report the same vulnerability but with different severities, they are given the same severity for award calculation.',\n",
       " 'Two big bugs have been found in the internal audit that have not been picked up by any wardens yet.',\n",
       " 'The term \"gov-wg\" refers to a Working Group set up for a DAO structure.',\n",
       " 'The tool that generates a specific output is not known, but most people use Slither.',\n",
       " \"InvariantTransactionData.transactionId is a unique identifier for the crosschain transfer to be used in Connext's protocol.\",\n",
       " 'The question of whether a minter or burner role is an issue was raised.',\n",
       " 'There was a spam issue with Yahoo and Hotmail email addresses in the past.',\n",
       " 'There is an outreach effort to connect with users and ask them about their experiences with C4.',\n",
       " 'There is a question about whether a minter or burner role is an issue.',\n",
       " 'Questions can be asked in private for detailed answers and guidance.',\n",
       " 'There are pending awards for LPT tokens and NFTX.',\n",
       " 'LPT tokens and NFTX awards are pending.',\n",
       " 'There is no email notification for the validity of each submitted issue.',\n",
       " 'There is a question about the difference between low/medium/high risk finds.',\n",
       " 'There has been an effort to move to authenticated warden accounts.',\n",
       " 'Results for past projects are being worked on and should be up soon.',\n",
       " 'The address of the C4 token is 0x6847D3A4c80a82e1fb26f1fC6F09F3Ad5BEB5222.',\n",
       " '\"gov-wg\" refers to a Working Group set up to establish a DAO structure.',\n",
       " 'There is a request to check https://github.com/code-423n4/code423n4.com/pull/62',\n",
       " \"There was an issue with a user's email flagging C4 emails as spam.\",\n",
       " 'Some users have experienced issues with receiving emails from CodeArena, with emails being flagged as spam.',\n",
       " 'There is a suggestion to add the severity of the bug to the C4 emails that are sent out after an issue is submitted.',\n",
       " 'After submitting a finding, users do not need to do anything else but wait until the contest ends and check the results on the website.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlated_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc5917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d834962",
   "metadata": {},
   "source": [
    "### Extract Questions and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "54240436",
   "metadata": {},
   "outputs": [],
   "source": [
    "u1 = '''\n",
    "We are trying to build a FAQ using questions and answers present in our Discord chatroom. Our company is CodeArena (C4), a company that helps other companies receive audits of their smart contracts. You are an expert at constructing frequently asked question documents. \n",
    "\n",
    "Please carefully review the chat log and extract question and answer pairs. Please keep extracting questions and answers until you've extracted them all. Questions may be implicit, so read between the lines. There may not always be an answer provided by another user in the chat. Please just leave the answer blank, if that's the case. \n",
    "\n",
    "Chat: \n",
    "{}\n",
    "\n",
    "Please provide a bulleted list of questions and answers from this chat.  Do not mention or reference any chat usernames or individuals. When a question or answer relates to a link, ALWAYS include the link. If possible, try to figure out \n",
    "a specific thing when a general noun is used (for example use \"ribbon finance contest\" rather \n",
    "than \"contest\" if the context supports it).  \n",
    "Format: \n",
    "Q: [Question] \n",
    "A: [Answer]\n",
    "\n",
    "Q: [Question] \n",
    "A: [Answer]\n",
    "'''\n",
    "\n",
    "all_qs = []\n",
    "interval_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "affc8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Interval: 5655 - 5675\n",
      "Current Interval: 5670 - 5690\n",
      "Current Interval: 5685 - 5705\n",
      "Current Interval: 5700 - 5720\n",
      "Current Interval: 5715 - 5735\n",
      "Current Interval: 5730 - 5750\n",
      "Current Interval: 5745 - 5765\n",
      "Current Interval: 5760 - 5780\n",
      "Current Interval: 5775 - 5795\n",
      "Current Interval: 5790 - 5810\n",
      "Current Interval: 5805 - 5825\n",
      "Current Interval: 5820 - 5840\n",
      "Current Interval: 5835 - 5855\n",
      "Current Interval: 5850 - 5870\n",
      "Current Interval: 5865 - 5885\n",
      "Current Interval: 5880 - 5900\n",
      "Current Interval: 5895 - 5915\n",
      "Current Interval: 5910 - 5930\n",
      "Current Interval: 5925 - 5945\n",
      "Current Interval: 5940 - 5960\n",
      "Current Interval: 5955 - 5975\n",
      "Current Interval: 5970 - 5990\n",
      "Current Interval: 5985 - 6005\n",
      "Current Interval: 6000 - 6020\n",
      "Current Interval: 6015 - 6035\n",
      "Current Interval: 6030 - 6050\n",
      "Current Interval: 6045 - 6065\n",
      "Current Interval: 6060 - 6080\n",
      "Current Interval: 6075 - 6095\n",
      "Current Interval: 6090 - 6110\n",
      "Current Interval: 6105 - 6125\n",
      "Current Interval: 6120 - 6140\n",
      "Current Interval: 6135 - 6155\n",
      "Current Interval: 6150 - 6170\n",
      "Current Interval: 6165 - 6185\n",
      "Current Interval: 6180 - 6200\n",
      "Current Interval: 6195 - 6215\n",
      "Current Interval: 6210 - 6230\n",
      "Current Interval: 6225 - 6245\n",
      "Current Interval: 6240 - 6260\n",
      "Current Interval: 6255 - 6275\n",
      "Current Interval: 6270 - 6290\n",
      "Current Interval: 6285 - 6305\n",
      "Current Interval: 6300 - 6320\n",
      "Current Interval: 6315 - 6335\n",
      "Current Interval: 6330 - 6350\n",
      "Current Interval: 6345 - 6365\n",
      "Current Interval: 6360 - 6380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "length = 20\n",
    "overlap = 5\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if interval_start > len(chat_data):\n",
    "            break\n",
    "        end_interval = interval_start + length\n",
    "        print(\"Current Interval:\", interval_start, \"-\", end_interval)\n",
    "        excerpt = chat_data[interval_start:end_interval]\n",
    "        prompt = u1.format(excerpt)\n",
    "        qs = model.predict_messages([HumanMessage(content=prompt)])    \n",
    "        list_of_qs = [x.strip(\"\\n\").strip(\" \") for x in qs.content.split(\"Q: \") if x != '']\n",
    "        all_qs += list_of_qs\n",
    "        interval_start += (length - overlap)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d3efdb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3595"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e989ed4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can I participate in contests without being a certified contributor?\\nA: You may participate without being certified. However, some contests will require certification for payouts if any of your submissions are awarded.'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_qs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a62db2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a JSON formatted string\n",
    "json_string = json.dumps(all_qs)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\"./codearena/codearena-qs_09_23.json\", \"w\") as file:\n",
    "    file.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dc357744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON string from the file\n",
    "with open(\"./codearena/codearena-qs_09_23.json\", \"r\") as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Convert the JSON formatted string back to a Python list\n",
    "all_qs2 = json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ac3b0281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3595"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_qs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d9794cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How should we treat upgradeable contracts findings in case of Medium-risk vulnerabilities, for example DoSing or bricking the contract?\n",
      "A: If the protocol can be bricked until the upgrade takes place, it's the text book definition of a Medium risk bug.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randrange(len(all_qs2))\n",
    "print(all_qs2[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ddaab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaaf7f92",
   "metadata": {},
   "source": [
    "### De-duplicate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87afad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_class = UseExisting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "4a0e9663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an undergrad IT student currently in my 3rd year. My current goal is to make my career in cybersecurity. I am totally invested in learning about smart contracts, but I fear that by doing so, my web2 security skills would be on a halt which I don't want. My only motivation learning about smart contracts is 'money' than bugcrowd/hackerone. What should I do? Should I complete focus on smart contract and make money or focus on web2 security and do this as a sidekick?\n",
      "A1: The focus shouldnt be on money, but on growing your skillset and knowledge. If thats the sole reason for learning about Web3, then you should focus on building a strong foundation in Web2 security. Youre still young, make full use of your time to discover what youre competent at and what interests you more. \n",
      "A2: Only you can answer that question. You know what matters more for you personally. Good money can be made both in Web2 and Web3 if you are good. It seems you still have a very on the surface understanding of both types of security practices. Perhaps you should deepen your knowledge in both until one side \"grabs\" you more than the other. \n",
      "A3: The focus should be on what you enjoy the most. If you like the crypto/finance world, you should focus on that.\n",
      "A4: The choice is all yours. Cybersecurity is a broad career path with many domains. If you want to focus as a Penetration Tester and juggle smart contract auditing, your first step is to learn about the technology then apply the cybersecurity concepts to it with an attacker mindset.\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "question_and_answers = []\n",
    "for q_a in all_qs2:\n",
    "    try:\n",
    "        question = q_a.split(\"A:\")[0]\n",
    "        answer = q_a.split(\"A:\")[1]\n",
    "        question_and_answers.append((question, {'answer': answer}))\n",
    "    except:\n",
    "        print(q_a)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a9a64c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3593"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(question_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174360c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "704a2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into a DataFrame\n",
    "all_questions, all_answers = zip(*question_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c00462b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Can I participate in contests without being a certified contributor?\\n',\n",
       " {'answer': ' You may participate without being certified. However, some contests will require certification for payouts if any of your submissions are awarded.'})"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_questions[-1], all_answers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "534c24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_texts(all_questions, embeddings, metadatas=all_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f4f709d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='More contest please?\\n', metadata={'answer': ' Ethos is big enough to keep wardens busy for a while.'}),\n",
       " Document(page_content='More contest coming out?\\n', metadata={'answer': ' [No answer provided]'}),\n",
       " Document(page_content='What is vs contest? \\n', metadata={'answer': ' A slightly different contest with only 3 wardens!'}),\n",
       " Document(page_content='Where can we see current ongoing contests?\\n', metadata={'answer': ' The team is currently talking to a number of projects about upcoming audits, implying that there are currently no ongoing contests.'})]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"contests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "0cbb2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have the FAISS index loaded as 'index'\n",
    "\n",
    "threshold_distance = 0.3  # Define a suitable threshold, 0.2 is very good\n",
    "groups = []\n",
    "grouped_indices = set()\n",
    "\n",
    "for i in range(db.index.ntotal):\n",
    "    if i not in grouped_indices:\n",
    "        query_embedding = np.array([db.index.reconstruct(i)])  # Get the embedding for the i-th index\n",
    "        D, I = db.index.search(query_embedding, db.index.ntotal)\n",
    "        similar_indices = I[D < threshold_distance**2].ravel()  # Squaring threshold because L2 distance\n",
    "        groups.append(similar_indices.tolist())\n",
    "        grouped_indices.update(similar_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "8e918626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "94e7636d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuyUlEQVR4nO3dfXRU9Z3H8c8kzAyEJoGgySTHECNteX5GIFUpCCTGFOvKtoug0EqlcoIKsRSjgAGswVDxkeqyK+IeYUXPUVRkMQMooISn2ClPbioUjV2Z0BVhhKzDkMz+0c2sYxIgOJO5P/J+nTPH3Ht/87vfe78e8jn33snYgsFgUAAAAAaJi3UBAAAALUWAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYp12sC4iW+vp6ff7550pMTJTNZot1OQAA4AIEg0F99dVXysjIUFxc89dZLtkA8/nnnyszMzPWZQAAgIvw2Wef6Yorrmh2+yUbYBITEyX9/QQkJSXFuBrrCQQCKi8vV25urux2e6zLgeiJ1dAPa6Ef1hLNfvh8PmVmZoZ+jzfnkg0wDbeNkpKSCDBNCAQCSkhIUFJSEv8YWAQ9sRb6YS30w1paox/ne/yDh3gBAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHFaFGBKS0t19dVXKzExUampqbr55ptVVVUVNubrr79WYWGhunTpou9973saP368ampqwsZUV1eroKBACQkJSk1N1ezZs3X27NmwMe+9954GDRokp9Op73//+1q5cuXFHSEAALjktCjAbNmyRYWFhdqxY4fcbrcCgYByc3N1+vTp0JhZs2bprbfe0quvvqotW7bo888/1y233BLaXldXp4KCAp05c0bbt2/Xiy++qJUrV2r+/PmhMUeOHFFBQYFGjRolj8ejmTNn6le/+pXeeeedCBwyAAAwXYu+C2nDhg1hyytXrlRqaqoqKys1YsQInTx5Us8//7xWr16t66+/XpL0wgsvqGfPntqxY4eGDx+u8vJyHTx4UBs3blRaWpoGDBigRYsWac6cOSopKZHD4dBzzz2n7OxsPfbYY5Kknj176v3339fjjz+uvLy8CB06AAAw1Xf6MseTJ09KklJSUiRJlZWVCgQCGjNmTGhMjx491LVrV1VUVGj48OGqqKhQ3759lZaWFhqTl5en6dOn68CBAxo4cKAqKirC5mgYM3PmzGZr8fv98vv9oWWfzyfp7184FQgEvsthXpIazgnnxjroibXQD2uhH9YSzX5c6JwXHWDq6+s1c+ZMXXPNNerTp48kyev1yuFwqFOnTmFj09LS5PV6Q2O+GV4atjdsO9cYn8+n//mf/1GHDh0a1VNaWqoFCxY0Wl9eXq6EhISLO8g2wO12x7oEfAs9sRb6YS30w1qi0Y/a2toLGnfRAaawsFD79+/X+++/f7FTRFRxcbGKiopCyz6fT5mZmcrNzVVSUlJE99WnxLxncfaXhN96CwQCcrvdGjt2LF9NbxH0xFroh7XQD2uJZj8a7qCcz0UFmBkzZmjdunXaunWrrrjiitB6l8ulM2fO6MSJE2FXYWpqauRyuUJjdu3aFTZfw6eUvjnm259cqqmpUVJSUpNXXyTJ6XTK6XQ2Wm+32yN+cv11tojO1xqaOwfROD/4buiJtdAPa6Ef1hKNflzofC36FFIwGNSMGTP0+uuva/PmzcrOzg7bPnjwYNntdm3atCm0rqqqStXV1crJyZEk5eTkaN++fTp27FhojNvtVlJSknr16hUa8805GsY0zAEAANq2Fl2BKSws1OrVq/XGG28oMTEx9MxKcnKyOnTooOTkZE2dOlVFRUVKSUlRUlKS7r77buXk5Gj48OGSpNzcXPXq1Uu33367ysrK5PV6NXfuXBUWFoauoNx111165pln9Nvf/lZ33HGHNm/erFdeeUVvv/12hA8fAACYqEVXYJ599lmdPHlSI0eOVHp6eui1Zs2a0JjHH39cP/nJTzR+/HiNGDFCLpdLr732Wmh7fHy81q1bp/j4eOXk5Oi2227T5MmTtXDhwtCY7Oxsvf3223K73erfv78ee+wx/eu//isfoQYAAJJaeAUmGAyed0z79u21bNkyLVu2rNkxWVlZWr9+/TnnGTlypP74xz+2pDwAANBG8F1IAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4LQ4wW7du1bhx45SRkSGbzaa1a9eGbbfZbE2+lixZEhpz5ZVXNtq+ePHisHn27t2r6667Tu3bt1dmZqbKysou7ggBAMAlp8UB5vTp0+rfv7+WLVvW5PajR4+GvVasWCGbzabx48eHjVu4cGHYuLvvvju0zefzKTc3V1lZWaqsrNSSJUtUUlKi5cuXt7RcAABwCWrX0jfk5+crPz+/2e0ulyts+Y033tCoUaN01VVXha1PTExsNLbBqlWrdObMGa1YsUIOh0O9e/eWx+PR0qVLNW3atJaWDAAALjEtDjAtUVNTo7ffflsvvvhio22LFy/WokWL1LVrV02cOFGzZs1Su3Z/L6eiokIjRoyQw+EIjc/Ly9Ojjz6qL7/8Up07d240n9/vl9/vDy37fD5JUiAQUCAQiOhxOeODEZ2vNXz7HDQsR/rc4OLRE2uhH9ZCP6wlmv240DmjGmBefPFFJSYm6pZbbglbf88992jQoEFKSUnR9u3bVVxcrKNHj2rp0qWSJK/Xq+zs7LD3pKWlhbY1FWBKS0u1YMGCRuvLy8uVkJAQqUOSJJUNjeh0rWL9+vVNrne73a1cCc6HnlgL/bAW+mEt0ehHbW3tBY2LaoBZsWKFJk2apPbt24etLyoqCv3cr18/ORwO/frXv1ZpaamcTudF7au4uDhsXp/Pp8zMTOXm5iopKeniDqAZfUreieh8rWF/SV7YciAQkNvt1tixY2W322NUFb6JnlgL/bAW+mEt0exHwx2U84lagNm2bZuqqqq0Zs2a844dNmyYzp49q08++UTdu3eXy+VSTU1N2JiG5eaem3E6nU2GH7vdHvGT66+zRXS+1tDcOYjG+cF3Q0+shX5YC/2wlmj040Lni9rfgXn++ec1ePBg9e/f/7xjPR6P4uLilJqaKknKycnR1q1bw+6Dud1ude/evcnbRwAAoG1pcYA5deqUPB6PPB6PJOnIkSPyeDyqrq4OjfH5fHr11Vf1q1/9qtH7Kyoq9MQTT+hPf/qT/vKXv2jVqlWaNWuWbrvttlA4mThxohwOh6ZOnaoDBw5ozZo1evLJJ8NuEQEAgLarxbeQ9uzZo1GjRoWWG0LFlClTtHLlSknSyy+/rGAwqFtvvbXR+51Op15++WWVlJTI7/crOztbs2bNCgsnycnJKi8vV2FhoQYPHqzLLrtM8+fP5yPUAABA0kUEmJEjRyoYPPfHiKdNm9Zs2Bg0aJB27Nhx3v3069dP27Zta2l5AACgDeC7kAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcVocYLZu3apx48YpIyNDNptNa9euDdv+i1/8QjabLex1ww03hI05fvy4Jk2apKSkJHXq1ElTp07VqVOnwsbs3btX1113ndq3b6/MzEyVlZW1/OgAAMAlqcUB5vTp0+rfv7+WLVvW7JgbbrhBR48eDb3+/d//PWz7pEmTdODAAbndbq1bt05bt27VtGnTQtt9Pp9yc3OVlZWlyspKLVmyRCUlJVq+fHlLywUAAJegdi19Q35+vvLz8885xul0yuVyNbnto48+0oYNG7R7924NGTJEkvT000/rxhtv1O9//3tlZGRo1apVOnPmjFasWCGHw6HevXvL4/Fo6dKlYUEHAAC0TS0OMBfivffeU2pqqjp37qzrr79eDz/8sLp06SJJqqioUKdOnULhRZLGjBmjuLg47dy5U//wD/+giooKjRgxQg6HIzQmLy9Pjz76qL788kt17ty50T79fr/8fn9o2efzSZICgYACgUBEj88ZH4zofK3h2+egYTnS5wYXj55YC/2wFvphLdHsx4XOGfEAc8MNN+iWW25Rdna2Dh8+rAceeED5+fmqqKhQfHy8vF6vUlNTw4to104pKSnyer2SJK/Xq+zs7LAxaWlpoW1NBZjS0lItWLCg0fry8nIlJCRE6vAkSWVDIzpdq1i/fn2T691udytXgvOhJ9ZCP6yFflhLNPpRW1t7QeMiHmAmTJgQ+rlv377q16+funXrpvfee0+jR4+O9O5CiouLVVRUFFr2+XzKzMxUbm6ukpKSIrqvPiXvRHS+1rC/JC9sORAIyO12a+zYsbLb7TGqCt9ET6yFflgL/bCWaPaj4Q7K+UTlFtI3XXXVVbrssst06NAhjR49Wi6XS8eOHQsbc/bsWR0/fjz03IzL5VJNTU3YmIbl5p6tcTqdcjqdjdbb7faIn1x/nS2i87WG5s5BNM4Pvht6Yi30w1roh7VEox8XOl/U/w7MX//6V33xxRdKT0+XJOXk5OjEiROqrKwMjdm8ebPq6+s1bNiw0JitW7eG3Qdzu93q3r17k7ePAABA29LiAHPq1Cl5PB55PB5J0pEjR+TxeFRdXa1Tp05p9uzZ2rFjhz755BNt2rRJP/3pT/X9739feXl/v4XRs2dP3XDDDbrzzju1a9cuffDBB5oxY4YmTJigjIwMSdLEiRPlcDg0depUHThwQGvWrNGTTz4ZdosIAAC0XS0OMHv27NHAgQM1cOBASVJRUZEGDhyo+fPnKz4+Xnv37tVNN92kH/7wh5o6daoGDx6sbdu2hd3eWbVqlXr06KHRo0frxhtv1LXXXhv2N16Sk5NVXl6uI0eOaPDgwbrvvvs0f/58PkINAAAkXcQzMCNHjlQw2PzHiN955/wPuKakpGj16tXnHNOvXz9t27atpeUBAIA2gO9CAgAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGaXGA2bp1q8aNG6eMjAzZbDatXbs2tC0QCGjOnDnq27evOnbsqIyMDE2ePFmff/552BxXXnmlbDZb2Gvx4sVhY/bu3avrrrtO7du3V2ZmpsrKyi7uCAEAwCWnxQHm9OnT6t+/v5YtW9ZoW21trT788EPNmzdPH374oV577TVVVVXppptuajR24cKFOnr0aOh19913h7b5fD7l5uYqKytLlZWVWrJkiUpKSrR8+fKWlgsAAC5B7Vr6hvz8fOXn5ze5LTk5WW63O2zdM888o6FDh6q6ulpdu3YNrU9MTJTL5WpynlWrVunMmTNasWKFHA6HevfuLY/Ho6VLl2ratGktLRkAAFxiov4MzMmTJ2Wz2dSpU6ew9YsXL1aXLl00cOBALVmyRGfPng1tq6io0IgRI+RwOELr8vLyVFVVpS+//DLaJQMAAItr8RWYlvj66681Z84c3XrrrUpKSgqtv+eeezRo0CClpKRo+/btKi4u1tGjR7V06VJJktfrVXZ2dthcaWlpoW2dO3dutC+/3y+/3x9a9vl8kv7+XE4gEIjocTnjgxGdrzV8+xw0LEf63ODi0RNroR/WQj+sJZr9uNA5oxZgAoGAfv7znysYDOrZZ58N21ZUVBT6uV+/fnI4HPr1r3+t0tJSOZ3Oi9pfaWmpFixY0Gh9eXm5EhISLmrO5pQNjeh0rWL9+vVNrv/2LT/EHj2xFvphLfTDWqLRj9ra2gsaF5UA0xBePv30U23evDns6ktThg0bprNnz+qTTz5R9+7d5XK5VFNTEzamYbm552aKi4vDgpHP51NmZqZyc3PPu/+W6lPyTkTnaw37S/LClgOBgNxut8aOHSu73R6jqvBN9MRa6Ie10A9riWY/Gu6gnE/EA0xDePn444/17rvvqkuXLud9j8fjUVxcnFJTUyVJOTk5evDBBxUIBEInxu12q3v37k3ePpIkp9PZ5NUbu90e8ZPrr7NFdL7W0Nw5iMb5wXdDT6yFflgL/bCWaPTjQudrcYA5deqUDh06FFo+cuSIPB6PUlJSlJ6ern/8x3/Uhx9+qHXr1qmurk5er1eSlJKSIofDoYqKCu3cuVOjRo1SYmKiKioqNGvWLN12222hcDJx4kQtWLBAU6dO1Zw5c7R//349+eSTevzxx1taLgAAuAS1OMDs2bNHo0aNCi033LaZMmWKSkpK9Oabb0qSBgwYEPa+d999VyNHjpTT6dTLL7+skpIS+f1+ZWdna9asWWG3f5KTk1VeXq7CwkINHjxYl112mebPn89HqAEAgKSLCDAjR45UMNj8p3DOtU2SBg0apB07dpx3P/369dO2bdtaWh4AAGgD+C4kAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGCcFgeYrVu3aty4ccrIyJDNZtPatWvDtgeDQc2fP1/p6enq0KGDxowZo48//jhszPHjxzVp0iQlJSWpU6dOmjp1qk6dOhU2Zu/evbruuuvUvn17ZWZmqqysrOVHBwAALkktDjCnT59W//79tWzZsia3l5WV6amnntJzzz2nnTt3qmPHjsrLy9PXX38dGjNp0iQdOHBAbrdb69at09atWzVt2rTQdp/Pp9zcXGVlZamyslJLlixRSUmJli9ffhGHCAAALjXtWvqG/Px85efnN7ktGAzqiSee0Ny5c/XTn/5UkvRv//ZvSktL09q1azVhwgR99NFH2rBhg3bv3q0hQ4ZIkp5++mndeOON+v3vf6+MjAytWrVKZ86c0YoVK+RwONS7d295PB4tXbo0LOgAAIC2KaLPwBw5ckRer1djxowJrUtOTtawYcNUUVEhSaqoqFCnTp1C4UWSxowZo7i4OO3cuTM0ZsSIEXI4HKExeXl5qqqq0pdffhnJkgEAgIFafAXmXLxeryQpLS0tbH1aWlpom9frVWpqangR7dopJSUlbEx2dnajORq2de7cudG+/X6//H5/aNnn80mSAoGAAoHAdzmsRpzxwYjO1xq+fQ4aliN9bnDx6Im10A9roR/WEs1+XOicEQ0wsVRaWqoFCxY0Wl9eXq6EhISI7qtsaESnaxXr169vcr3b7W7lSnA+9MRa6Ie10A9riUY/amtrL2hcRAOMy+WSJNXU1Cg9PT20vqamRgMGDAiNOXbsWNj7zp49q+PHj4fe73K5VFNTEzamYblhzLcVFxerqKgotOzz+ZSZmanc3FwlJSV9twP7lj4l70R0vtawvyQvbDkQCMjtdmvs2LGy2+0xqgrfRE+shX5YC/2wlmj2o+EOyvlENMBkZ2fL5XJp06ZNocDi8/m0c+dOTZ8+XZKUk5OjEydOqLKyUoMHD5Ykbd68WfX19Ro2bFhozIMPPqhAIBA6MW63W927d2/y9pEkOZ1OOZ3ORuvtdnvET66/zhbR+VpDc+cgGucH3w09sRb6YS30w1qi0Y8Lna/FD/GeOnVKHo9HHo9H0t8f3PV4PKqurpbNZtPMmTP18MMP680339S+ffs0efJkZWRk6Oabb5Yk9ezZUzfccIPuvPNO7dq1Sx988IFmzJihCRMmKCMjQ5I0ceJEORwOTZ06VQcOHNCaNWv05JNPhl1hAQAAbVeLr8Ds2bNHo0aNCi03hIopU6Zo5cqV+u1vf6vTp09r2rRpOnHihK699lpt2LBB7du3D71n1apVmjFjhkaPHq24uDiNHz9eTz31VGh7cnKyysvLVVhYqMGDB+uyyy7T/Pnz+Qg1AACQdBEBZuTIkQoGm/8Ujs1m08KFC7Vw4cJmx6SkpGj16tXn3E+/fv20bdu2lpYHAADaAL4LCQAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACME/EAc+WVV8pmszV6FRYWSpJGjhzZaNtdd90VNkd1dbUKCgqUkJCg1NRUzZ49W2fPno10qQAAwFDtIj3h7t27VVdXF1rev3+/xo4dq5/97GehdXfeeacWLlwYWk5ISAj9XFdXp4KCArlcLm3fvl1Hjx7V5MmTZbfb9cgjj0S6XAAAYKCIB5jLL788bHnx4sXq1q2bfvzjH4fWJSQkyOVyNfn+8vJyHTx4UBs3blRaWpoGDBigRYsWac6cOSopKZHD4Yh0yQAAwDARDzDfdObMGb300ksqKiqSzWYLrV+1apVeeukluVwujRs3TvPmzQtdhamoqFDfvn2VlpYWGp+Xl6fp06frwIEDGjhwYJP78vv98vv9oWWfzydJCgQCCgQCET0uZ3wwovO1hm+fg4blSJ8bXDx6Yi30w1roh7VEsx8XOmdUA8zatWt14sQJ/eIXvwitmzhxorKyspSRkaG9e/dqzpw5qqqq0muvvSZJ8nq9YeFFUmjZ6/U2u6/S0lItWLCg0fry8vKwW1SRUDY0otO1ivXr1ze53u12t3IlOB96Yi30w1roh7VEox+1tbUXNC6qAeb5559Xfn6+MjIyQuumTZsW+rlv375KT0/X6NGjdfjwYXXr1u2i91VcXKyioqLQss/nU2ZmpnJzc5WUlHTR8zalT8k7EZ2vNewvyQtbDgQCcrvdGjt2rOx2e4yqwjfRE2uhH9ZCP6wlmv1ouINyPlELMJ9++qk2btwYurLSnGHDhkmSDh06pG7dusnlcmnXrl1hY2pqaiSp2edmJMnpdMrpdDZab7fbI35y/XW28w+ymB/MKw9bdsYHVTZUGvi7zZY+nk8WF8S6hFYXjf9ncfHoh7XQD2uJRj8udL6o/R2YF154QampqSooOPcvII/HI0lKT0+XJOXk5Gjfvn06duxYaIzb7VZSUpJ69eoVrXIBAIBBonIFpr6+Xi+88IKmTJmidu3+fxeHDx/W6tWrdeONN6pLly7au3evZs2apREjRqhfv36SpNzcXPXq1Uu33367ysrK5PV6NXfuXBUWFjZ5hQUAALQ9UQkwGzduVHV1te64446w9Q6HQxs3btQTTzyh06dPKzMzU+PHj9fcuXNDY+Lj47Vu3TpNnz5dOTk56tixo6ZMmRL2d2MAAEDbFpUAk5ubq2Cw8UeNMzMztWXLlvO+Pysrq9lPzQAAAPBdSAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOBEPMCUlJbLZbGGvHj16hLZ//fXXKiwsVJcuXfS9731P48ePV01NTdgc1dXVKigoUEJCglJTUzV79mydPXs20qUCAABDtYvGpL1799bGjRv/fyft/n83s2bN0ttvv61XX31VycnJmjFjhm655RZ98MEHkqS6ujoVFBTI5XJp+/btOnr0qCZPniy73a5HHnkkGuUCAADDRCXAtGvXTi6Xq9H6kydP6vnnn9fq1at1/fXXS5JeeOEF9ezZUzt27NDw4cNVXl6ugwcPauPGjUpLS9OAAQO0aNEizZkzRyUlJXI4HNEoGQAAGCQqAebjjz9WRkaG2rdvr5ycHJWWlqpr166qrKxUIBDQmDFjQmN79Oihrl27qqKiQsOHD1dFRYX69u2rtLS00Ji8vDxNnz5dBw4c0MCBA5vcp9/vl9/vDy37fD5JUiAQUCAQiOjxOeODEZ0vFpxxwbD/WlWke2dlDcfalo7ZyuiHtdAPa4lmPy50zogHmGHDhmnlypXq3r27jh49qgULFui6667T/v375fV65XA41KlTp7D3pKWlyev1SpK8Xm9YeGnY3rCtOaWlpVqwYEGj9eXl5UpISPiORxWubGhEp4upRUPqY13COa1fvz7WJbQ6t9sd6xLwDfTDWuiHtUSjH7W1tRc0LuIBJj8/P/Rzv379NGzYMGVlZemVV15Rhw4dIr27kOLiYhUVFYWWfT6fMjMzlZubq6SkpIjuq0/JOxGdLxaccUEtGlKveXvi5K+3xbqcZu0vyYt1Ca0mEAjI7XZr7NixstvtsS6nzaMf1kI/rCWa/Wi4g3I+UbmF9E2dOnXSD3/4Qx06dEhjx47VmTNndOLEibCrMDU1NaFnZlwul3bt2hU2R8OnlJp6rqaB0+mU0+lstN5ut0f85PrrrPsLv6X89TZLH09b/IcqGv/P4uLRD2uhH9YSjX5c6HxR/zswp06d0uHDh5Wenq7BgwfLbrdr06ZNoe1VVVWqrq5WTk6OJCknJ0f79u3TsWPHQmPcbreSkpLUq1evaJcLAAAMEPErML/5zW80btw4ZWVl6fPPP9dDDz2k+Ph43XrrrUpOTtbUqVNVVFSklJQUJSUl6e6771ZOTo6GDx8uScrNzVWvXr10++23q6ysTF6vV3PnzlVhYWGTV1gAAEDbE/EA89e//lW33nqrvvjiC11++eW69tprtWPHDl1++eWSpMcff1xxcXEaP368/H6/8vLy9Ic//CH0/vj4eK1bt07Tp09XTk6OOnbsqClTpmjhwoWRLhUAABgq4gHm5ZdfPuf29u3ba9myZVq2bFmzY7Kystrkp08AAMCF4buQAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxIh5gSktLdfXVVysxMVGpqam6+eabVVVVFTZm5MiRstlsYa+77rorbEx1dbUKCgqUkJCg1NRUzZ49W2fPno10uQAAwEDtIj3hli1bVFhYqKuvvlpnz57VAw88oNzcXB08eFAdO3YMjbvzzju1cOHC0HJCQkLo57q6OhUUFMjlcmn79u06evSoJk+eLLvdrkceeSTSJQMAAMNEPMBs2LAhbHnlypVKTU1VZWWlRowYEVqfkJAgl8vV5Bzl5eU6ePCgNm7cqLS0NA0YMECLFi3SnDlzVFJSIofDEemyAQCAQSIeYL7t5MmTkqSUlJSw9atWrdJLL70kl8ulcePGad68eaGrMBUVFerbt6/S0tJC4/Py8jR9+nQdOHBAAwcObLQfv98vv98fWvb5fJKkQCCgQCAQ0WNyxgcjOl8sOOOCYf+1qkj3zsoajrUtHbOV0Q9roR/WEs1+XOictmAwGLXfYPX19brpppt04sQJvf/++6H1y5cvV1ZWljIyMrR3717NmTNHQ4cO1WuvvSZJmjZtmj799FO98847offU1taqY8eOWr9+vfLz8xvtq6SkRAsWLGi0fvXq1WG3pwAAgHXV1tZq4sSJOnnypJKSkpodF9UrMIWFhdq/f39YeJH+HlAa9O3bV+np6Ro9erQOHz6sbt26XdS+iouLVVRUFFr2+XzKzMxUbm7uOU/AxehT8s75B1mcMy6oRUPqNW9PnPz1tliX06z9JXmxLqHVBAIBud1ujR07Vna7PdbltHn0w1roh7VEsx8Nd1DOJ2oBZsaMGVq3bp22bt2qK6644pxjhw0bJkk6dOiQunXrJpfLpV27doWNqampkaRmn5txOp1yOp2N1tvt9oifXH+ddX/ht5S/3mbp42mL/1BF4/9ZXDz6YS30w1qi0Y8LnS/iH6MOBoOaMWOGXn/9dW3evFnZ2dnnfY/H45EkpaenS5JycnK0b98+HTt2LDTG7XYrKSlJvXr1inTJAADAMBG/AlNYWKjVq1frjTfeUGJiorxeryQpOTlZHTp00OHDh7V69WrdeOON6tKli/bu3atZs2ZpxIgR6tevnyQpNzdXvXr10u23366ysjJ5vV7NnTtXhYWFTV5lAQAAbUvEr8A8++yzOnnypEaOHKn09PTQa82aNZIkh8OhjRs3Kjc3Vz169NB9992n8ePH66233grNER8fr3Xr1ik+Pl45OTm67bbbNHny5LC/GwMAANquiF+BOd+HmjIzM7Vly5bzzpOVlaX169dHqiwAAHAJ4buQAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBx2sW6gHNZtmyZlixZIq/Xq/79++vpp5/W0KFDY10WWtGV978d6xJa7JPFBbEuAQAueZa9ArNmzRoVFRXpoYce0ocffqj+/fsrLy9Px44di3VpAAAgxiwbYJYuXao777xTv/zlL9WrVy8999xzSkhI0IoVK2JdGgAAiDFL3kI6c+aMKisrVVxcHFoXFxenMWPGqKKiosn3+P1++f3+0PLJkyclScePH1cgEIhofe3Ono7ofLHQrj6o2tp6tQvEqa7eFutyLinf/80rF/U+Z1xQcwfWa8CDr8nfyj3ZWTy6VfdngkAgoNraWn3xxRey2+2xLqfNox/WEs1+fPXVV5KkYDB4znGWDDD//d//rbq6OqWlpYWtT0tL03/+5382+Z7S0lItWLCg0frs7Oyo1HgpmBjrAtBIrHpy2WMx2jEANOOrr75ScnJys9stGWAuRnFxsYqKikLL9fX1On78uLp06SKbjSsM3+bz+ZSZmanPPvtMSUlJsS4HoidWQz+shX5YSzT7EQwG9dVXXykjI+Oc4ywZYC677DLFx8erpqYmbH1NTY1cLleT73E6nXI6nWHrOnXqFK0SLxlJSUn8Y2Ax9MRa6Ie10A9riVY/znXlpYElH+J1OBwaPHiwNm3aFFpXX1+vTZs2KScnJ4aVAQAAK7DkFRhJKioq0pQpUzRkyBANHTpUTzzxhE6fPq1f/vKXsS4NAADEmGUDzD/90z/pb3/7m+bPny+v16sBAwZow4YNjR7sxcVxOp166KGHGt12Q+zQE2uhH9ZCP6zFCv2wBc/3OSUAAACLseQzMAAAAOdCgAEAAMYhwAAAAOMQYAAAgHEIMG1MaWmprr76aiUmJio1NVU333yzqqqqYl0W/s/ixYtls9k0c+bMWJfSZv3Xf/2XbrvtNnXp0kUdOnRQ3759tWfPnliX1WbV1dVp3rx5ys7OVocOHdStWzctWrTovN+Tg8jYunWrxo0bp4yMDNlsNq1duzZsezAY1Pz585Wenq4OHTpozJgx+vjjj1ulNgJMG7NlyxYVFhZqx44dcrvdCgQCys3N1enT5n9Bpel2796tf/7nf1a/fv1iXUqb9eWXX+qaa66R3W7Xf/zHf+jgwYN67LHH1Llz51iX1mY9+uijevbZZ/XMM8/oo48+0qOPPqqysjI9/fTTsS6tTTh9+rT69++vZcuWNbm9rKxMTz31lJ577jnt3LlTHTt2VF5enr7++uuo18bHqNu4v/3tb0pNTdWWLVs0YsSIWJfTZp06dUqDBg3SH/7wBz388MMaMGCAnnjiiViX1ebcf//9+uCDD7Rt27ZYl4L/85Of/ERpaWl6/vnnQ+vGjx+vDh066KWXXophZW2PzWbT66+/rptvvlnS36++ZGRk6L777tNvfvMbSdLJkyeVlpamlStXasKECVGthyswbdzJkyclSSkpKTGupG0rLCxUQUGBxowZE+tS2rQ333xTQ4YM0c9+9jOlpqZq4MCB+pd/+ZdYl9Wm/ehHP9KmTZv05z//WZL0pz/9Se+//77y8/NjXBmOHDkir9cb9u9WcnKyhg0bpoqKiqjv37J/iRfRV19fr5kzZ+qaa65Rnz59Yl1Om/Xyyy/rww8/1O7du2NdSpv3l7/8Rc8++6yKior0wAMPaPfu3brnnnvkcDg0ZcqUWJfXJt1///3y+Xzq0aOH4uPjVVdXp9/97neaNGlSrEtr87xeryQ1+gv5aWlpoW3RRIBpwwoLC7V//369//77sS6lzfrss8907733yu12q3379rEup82rr6/XkCFD9Mgjj0iSBg4cqP379+u5554jwMTIK6+8olWrVmn16tXq3bu3PB6PZs6cqYyMDHrSxnELqY2aMWOG1q1bp3fffVdXXHFFrMtpsyorK3Xs2DENGjRI7dq1U7t27bRlyxY99dRTateunerq6mJdYpuSnp6uXr16ha3r2bOnqqurY1QRZs+erfvvv18TJkxQ3759dfvtt2vWrFkqLS2NdWltnsvlkiTV1NSEra+pqQltiyYCTBsTDAY1Y8YMvf7669q8ebOys7NjXVKbNnr0aO3bt08ejyf0GjJkiCZNmiSPx6P4+PhYl9imXHPNNY3+rMCf//xnZWVlxagi1NbWKi4u/FdVfHy86uvrY1QRGmRnZ8vlcmnTpk2hdT6fTzt37lROTk7U988tpDamsLBQq1ev1htvvKHExMTQfcrk5GR16NAhxtW1PYmJiY2eP+rYsaO6dOnCc0kxMGvWLP3oRz/SI488op///OfatWuXli9fruXLl8e6tDZr3Lhx+t3vfqeuXbuqd+/e+uMf/6ilS5fqjjvuiHVpbcKpU6d06NCh0PKRI0fk8XiUkpKirl27aubMmXr44Yf1gx/8QNnZ2Zo3b54yMjJCn1SKqiDaFElNvl544YVYl4b/8+Mf/zh47733xrqMNuutt94K9unTJ+h0OoM9evQILl++PNYltWk+ny947733Brt27Rps37598Kqrrgo++OCDQb/fH+vS2oR33323yd8ZU6ZMCQaDwWB9fX1w3rx5wbS0tKDT6QyOHj06WFVV1Sq18XdgAACAcXgGBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADj/C9cyqmXScq1xwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series([len(group) for group in groups]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "fb9b7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 199\n",
      "4 218\n",
      "10 508\n",
      "3 775\n",
      "5 1014\n",
      "3 1037\n",
      "3 1208\n",
      "3 1315\n",
      "3 1366\n",
      "3 1405\n",
      "4 1427\n",
      "3 1441\n",
      "3 1455\n",
      "3 1457\n",
      "3 1479\n",
      "3 1488\n",
      "3 1512\n",
      "4 1534\n",
      "3 1554\n",
      "5 1598\n",
      "4 1643\n",
      "3 1649\n",
      "3 1728\n",
      "3 1814\n",
      "3 1916\n",
      "3 1983\n",
      "3 2007\n",
      "3 2186\n",
      "3 2204\n",
      "4 2245\n",
      "4 2352\n",
      "3 2399\n",
      "3 2402\n",
      "6 2407\n",
      "4 2481\n",
      "3 2489\n",
      "3 2551\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(groups)):\n",
    "    if len(groups[x]) > 2:\n",
    "        print(len(groups[x]), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "90c979c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[241, 1034, 1037, 2152],\n",
       " [264, 268, 265, 267],\n",
       " [623, 624, 1348, 1615, 2388, 2385, 2314, 2318, 2194, 2197],\n",
       " [966, 969, 979],\n",
       " [1267, 1983, 1988, 2672, 2674],\n",
       " [1293, 2391, 1605],\n",
       " [1509, 1512, 1348],\n",
       " [1657, 1661, 1674],\n",
       " [1727, 1731, 1753],\n",
       " [1782, 2702, 2705],\n",
       " [1808, 2729, 2616, 2364],\n",
       " [1824, 1828, 2149],\n",
       " [1844, 1853, 1855],\n",
       " [1846, 1850, 1847],\n",
       " [1880, 1883, 3368],\n",
       " [1891, 1912, 1894],\n",
       " [1925, 1938, 1942],\n",
       " [1954, 1989, 1957, 1984],\n",
       " [1985, 1990, 2019],\n",
       " [2046, 2937, 3098, 3095, 3528],\n",
       " [2102, 2105, 2355, 2217],\n",
       " [2110, 2178, 2181],\n",
       " [2213, 2217, 2355],\n",
       " [2333, 2347, 2337],\n",
       " [2466, 2470, 2479],\n",
       " [2556, 2566, 2569],\n",
       " [2591, 2595, 2597],\n",
       " [2824, 3000, 2997],\n",
       " [2847, 2857, 2854],\n",
       " [2899, 3120, 3424, 3243],\n",
       " [3038, 3041, 3124, 3125],\n",
       " [3093, 3108, 3105],\n",
       " [3097, 3100, 3107],\n",
       " [3106, 3109, 3133, 3135, 3152, 3167],\n",
       " [3208, 3211, 3227, 3230],\n",
       " [3219, 3222, 3254],\n",
       " [3309, 3320, 3312]]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[group for group in groups if len(group) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "48d09628",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_at_i = db.index.reconstruct(1348)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b6383ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.5435183e-02, -1.6280942e-02, -9.6684992e-03, ...,\n",
       "        1.4441899e-02,  5.8949114e-05, -8.2718907e-05], dtype=float32)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_at_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "395910f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content=\"For submissions, if we have code that runs poc for each bug, how should we submit it? I'm thinking just adding a zip file to the submission is probably easiest but I could also share my private github repo with someone.\\n\", metadata={'answer': ' How large is the poc?'}),\n",
       "  0.0)]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score_by_vector(db.index.reconstruct(30), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e811c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_at_index(array):\n",
    "    return [db.similarity_search_by_vector(db.index.reconstruct(x), k=1)[0] for x in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "072d890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='How can we apply to become a certified warden?\\n', metadata={'answer': ' You can apply to become a certified warden by filling this form: https://code4rena.com/certified-contributor-application.'}),\n",
       " Document(page_content='How can we apply to become a certified warden?\\n', metadata={'answer': ' You can apply to become a certified warden by filling this form: https://code4rena.com/certified-contributor-application.'}),\n",
       " Document(page_content='How to become a certified warden?\\n', metadata={'answer': ' You can become a certified warden by following the process outlined in this link: https://docs.code4rena.com/roles/certified-contributors'}),\n",
       " Document(page_content='How does one become a certified warden?\\n', metadata={'answer': ' Read the documentation, you need to complete a KYC (Know Your Customer) process.'}),\n",
       " Document(page_content='How does one become a certified warden?\\n', metadata={'answer': ' Read the documentation, you need to complete a KYC (Know Your Customer) process.'}),\n",
       " Document(page_content='How does one become a certified warden? \\n', metadata={'answer': ' You can read more about becoming certified here: https://docs.code4rena.com/roles/certified-contributors.'}),\n",
       " Document(page_content='What is the process of becoming a certified warden?\\n', metadata={'answer': ''}),\n",
       " Document(page_content='What is the process of becoming a certified warden?\\n', metadata={'answer': ''}),\n",
       " Document(page_content='What do you need to do to become a certified warden?\\n', metadata={'answer': ' You can find information on becoming a certified warden at this link: https://docs.code4rena.com/roles/certified-contributors'}),\n",
       " Document(page_content='What do you need to do to become a certified warden?\\n', metadata={'answer': ' You can find information on becoming a certified warden at this link: https://docs.code4rena.com/roles/certified-contributors'})]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_at_index([623, 624, 1348, 1615, 2388, 2385, 2314, 2318, 2194, 2197])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "bf08eb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Will there be a mitigation review for Chainlink CCIP as mentioned in the original RSVP message?\\n', metadata={'answer': ' Yes, this is still planned.'}),\n",
       "  0.0),\n",
       " (Document(page_content='Will there be a mitigation review for Chainlink CCIP as mentioned in the original RSVP message?\\n', metadata={'answer': ' Yes this is still planned.'}),\n",
       "  1.5012198e-05),\n",
       " (Document(page_content='Will there be a mitigation review for Chainlink CCIP as mentioned in the original RSVP message https://discord.com/channels/810916927919620096/958800160870240286/1111007546183012382?\\n', metadata={'answer': ''}),\n",
       "  0.03938529),\n",
       " (Document(page_content='Will the mitigation review be limited to the top wardens of the corresponding initial contest?\\n', metadata={'answer': ' Yes, correct.'}),\n",
       "  0.34130633),\n",
       " (Document(page_content='Is the xETH - Mitigation Review Open for all the certificates users?\\n', metadata={'answer': ' Hi there. xETH Mit Rev. will be open to those who participated in the original Invitational audit.'}),\n",
       "  0.35897225),\n",
       " (Document(page_content='Is mitigation review limited to the top wardens of the corresponding initial contest?\\n', metadata={'answer': ' Yes, mitigation review will be limited to the top wardens of the corresponding initial contest.'}),\n",
       "  0.37170786),\n",
       " (Document(page_content='What is Mitigation review contest?\\n', metadata={'answer': ' Sometimes projects want to invite the top wardens back after the contests to review bug mitigations.'}),\n",
       "  0.37368223),\n",
       " (Document(page_content='What is Mitigation review contest?\\n', metadata={'answer': ''}),\n",
       "  0.3737302),\n",
       " (Document(page_content='Is the \"Mitigation review contest\" only in GoGoPool contest or will it be on any future contest?\\n', metadata={'answer': ' Yes, there will be more contests with this structure going forward: an initial audit prize pool + a mitigation review pool.'}),\n",
       "  0.37432125),\n",
       " (Document(page_content='What does a general Mitigation Review process consist of and who are eligible to participate?\\n', metadata={'answer': ' N/A'}),\n",
       "  0.37906706)]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search_with_score_by_vector(db.index.reconstruct(3312), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a759875",
   "metadata": {},
   "source": [
    "### FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "f9882c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_enhance = '''\n",
    "You are an expert at constructing frequently asked question documents. We are trying to improve a FAQ using information present in our Discord chatroom. Our company is CodeArena (C4), a company that helps other companies receive audits of their smart contracts.  \n",
    "\n",
    "Your task will be to create a high quality question and answer pair. Below you are given several related questions we've seen and an answer for each (if we currently have one). After that we include a collection of observations from the chat history in our questions channel, please use these observations to improve your answer.\n",
    "\n",
    "\n",
    "Questions and Answers:\n",
    "{}\n",
    "\n",
    "Observations from the chat:\n",
    "{}\n",
    "\n",
    "Please respond with just one improved question and answer. When a link is relevant to the answer, ALWAYS include the link. Try to include all information of value from the observations in the answer, but you can omit information not related to the question's topic. It's ok to express uncertainity. Adding additional context, definitions, or insights from the observations is welcome.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c1f49ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_a_string = \"\"\n",
    "for x in get_at_index([623, 624, 1348, 1615, 2388, 2385, 2314, 2318, 2194, 2197]):\n",
    "    q_a_string += \" \".join([\"Q:\", x.page_content.rstrip(\"\\n\"), \"\\n\" \"A:\", x.metadata['answer'], \"\\n\"])\n",
    "    q_a_string += \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "1b4ece10",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_ = facts_db.similarity_search(\"How can we apply to become a certified warden?\" , k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "984ee3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_to_include = '\\n'.join([x.page_content for x in facts_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "b99a629c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "140fc118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0\n",
      "Working on 1\n",
      "Working on 2\n",
      "Working on 3\n",
      "Working on 4\n",
      "Working on 5\n",
      "Working on 6\n",
      "Working on 7\n",
      "Working on 8\n",
      "Working on 9\n",
      "Working on 10\n",
      "Working on 11\n",
      "Working on 12\n",
      "Working on 13\n",
      "Working on 14\n",
      "Working on 15\n",
      "Working on 16\n",
      "Working on 17\n",
      "Working on 18\n",
      "Working on 19\n",
      "Working on 20\n",
      "Working on 21\n",
      "Working on 22\n",
      "Working on 23\n",
      "Working on 24\n",
      "Working on 25\n",
      "Working on 26\n",
      "Working on 27\n",
      "Working on 28\n",
      "Working on 29\n",
      "Working on 30\n",
      "Working on 31\n",
      "Working on 32\n",
      "Working on 33\n",
      "Working on 34\n",
      "Working on 35\n",
      "Working on 36\n",
      "Working on 37\n",
      "Working on 38\n",
      "Working on 39\n",
      "Working on 40\n",
      "Working on 41\n",
      "Working on 42\n",
      "Working on 43\n",
      "Working on 44\n",
      "Working on 45\n",
      "Working on 46\n",
      "Working on 47\n",
      "Working on 48\n",
      "Working on 49\n",
      "Working on 50\n",
      "Working on 51\n",
      "Working on 52\n",
      "Working on 53\n",
      "Working on 54\n",
      "Working on 55\n",
      "Working on 56\n",
      "Working on 57\n",
      "Working on 58\n",
      "Working on 59\n",
      "Working on 60\n",
      "Working on 61\n",
      "Working on 62\n",
      "Working on 63\n",
      "Working on 64\n",
      "Working on 65\n",
      "Working on 66\n",
      "Working on 67\n",
      "Working on 68\n",
      "Working on 69\n",
      "Working on 70\n",
      "Working on 71\n",
      "Working on 72\n",
      "Working on 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 74\n",
      "Working on 75\n",
      "Working on 76\n",
      "Working on 77\n",
      "Working on 78\n",
      "Working on 79\n",
      "Working on 80\n",
      "Working on 81\n",
      "Working on 82\n",
      "Working on 83\n",
      "Working on 84\n",
      "Working on 85\n",
      "Working on 86\n",
      "Working on 87\n",
      "Working on 88\n",
      "Working on 89\n",
      "Working on 90\n",
      "Working on 91\n",
      "Working on 92\n",
      "Working on 93\n",
      "Working on 94\n",
      "Working on 95\n",
      "Working on 96\n",
      "Working on 97\n",
      "Working on 98\n",
      "Working on 99\n",
      "Working on 100\n",
      "Working on 101\n",
      "Working on 102\n",
      "Working on 103\n",
      "Working on 104\n",
      "Working on 105\n",
      "Working on 106\n",
      "Working on 107\n",
      "Working on 108\n",
      "Working on 109\n",
      "Working on 110\n",
      "Working on 111\n",
      "Working on 112\n",
      "Working on 113\n",
      "Working on 114\n",
      "Working on 115\n",
      "Working on 116\n",
      "Working on 117\n",
      "Working on 118\n",
      "Working on 119\n",
      "Working on 120\n",
      "Working on 121\n",
      "Working on 122\n",
      "Working on 123\n",
      "Working on 124\n",
      "Working on 125\n",
      "Working on 126\n",
      "Working on 127\n",
      "Working on 128\n",
      "Working on 129\n",
      "Working on 130\n",
      "Working on 131\n",
      "Working on 132\n",
      "Working on 133\n",
      "Working on 134\n",
      "Working on 135\n",
      "Working on 136\n",
      "Working on 137\n",
      "Working on 138\n",
      "Working on 139\n",
      "Working on 140\n",
      "Working on 141\n",
      "Working on 142\n",
      "Working on 143\n",
      "Working on 144\n",
      "Working on 145\n",
      "Working on 146\n",
      "Working on 147\n",
      "Working on 148\n",
      "Working on 149\n",
      "Working on 150\n",
      "Working on 151\n",
      "Working on 152\n",
      "Working on 153\n",
      "Working on 154\n",
      "Working on 155\n",
      "Working on 156\n",
      "Working on 157\n",
      "Working on 158\n",
      "Working on 159\n",
      "Working on 160\n",
      "Working on 161\n",
      "Working on 162\n",
      "Working on 163\n",
      "Working on 164\n",
      "Working on 165\n",
      "Working on 166\n",
      "Working on 167\n",
      "Working on 168\n",
      "Working on 169\n",
      "Working on 170\n",
      "Working on 171\n",
      "Working on 172\n",
      "Working on 173\n",
      "Working on 174\n",
      "Working on 175\n",
      "Working on 176\n",
      "Working on 177\n",
      "Working on 178\n",
      "Working on 179\n",
      "Working on 180\n",
      "Working on 181\n",
      "Working on 182\n",
      "Working on 183\n",
      "Working on 184\n",
      "Working on 185\n",
      "Working on 186\n",
      "Working on 187\n",
      "Working on 188\n",
      "Working on 189\n",
      "Working on 190\n",
      "Working on 191\n",
      "Working on 192\n",
      "Working on 193\n",
      "Working on 194\n",
      "Working on 195\n",
      "Working on 196\n",
      "Working on 197\n",
      "Working on 198\n",
      "Working on 199\n",
      "Working on 200\n",
      "Working on 201\n",
      "Working on 202\n",
      "Working on 203\n",
      "Working on 204\n",
      "Working on 205\n",
      "Working on 206\n",
      "Working on 207\n",
      "Working on 208\n",
      "Working on 209\n",
      "Working on 210\n",
      "Working on 211\n",
      "Working on 212\n",
      "Working on 213\n",
      "Working on 214\n",
      "Working on 215\n",
      "Working on 216\n",
      "Working on 217\n",
      "Working on 218\n",
      "Working on 219\n",
      "Working on 220\n",
      "Working on 221\n",
      "Working on 222\n",
      "Working on 223\n",
      "Working on 224\n",
      "Working on 225\n",
      "Working on 226\n",
      "Working on 227\n",
      "Working on 228\n",
      "Working on 229\n",
      "Working on 230\n",
      "Working on 231\n",
      "Working on 232\n",
      "Working on 233\n",
      "Working on 234\n",
      "Working on 235\n",
      "Working on 236\n",
      "Working on 237\n",
      "Working on 238\n",
      "Working on 239\n",
      "Working on 240\n",
      "Working on 241\n",
      "Working on 242\n",
      "Working on 243\n",
      "Working on 244\n",
      "Working on 245\n",
      "Working on 246\n",
      "Working on 247\n",
      "Working on 248\n",
      "Working on 249\n",
      "Working on 250\n",
      "Working on 251\n",
      "Working on 252\n",
      "Working on 253\n",
      "Working on 254\n",
      "Working on 255\n",
      "Working on 256\n",
      "Working on 257\n",
      "Working on 258\n",
      "Working on 259\n",
      "Working on 260\n",
      "Working on 261\n",
      "Working on 262\n",
      "Working on 263\n",
      "Working on 264\n",
      "Working on 265\n",
      "Working on 266\n",
      "Working on 267\n",
      "Working on 268\n",
      "Working on 269\n",
      "Working on 270\n",
      "Working on 271\n",
      "Working on 272\n",
      "Working on 273\n",
      "Working on 274\n",
      "Working on 275\n",
      "Working on 276\n",
      "Working on 277\n",
      "Working on 278\n",
      "Working on 279\n",
      "Working on 280\n",
      "Working on 281\n",
      "Working on 282\n",
      "Working on 283\n",
      "Working on 284\n",
      "Working on 285\n",
      "Working on 286\n",
      "Working on 287\n",
      "Working on 288\n",
      "Working on 289\n",
      "Working on 290\n",
      "Working on 291\n",
      "Working on 292\n",
      "Working on 293\n",
      "Working on 294\n",
      "Working on 295\n",
      "Working on 296\n",
      "Working on 297\n",
      "Working on 298\n",
      "Working on 299\n",
      "Working on 300\n",
      "Working on 301\n",
      "Working on 302\n",
      "Working on 303\n",
      "Working on 304\n",
      "Working on 305\n",
      "Working on 306\n",
      "Working on 307\n",
      "Working on 308\n",
      "Working on 309\n",
      "Working on 310\n",
      "Working on 311\n",
      "Working on 312\n",
      "Working on 313\n",
      "Working on 314\n",
      "Working on 315\n",
      "Working on 316\n",
      "Working on 317\n",
      "Working on 318\n",
      "Working on 319\n",
      "Working on 320\n",
      "Working on 321\n",
      "Working on 322\n",
      "Working on 323\n",
      "Working on 324\n",
      "Working on 325\n",
      "Working on 326\n",
      "Working on 327\n",
      "Working on 328\n",
      "Working on 329\n",
      "Working on 330\n",
      "Working on 331\n",
      "Working on 332\n",
      "Working on 333\n",
      "Working on 334\n",
      "Working on 335\n",
      "Working on 336\n",
      "Working on 337\n",
      "Working on 338\n",
      "Working on 339\n",
      "Working on 340\n",
      "Working on 341\n",
      "Working on 342\n",
      "Working on 343\n",
      "Working on 344\n",
      "Working on 345\n",
      "Working on 346\n",
      "Working on 347\n",
      "Working on 348\n",
      "Working on 349\n",
      "Working on 350\n",
      "Working on 351\n",
      "Working on 352\n",
      "Working on 353\n",
      "Working on 354\n",
      "Working on 355\n",
      "Working on 356\n",
      "Working on 357\n",
      "Working on 358\n",
      "Working on 359\n",
      "Working on 360\n",
      "Working on 361\n",
      "Working on 362\n",
      "Working on 363\n",
      "Working on 364\n",
      "Working on 365\n",
      "Working on 366\n",
      "Working on 367\n",
      "Working on 368\n",
      "Working on 369\n",
      "Working on 370\n",
      "Working on 371\n",
      "Working on 372\n",
      "Working on 373\n",
      "Working on 374\n",
      "Working on 375\n",
      "Working on 376\n",
      "Working on 377\n",
      "Working on 378\n",
      "Working on 379\n",
      "Working on 380\n",
      "Working on 381\n",
      "Working on 382\n",
      "Working on 383\n",
      "Working on 384\n",
      "Working on 385\n",
      "Working on 386\n",
      "Working on 387\n",
      "Working on 388\n",
      "Working on 389\n",
      "Working on 390\n",
      "Working on 391\n",
      "Working on 392\n",
      "Working on 393\n",
      "Working on 394\n",
      "Working on 395\n",
      "Working on 396\n",
      "Working on 397\n",
      "Working on 398\n",
      "Working on 399\n",
      "Working on 400\n",
      "Working on 401\n",
      "Working on 402\n",
      "Working on 403\n",
      "Working on 404\n",
      "Working on 405\n",
      "Working on 406\n",
      "Working on 407\n",
      "Working on 408\n",
      "Working on 409\n",
      "Working on 410\n",
      "Working on 411\n",
      "Working on 412\n",
      "Working on 413\n",
      "Working on 414\n",
      "Working on 415\n",
      "Working on 416\n",
      "Working on 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 418\n",
      "Working on 419\n",
      "Working on 420\n",
      "Working on 421\n",
      "Working on 422\n",
      "Working on 423\n",
      "Working on 424\n",
      "Working on 425\n",
      "Working on 426\n",
      "Working on 427\n",
      "Working on 428\n",
      "Working on 429\n",
      "Working on 430\n",
      "Working on 431\n",
      "Working on 432\n",
      "Working on 433\n",
      "Working on 434\n",
      "Working on 435\n",
      "Working on 436\n",
      "Working on 437\n",
      "Working on 438\n",
      "Working on 439\n",
      "Working on 440\n",
      "Working on 441\n",
      "Working on 442\n",
      "Working on 443\n",
      "Working on 444\n",
      "Working on 445\n",
      "Working on 446\n",
      "Working on 447\n",
      "Working on 448\n",
      "Working on 449\n",
      "Working on 450\n",
      "Working on 451\n",
      "Working on 452\n",
      "Working on 453\n",
      "Working on 454\n",
      "Working on 455\n",
      "Working on 456\n",
      "Working on 457\n",
      "Working on 458\n",
      "Working on 459\n",
      "Working on 460\n",
      "Working on 461\n",
      "Working on 462\n",
      "Working on 463\n",
      "Working on 464\n",
      "Working on 465\n",
      "Working on 466\n",
      "Working on 467\n",
      "Working on 468\n",
      "Working on 469\n",
      "Working on 470\n",
      "Working on 471\n",
      "Working on 472\n",
      "Working on 473\n",
      "Working on 474\n",
      "Working on 475\n",
      "Working on 476\n",
      "Working on 477\n",
      "Working on 478\n",
      "Working on 479\n",
      "Working on 480\n",
      "Working on 481\n",
      "Working on 482\n",
      "Working on 483\n",
      "Working on 484\n",
      "Working on 485\n",
      "Working on 486\n",
      "Working on 487\n",
      "Working on 488\n",
      "Working on 489\n",
      "Working on 490\n",
      "Working on 491\n",
      "Working on 492\n",
      "Working on 493\n",
      "Working on 494\n",
      "Working on 495\n",
      "Working on 496\n",
      "Working on 497\n",
      "Working on 498\n",
      "Working on 499\n",
      "Working on 500\n",
      "Working on 501\n",
      "Working on 502\n",
      "Working on 503\n",
      "Working on 504\n",
      "Working on 505\n",
      "Working on 506\n",
      "Working on 507\n",
      "Working on 508\n",
      "Working on 509\n",
      "Working on 510\n",
      "Working on 511\n",
      "Working on 512\n",
      "Working on 513\n",
      "Working on 514\n",
      "Working on 515\n",
      "Working on 516\n",
      "Working on 517\n",
      "Working on 518\n",
      "Working on 519\n",
      "Working on 520\n",
      "Working on 521\n",
      "Working on 522\n",
      "Working on 523\n",
      "Working on 524\n",
      "Working on 525\n",
      "Working on 526\n",
      "Working on 527\n",
      "Working on 528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 529\n",
      "Working on 530\n",
      "Working on 531\n",
      "Working on 532\n",
      "Working on 533\n",
      "Working on 534\n",
      "Working on 535\n",
      "Working on 536\n",
      "Working on 537\n",
      "Working on 538\n",
      "Working on 539\n",
      "Working on 540\n",
      "Working on 541\n",
      "Working on 542\n",
      "Working on 543\n",
      "Working on 544\n",
      "Working on 545\n",
      "Working on 546\n",
      "Working on 547\n",
      "Working on 548\n",
      "Working on 549\n",
      "Working on 550\n",
      "Working on 551\n",
      "Working on 552\n",
      "Working on 553\n",
      "Working on 554\n",
      "Working on 555\n",
      "Working on 556\n",
      "Working on 557\n",
      "Working on 558\n",
      "Working on 559\n",
      "Working on 560\n",
      "Working on 561\n",
      "Working on 562\n",
      "Working on 563\n",
      "Working on 564\n",
      "Working on 565\n",
      "Working on 566\n",
      "Working on 567\n",
      "Working on 568\n",
      "Working on 569\n",
      "Working on 570\n",
      "Working on 571\n",
      "Working on 572\n",
      "Working on 573\n",
      "Working on 574\n",
      "Working on 575\n",
      "Working on 576\n",
      "Working on 577\n",
      "Working on 578\n",
      "Working on 579\n",
      "Working on 580\n",
      "Working on 581\n",
      "Working on 582\n",
      "Working on 583\n",
      "Working on 584\n",
      "Working on 585\n",
      "Working on 586\n",
      "Working on 587\n",
      "Working on 588\n",
      "Working on 589\n",
      "Working on 590\n",
      "Working on 591\n",
      "Working on 592\n",
      "Working on 593\n",
      "Working on 594\n",
      "Working on 595\n",
      "Working on 596\n",
      "Working on 597\n",
      "Working on 598\n",
      "Working on 599\n",
      "Working on 600\n",
      "Working on 601\n",
      "Working on 602\n",
      "Working on 603\n",
      "Working on 604\n",
      "Working on 605\n",
      "Working on 606\n",
      "Working on 607\n",
      "Working on 608\n",
      "Working on 609\n",
      "Working on 610\n",
      "Working on 611\n",
      "Working on 612\n",
      "Working on 613\n",
      "Working on 614\n",
      "Working on 615\n",
      "Working on 616\n",
      "Working on 617\n",
      "Working on 618\n",
      "Working on 619\n",
      "Working on 620\n",
      "Working on 621\n",
      "Working on 622\n",
      "Working on 623\n",
      "Working on 624\n",
      "Working on 625\n",
      "Working on 626\n",
      "Working on 627\n",
      "Working on 628\n",
      "Working on 629\n",
      "Working on 630\n",
      "Working on 631\n",
      "Working on 632\n",
      "Working on 633\n",
      "Working on 634\n",
      "Working on 635\n",
      "Working on 636\n",
      "Working on 637\n",
      "Working on 638\n",
      "Working on 639\n",
      "Working on 640\n",
      "Working on 641\n",
      "Working on 642\n",
      "Working on 643\n",
      "Working on 644\n",
      "Working on 645\n",
      "Working on 646\n",
      "Working on 647\n",
      "Working on 648\n",
      "Working on 649\n",
      "Working on 650\n",
      "Working on 651\n",
      "Working on 652\n",
      "Working on 653\n",
      "Working on 654\n",
      "Working on 655\n",
      "Working on 656\n",
      "Working on 657\n",
      "Working on 658\n",
      "Working on 659\n",
      "Working on 660\n",
      "Working on 661\n",
      "Working on 662\n",
      "Working on 663\n",
      "Working on 664\n",
      "Working on 665\n",
      "Working on 666\n",
      "Working on 667\n",
      "Working on 668\n",
      "Working on 669\n",
      "Working on 670\n",
      "Working on 671\n",
      "Working on 672\n",
      "Working on 673\n",
      "Working on 674\n",
      "Working on 675\n",
      "Working on 676\n",
      "Working on 677\n",
      "Working on 678\n",
      "Working on 679\n",
      "Working on 680\n",
      "Working on 681\n",
      "Working on 682\n",
      "Working on 683\n",
      "Working on 684\n",
      "Working on 685\n",
      "Working on 686\n",
      "Working on 687\n",
      "Working on 688\n",
      "Working on 689\n",
      "Working on 690\n",
      "Working on 691\n",
      "Working on 692\n",
      "Working on 693\n",
      "Working on 694\n",
      "Working on 695\n",
      "Working on 696\n",
      "Working on 697\n",
      "Working on 698\n",
      "Working on 699\n",
      "Working on 700\n",
      "Working on 701\n",
      "Working on 702\n",
      "Working on 703\n",
      "Working on 704\n",
      "Working on 705\n",
      "Working on 706\n",
      "Working on 707\n",
      "Working on 708\n",
      "Working on 709\n",
      "Working on 710\n",
      "Working on 711\n",
      "Working on 712\n",
      "Working on 713\n",
      "Working on 714\n",
      "Working on 715\n",
      "Working on 716\n",
      "Working on 717\n",
      "Working on 718\n",
      "Working on 719\n",
      "Working on 720\n",
      "Working on 721\n",
      "Working on 722\n",
      "Working on 723\n",
      "Working on 724\n",
      "Working on 725\n",
      "Working on 726\n",
      "Working on 727\n",
      "Working on 728\n",
      "Working on 729\n",
      "Working on 730\n",
      "Working on 731\n",
      "Working on 732\n",
      "Working on 733\n",
      "Working on 734\n",
      "Working on 735\n",
      "Working on 736\n",
      "Working on 737\n",
      "Working on 738\n",
      "Working on 739\n",
      "Working on 740\n",
      "Working on 741\n",
      "Working on 742\n",
      "Working on 743\n",
      "Working on 744\n",
      "Working on 745\n",
      "Working on 746\n",
      "Working on 747\n",
      "Working on 748\n",
      "Working on 749\n",
      "Working on 750\n",
      "Working on 751\n",
      "Working on 752\n",
      "Working on 753\n",
      "Working on 754\n",
      "Working on 755\n",
      "Working on 756\n",
      "Working on 757\n",
      "Working on 758\n",
      "Working on 759\n",
      "Working on 760\n",
      "Working on 761\n",
      "Working on 762\n",
      "Working on 763\n",
      "Working on 764\n",
      "Working on 765\n",
      "Working on 766\n",
      "Working on 767\n",
      "Working on 768\n",
      "Working on 769\n",
      "Working on 770\n",
      "Working on 771\n",
      "Working on 772\n",
      "Working on 773\n",
      "Working on 774\n",
      "Working on 775\n",
      "Working on 776\n",
      "Working on 777\n",
      "Working on 778\n",
      "Working on 779\n",
      "Working on 780\n",
      "Working on 781\n",
      "Working on 782\n",
      "Working on 783\n",
      "Working on 784\n",
      "Working on 785\n",
      "Working on 786\n",
      "Working on 787\n",
      "Working on 788\n",
      "Working on 789\n",
      "Working on 790\n",
      "Working on 791\n",
      "Working on 792\n",
      "Working on 793\n",
      "Working on 794\n",
      "Working on 795\n",
      "Working on 796\n",
      "Working on 797\n",
      "Working on 798\n",
      "Working on 799\n",
      "Working on 800\n",
      "Working on 801\n",
      "Working on 802\n",
      "Working on 803\n",
      "Working on 804\n",
      "Working on 805\n",
      "Working on 806\n",
      "Working on 807\n",
      "Working on 808\n",
      "Working on 809\n",
      "Working on 810\n",
      "Working on 811\n",
      "Working on 812\n",
      "Working on 813\n",
      "Working on 814\n",
      "Working on 815\n",
      "Working on 816\n",
      "Working on 817\n",
      "Working on 818\n",
      "Working on 819\n",
      "Working on 820\n",
      "Working on 821\n",
      "Working on 822\n",
      "Working on 823\n",
      "Working on 824\n",
      "Working on 825\n",
      "Working on 826\n",
      "Working on 827\n",
      "Working on 828\n",
      "Working on 829\n",
      "Working on 830\n",
      "Working on 831\n",
      "Working on 832\n",
      "Working on 833\n",
      "Working on 834\n",
      "Working on 835\n",
      "Working on 836\n",
      "Working on 837\n",
      "Working on 838\n",
      "Working on 839\n",
      "Working on 840\n",
      "Working on 841\n",
      "Working on 842\n",
      "Working on 843\n",
      "Working on 844\n",
      "Working on 845\n",
      "Working on 846\n",
      "Working on 847\n",
      "Working on 848\n",
      "Working on 849\n",
      "Working on 850\n",
      "Working on 851\n",
      "Working on 852\n",
      "Working on 853\n",
      "Working on 854\n",
      "Working on 855\n",
      "Working on 856\n",
      "Working on 857\n",
      "Working on 858\n",
      "Working on 859\n",
      "Working on 860\n",
      "Working on 861\n",
      "Working on 862\n",
      "Working on 863\n",
      "Working on 864\n",
      "Working on 865\n",
      "Working on 866\n",
      "Working on 867\n",
      "Working on 868\n",
      "Working on 869\n",
      "Working on 870\n",
      "Working on 871\n",
      "Working on 872\n",
      "Working on 873\n",
      "Working on 874\n",
      "Working on 875\n",
      "Working on 876\n",
      "Working on 877\n",
      "Working on 878\n",
      "Working on 879\n",
      "Working on 880\n",
      "Working on 881\n",
      "Working on 882\n",
      "Working on 883\n",
      "Working on 884\n",
      "Working on 885\n",
      "Working on 886\n",
      "Working on 887\n",
      "Working on 888\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[366], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m prompt \u001b[38;5;241m=\u001b[39m q_a_enhance\u001b[38;5;241m.\u001b[39mformat(q_a_string, facts_to_include)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#print(prompt)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m qs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_messages([HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt)])   \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#print(qs.content)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m new_qa_pairs\u001b[38;5;241m.\u001b[39mappend(qs\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:601\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(messages, stop\u001b[38;5;241m=\u001b[39m_stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 551\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    552\u001b[0m         [messages], stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    553\u001b[0m     )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    308\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    310\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    314\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 299\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[1;32m    300\u001b[0m                 m,\n\u001b[1;32m    301\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    302\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    303\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    304\u001b[0m             )\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[1;32m    447\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    448\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:345\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    344\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    346\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _completion_with_retry(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/langchain/chat_models/openai.py:276\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[38;5;241m=\u001b[39mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m _thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mrequest_timeout \u001b[38;5;28;01mif\u001b[39;00m request_timeout \u001b[38;5;28;01melse\u001b[39;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[38;5;241m=\u001b[39m_thread_context\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/ollama-test/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_qa_pairs = []\n",
    "for x in range(len(dc)):\n",
    "    try:\n",
    "        print(\"Working on\", x)\n",
    "        group = groups[x]\n",
    "        q_a_string = \"\"\n",
    "        docs = get_at_index(group)\n",
    "        for x in docs:\n",
    "            q_a_string += \" \".join([\"Q:\", x.page_content.rstrip(\"\\n\"), \"\\n\" \"A:\", x.metadata['answer'], \"\\n\"])\n",
    "            q_a_string += \"\\n\"\n",
    "        facts_ = facts_db.similarity_search(docs[0].page_content , k=30)\n",
    "        facts_to_include = '\\n'.join([x.page_content for x in facts_])\n",
    "        prompt = q_a_enhance.format(q_a_string, facts_to_include)\n",
    "        #print(prompt)\n",
    "        qs = model.predict_messages([HumanMessage(content=prompt)])   \n",
    "        #print(qs.content)\n",
    "        new_qa_pairs.append(qs.content)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "00cc0fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "888"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "56f3f86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Q: How can I best provide code for a test or proof of concept in my CodeArena submission?\\nA: The best method to provide code for a test or proof of concept in your CodeArena submission depends on various factors such as the length of the code, potential exposure of vulnerabilities, and the complexity of the code's setup. If the code is not too lengthy, you can add it directly to the report under the 'Proof of Concept' section. You can also provide direct links to all referenced code in GitHub, along with screenshots, logs, or any other relevant proof that illustrates the concept. \\n\\nIf the proof of concept is too large to be embedded directly or if the code reveals potential vulnerabilities, it is recommended to use a private gist or a private GitHub repo. Some wardens have also added a zip file to the submission. It's important to note, when linking to a GitHub repo, it does not automatically pull in that code snippet to the report.\\n\\nIt is also acceptable to use external platforms like Gist for submitting long proofs of concept. However, when showing places of vulnerability, it's recommended to include both the URL to the repository with the line number and a code block. \\n\\nUnderstanding the markdown code to include GitHub code in report can also be helpful. Here is a link to learn how to include that: [https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks].\\n\\nRemember, the 'Proof of Concept' section is just one part of your submission. You should also clearly explain the vulnerability and its impact on the protocol/code in the 'Impact' section.\\n\\nFor more detailed guidance, you can refer to the Code4Arena's submission policy at [https://docs.code4rena.com/roles/wardens/submission-policy#how-to-include-a-proof-of-concept].\""
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a8bf189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a JSON formatted string\n",
    "json_string = json.dumps(new_qa_pairs2)\n",
    "\n",
    "# Write the JSON string to a file\n",
    "with open(\"./codearena/new_qa_pairs_09_24.json\", \"w\") as file:\n",
    "    file.write(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "827c0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON string from the file\n",
    "with open(\"./codearena/new_qa_pairs_09_24.json\", \"r\") as file:\n",
    "    json_string = file.read()\n",
    "\n",
    "# Convert the JSON formatted string back to a Python list\n",
    "new_qa_pairs2 = json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f82312ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_qa_pairs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "f851ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_qa_pairs2 += new_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "d1cf592d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2767"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_qa_pairs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "80d578b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0\n",
      "Working on 1\n",
      "Working on 2\n",
      "Working on 3\n",
      "Working on 4\n",
      "Working on 5\n",
      "Working on 6\n",
      "Working on 7\n",
      "Working on 8\n",
      "Working on 9\n",
      "Working on 16\n",
      "Working on 17\n",
      "Working on 18\n",
      "Working on 19\n",
      "Working on 20\n",
      "Working on 21\n",
      "Working on 22\n",
      "Working on 23\n",
      "Working on 24\n",
      "Working on 25\n",
      "Working on 26\n",
      "Working on 27\n",
      "Working on 28\n",
      "Working onWorking on 30\n",
      " 29\n",
      "Working on 31\n",
      "Working on 32\n",
      "Working on 33\n",
      "Working on 34\n",
      "Working on 35\n",
      "Working on 36\n",
      "Working on 37\n",
      "Working on 38\n",
      "Working on 39\n",
      "Working on 40\n",
      "Working on 41\n",
      "Working on 42\n",
      "Working on 43\n",
      "Working on 44\n",
      "Working on 45\n",
      "Working on 46\n",
      "Working on 47\n",
      "Working on 48\n",
      "Working on 49\n",
      "Working on 50\n",
      "Working on 51\n",
      "Working on 52\n",
      "Working on 53\n",
      "Working on 54\n",
      "Working on 55\n",
      "Working on 56\n",
      "Working onWorking on 58\n",
      " 57\n",
      "Working on 59\n",
      "Working on 60\n",
      "Working on 61\n",
      "Working on 62\n",
      "Working on 63\n",
      "Working on 64\n",
      "Working on 65\n",
      "Working on 66\n",
      "Working on 67\n",
      "Working on 68\n",
      "Working on 69\n",
      "Working on 70\n",
      "Working on 71\n",
      "Working on 72\n",
      "Working on 73\n",
      "Working on 74\n",
      "Working on 75\n",
      "Working on 76\n",
      "Working on 77\n",
      "Working on 78\n",
      "Working on 79\n",
      "Working on 80\n",
      "Working on 81\n",
      "Working on 82\n",
      "Working on 83\n",
      "Working on 84\n",
      "Working on 85\n",
      "Working on 86\n",
      "Working on 87\n",
      "Working on 88\n",
      "Working on 89\n",
      "Working on 90\n",
      "Working on 91\n",
      "Working on 92\n",
      "Working on 93\n",
      "Working onWorking on 95\n",
      " 94\n",
      "Working on 96\n",
      "Working on 97\n",
      "Working on 98\n",
      "Working on 99\n",
      "Working on 100\n",
      "Working on 101\n",
      "Working on 102\n",
      "Working on 103\n",
      "Working on 104\n",
      "Working on 105\n",
      "Working on 106\n",
      "Working on 107\n",
      "Working on 108\n",
      "Working on 109\n",
      "Working on 110\n",
      "Working on 111\n",
      "Working on 112\n",
      "Working on 113\n",
      "Working on 114\n",
      "Working on 115\n",
      "Working on 116\n",
      "Working on 117\n",
      "Working on 118\n",
      "Working on 119\n",
      "Working on 120\n",
      "Working on 121\n",
      "Working on 122\n",
      "Working on 123\n",
      "Working on 124\n",
      "Working on 125\n",
      "Working on 126\n",
      "Working on 127\n",
      "Working on 128\n",
      "Working on 129\n",
      "Working on 130\n",
      "Working on 131\n",
      "Working on 132\n",
      "Working on 133\n",
      "Working on 134\n",
      "Working on 135\n",
      "Working on 136\n",
      "Working on 137\n",
      "Working on 138\n",
      "Working on 139\n",
      "Working on 140\n",
      "Working on 141\n",
      "Working on 142\n",
      "Working on 143\n",
      "Working on 144\n",
      "Working on 145\n",
      "Working on 146\n",
      "Working on 147\n",
      "Working on 148\n",
      "Working on 149\n",
      "Working onWorking on 151\n",
      " 150\n",
      "Working on 152\n",
      "Working on 153\n",
      "Working on 154\n",
      "Working on 155\n",
      "Working on 156\n",
      "Working on 157\n",
      "Working on 158\n",
      "Working on 159\n",
      "Working on 160\n",
      "Working on 161\n",
      "Working on 162\n",
      "Working on 163\n",
      "Working on 164\n",
      "Working on 165\n",
      "Working on 166\n",
      "Working on 167\n",
      "Working on 168\n",
      "Working on 169\n",
      "Working on 170\n",
      "Working on 171\n",
      "Working on 172\n",
      "Working on 173\n",
      "Working on 174\n",
      "Working on 175\n",
      "Working on 176\n",
      "Working on 177\n",
      "Working on 178\n",
      "Working on 179\n",
      "Working onWorking on 181\n",
      " 180\n",
      "Working on 182\n",
      "Working on 183\n",
      "Working on 184\n",
      "Working on 185\n",
      "Working on 186\n",
      "Working on 187\n",
      "Working on 188\n",
      "Working on 189\n",
      "Working on 190\n",
      "Working on 191\n",
      "Working on 192\n",
      "Working on 193\n",
      "Working on 194\n",
      "Working on 195\n",
      "Working on 196\n",
      "Working on 197\n",
      "Working on 198\n",
      "Working on 199\n",
      "Working on 200\n",
      "Working on 201\n",
      "Working on 202\n",
      "Working on 203\n",
      "Working on 204\n",
      "Working on 205\n",
      "Working on 206\n",
      "Working on 207\n",
      "Working on 208\n",
      "Working on 209\n",
      "Working on 210\n",
      "Working on 211\n",
      "Working on 212\n",
      "Working on 213\n",
      "Working on 214\n",
      "Working on 215\n",
      "Working on 216\n",
      "Working on 217\n",
      "Working on 218\n",
      "Working on 219\n",
      "Working on 220\n",
      "Working on 221\n",
      "Working on 222\n",
      "Working on 223\n",
      "Working on 224\n",
      "Working on 225\n",
      "Working on 226\n",
      "Working on 227\n",
      "Working on 228\n",
      "Working on 229\n",
      "Working on 230\n",
      "Working on 231\n",
      "Working on 232\n",
      "Working on 233\n",
      "Working on 234\n",
      "Working on 235\n",
      "Working on 236\n",
      "Working on 237\n",
      "Working on 238\n",
      "Working on 239\n",
      "Working on 240\n",
      "Working on 241\n",
      "Working on 242\n",
      "Working on 243\n",
      "Working on 244\n",
      "Working on 245\n",
      "Working on 246\n",
      "Working on 247\n",
      "Working on 248\n",
      "Working on 249\n",
      "Working on 250\n",
      "Working on 251\n",
      "Working on 252\n",
      "Working on 253\n",
      "Working on 254\n",
      "Working on 255\n",
      "Working on 256\n",
      "Working on 257\n",
      "Working on 258\n",
      "Working on 259\n",
      "Working onWorking on 261\n",
      " 260\n",
      "Working on 262\n",
      "Working on 263\n",
      "Working on 264\n",
      "Working on 265\n",
      "Working on 266\n",
      "Working on 267\n",
      "Working on 268\n",
      "Working on 269\n",
      "Working on 270\n",
      "Working on 271\n",
      "Working on 272\n",
      "Working on 273\n",
      "Working on 274\n",
      "Working on 275\n",
      "Working on 276\n",
      "Working on 277\n",
      "Working on 278\n",
      "Working on 279\n",
      "Working on 280\n",
      "Working on 281\n",
      "Working on 282\n",
      "Working on 283\n",
      "Working on 284\n",
      "Working on 285\n",
      "Working on 286\n",
      "Working on 287\n",
      "Working on 288\n",
      "Working on 289\n",
      "Working on 290\n",
      "Working on 291\n",
      "Working on 292\n",
      "Working on 293\n",
      "Working on 294\n",
      "Working on 295\n",
      "Working on 296\n",
      "Working on 297\n",
      "Working on 298\n",
      "Working on 299\n",
      "Working on 300\n",
      "Working on 301\n",
      "Working on 302\n",
      "Working on 303\n",
      "Working on 304\n",
      "Working on 305\n",
      "Working on 306\n",
      "Working on 307\n",
      "Working on 308\n",
      "Working on 309\n",
      "Working on 310\n",
      "Working on 311\n",
      "Working on 312\n",
      "Working on 313\n",
      "Working on 314\n",
      "Working on 315\n",
      "Working on 316\n",
      "Working on 317\n",
      "Working on 318\n",
      "Working on 319\n",
      "Working on 320\n",
      "Working on 321\n",
      "Working on 322\n",
      "Working on 323\n",
      "Working on 324\n",
      "Working on 325\n",
      "Working on 326\n",
      "Working on 327\n",
      "Working on 328\n",
      "Working onWorking on 330\n",
      " 329\n",
      "Working on 331\n",
      "Working on 332\n",
      "Working on 333\n",
      "Working on 334\n",
      "Working on 335\n",
      "Working on 336\n",
      "Working on 337\n",
      "Working on 338\n",
      "Working on 339\n",
      "Working on 340\n",
      "Working on 341\n",
      "Working on 342\n",
      "Working on 343\n",
      "Working onWorking on 345\n",
      " 344\n",
      "Working on 346\n",
      "Working on 347\n",
      "Working on 348\n",
      "Working on 349\n",
      "Working on 350\n",
      "Working on 351\n",
      "Working onWorking on 353\n",
      " 352\n",
      "Working on 354\n",
      "Working on 355\n",
      "Working on 356\n",
      "Working on 357\n",
      "Working on 358\n",
      "Working on 359\n",
      "Working on 360\n",
      "Working on 361\n",
      "Working on 362\n",
      "Working on 363\n",
      "Working on 364\n",
      "Working on 365\n",
      "Working on 366\n",
      "Working on 367\n",
      "Working on 368\n",
      "Working on 369\n",
      "Working on 370\n",
      "Working on 371\n",
      "Working on 372\n",
      "Working on 373\n",
      "Working on 374\n",
      "Working on 375\n",
      "Working on 376\n",
      "Working on 377\n",
      "Working on 378\n",
      "Working on 379\n",
      "Working on 380\n",
      "Working on 381\n",
      "Working on 382\n",
      "Working on 383\n",
      "Working on 384\n",
      "Working on 385\n",
      "Working on 386\n",
      "Working on 387\n",
      "Working on 388\n",
      "Working on 389\n",
      "Working on 390\n",
      "Working on 391\n",
      "Working on 392\n",
      "Working on 393\n",
      "Working on 394\n",
      "Working on 395\n",
      "Working on 396\n",
      "Working on 397\n",
      "Working on 398\n",
      "Working on 399\n",
      "Working on 400\n",
      "Working on 401\n",
      "Working on 402\n",
      "Working on 403\n",
      "Working on 404\n",
      "Working on 405\n",
      "Working on 406\n",
      "Working onWorking on 408\n",
      " 407\n",
      "Working on 409\n",
      "Working on 410\n",
      "Working on 411\n",
      "Working on 412\n",
      "Working on 413\n",
      "Working on 414\n",
      "Working on 415\n",
      "Working on 416\n",
      "Working on 417\n",
      "Working on 418\n",
      "Working on 419\n",
      "Working on 420\n",
      "Working on 421\n",
      "Working on 422\n",
      "Working on 423\n",
      "Working on 424\n",
      "Working on 425\n",
      "Working on 426\n",
      "Working on 427\n",
      "Working on 428\n",
      "Working on 429\n",
      "Working on 430\n",
      "Working on 431\n",
      "Working on 432\n",
      "Working on 433\n",
      "Working on 434\n",
      "Working on 435\n",
      "Working on 436\n",
      "Working on 437\n",
      "Working on 438\n",
      "Working on 439\n",
      "Working on 440\n",
      "Working on 441\n",
      "Working on 442\n",
      "Working on 443\n",
      "Working on 444\n",
      "Working on 445\n",
      "Working on 446\n",
      "Working on 447\n",
      "Working on 448\n",
      "Working on 449\n",
      "Working on 450\n",
      "Working on 451\n",
      "Working on 452\n",
      "Working on 453\n",
      "Working on 454\n",
      "Working on 455\n",
      "Working on 456\n",
      "Working on 457\n",
      "Working on 458\n",
      "Working on 459\n",
      "Working on 460\n",
      "Working on 461\n",
      "Working on 462\n",
      "Working on 463\n",
      "Working on 464\n",
      "Working on 465\n",
      "Working on 466\n",
      "Working on 467\n",
      "Working on 468\n",
      "Working on 469\n",
      "Working onWorking on 471\n",
      " 470\n",
      "Working on 472\n",
      "Working on 473\n",
      "Working on 474\n",
      "Working on 475\n",
      "Working on 476\n",
      "Working on 477\n",
      "Working on 478\n",
      "Working on 479\n",
      "Working on 480\n",
      "Working on 481\n",
      "Working on 482\n",
      "Working on 483\n",
      "Working on 484\n",
      "Working onWorking on 486\n",
      " 485\n",
      "Working on 487\n",
      "Working on 488\n",
      "Working on 489\n",
      "Working on 490\n",
      "Working on 491\n",
      "Working on 492\n",
      "Working on 493\n",
      "Working on 494\n",
      "Working on 495\n",
      "Working on 496\n",
      "Working on 497\n",
      "Working on 498\n",
      "Working on 499\n",
      "Working on 500\n",
      "Working on 501\n",
      "Working on 502\n",
      "Working on 503\n",
      "Working on 504\n",
      "Working onWorking on 506\n",
      " 505\n",
      "Working on 507\n",
      "Working on 508\n",
      "Working on 509\n",
      "Working on 510\n",
      "Working on 511\n",
      "Working on 512\n",
      "Working on 513\n",
      "Working on 514\n",
      "Working on 515\n",
      "Working on 516\n",
      "Working on 517\n",
      "Working on 518\n",
      "Working on 519\n",
      "Working on 520\n",
      "Working on 521\n",
      "Working on 522\n",
      "Working on 523\n",
      "Working on 524\n",
      "Working on 525\n",
      "Working on 526\n",
      "Working on 527\n",
      "Working on 528\n",
      "Working on 529\n",
      "Working on 530\n",
      "Working on 531\n",
      "Working on 532\n",
      "Working on 533\n",
      "Working on 534\n",
      "Working on 535\n",
      "Working on 536\n",
      "Working on 537\n",
      "Working on 538\n",
      "Working on 539\n",
      "Working on 540\n",
      "Working on 541\n",
      "Working on 542\n",
      "Working on 543\n",
      "Working on 544\n",
      "Working on 545\n",
      "Working onWorking on 547\n",
      " 546\n",
      "Working on 548\n",
      "Working on 549\n",
      "Working on 550\n",
      "Working on 551\n",
      "Working on 552\n",
      "Working on 553\n",
      "Working on 554\n",
      "Working on 555\n",
      "Working on 556\n",
      "Working on 557\n",
      "Working on 558\n",
      "Working on 559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 560\n",
      "Working on 561\n",
      "Working on 562\n",
      "Working on 563\n",
      "Working on 564\n",
      "Working on 565\n",
      "Working on 566\n",
      "Working on 567\n",
      "Working on 568\n",
      "Working on 569\n",
      "Working on 570\n",
      "Working on 571\n",
      "Working on 572\n",
      "Working on 573\n",
      "Working onWorking on 575\n",
      " 574\n",
      "Working on 576\n",
      "Working on 577\n",
      "Working on 578\n",
      "Working on 579\n",
      "Working on 580\n",
      "Working on 581\n",
      "Working on 582\n",
      "Working on 583\n",
      "Working on 584\n",
      "Working on 585\n",
      "Working on 586\n",
      "Working on 587\n",
      "Working on 588\n",
      "Working on 589\n",
      "Working on 590\n",
      "Working on 591\n",
      "Working on 592\n",
      "Working on 593\n",
      "Working on 594\n",
      "Working on 595\n",
      "Working on 596\n",
      "Working on 597\n",
      "Working on 598\n",
      "Working on 599\n",
      "Working on 600\n",
      "Working on 601\n",
      "Working on 602\n",
      "Working on 603\n",
      "Working on 604\n",
      "Working on 605\n",
      "Working on 606\n",
      "Working on 607\n",
      "Working on 608\n",
      "Working on 609\n",
      "Working on 610\n",
      "Working on 611\n",
      "Working on 612\n",
      "Working on 613\n",
      "Working on 614\n",
      "Working on 615\n",
      "Working on 616\n",
      "Working on 617\n",
      "Working on 618\n",
      "Working onWorking on 620\n",
      " 619\n",
      "Working on 621\n",
      "Working on 622\n",
      "Working on 623\n",
      "Working on 624\n",
      "Working on 625\n",
      "Working onWorking on 627\n",
      " 626\n",
      "Working on 628\n",
      "Working on 629\n",
      "Working on 630\n",
      "Working on 631\n",
      "Working on 632\n",
      "Working on 633\n",
      "Working on 634\n",
      "Working on 635\n",
      "Working on 636\n",
      "Working onWorking on 638\n",
      " 637\n",
      "Working on 639\n",
      "Working on 640\n",
      "Working on 641\n",
      "Working on 642\n",
      "Working on 643\n",
      "Working on 644\n",
      "Working on 645\n",
      "Working on 646\n",
      "Working on 647\n",
      "Working on 648\n",
      "Working on 649\n",
      "Working on 650\n",
      "Working on 651\n",
      "Working on 652\n",
      "Working on 653\n",
      "Working on 654\n",
      "Working on 655\n",
      "Working on 656\n",
      "Working on 657\n",
      "Working on 658\n",
      "Working on 659\n",
      "Working on 660\n",
      "Working onWorking on 662\n",
      " 661\n",
      "Working on 663\n",
      "Working on 664\n",
      "Working on 665\n",
      "Working on 666\n",
      "Working on 667\n",
      "Working onWorking on 669\n",
      " 668\n",
      "Working on 670\n",
      "Working on 671\n",
      "Working on 672\n",
      "Working on 673\n",
      "Working on 674\n",
      "Working onWorking on 676\n",
      " 675\n",
      "Working on 677\n",
      "Working on 678\n",
      "Working on 679\n",
      "Working on 680\n",
      "Working on 681\n",
      "Working on 682\n",
      "Working on 683\n",
      "Working on 684\n",
      "Working on 685\n",
      "Working on 686\n",
      "Working on 687\n",
      "Working on 688\n",
      "Working on 689\n",
      "Working on 690\n",
      "Working on 691\n",
      "Working on 692\n",
      "Working on 693\n",
      "Working on 694\n",
      "Working onWorking on 696\n",
      " 695\n",
      "Working on 697\n",
      "Working on 698\n",
      "Working on 699\n",
      "Working onWorking on 701\n",
      " 700\n",
      "Working on 702\n",
      "Working on 703\n",
      "Working on 704\n",
      "Working on 705\n",
      "Working on 706\n",
      "Working on 707\n",
      "Working on 708\n",
      "Working on 709\n",
      "Working on 710\n",
      "Working on 711\n",
      "Working on 712\n",
      "Working on 713\n",
      "Working on 714\n",
      "Working on 715\n",
      "Working on 716\n",
      "Working on 717\n",
      "Working on 718\n",
      "Working on 719\n",
      "Working on 720\n",
      "Working on 721\n",
      "Working on 722\n",
      "Working on 723\n",
      "Working on 724\n",
      "Working on 725\n",
      "Working on 726\n",
      "Working on 727\n",
      "Working on 728\n",
      "Working on 729\n",
      "Working on 730\n",
      "Working on 731\n",
      "Working on 732\n",
      "Working on 733\n",
      "Working on 734\n",
      "Working on 735\n",
      "Working on 736\n",
      "Working on 737\n",
      "Working on 738\n",
      "Working on 739\n",
      "Working on 740\n",
      "Working on 741\n",
      "Working on 742\n",
      "Working on 743\n",
      "Working on 744\n",
      "Working on 745\n",
      "Working on 746\n",
      "Working on 747\n",
      "Working on 748\n",
      "Working on 749\n",
      "Working on 750\n",
      "Working on 751\n",
      "Working on 752\n",
      "Working on 753\n",
      "Working on 754\n",
      "Working on 755\n",
      "Working on 756\n",
      "Working on 757\n",
      "Working on 758\n",
      "Working on 759\n",
      "Working on 760\n",
      "Working on 761\n",
      "Working on 762\n",
      "Working on 763\n",
      "Working on 764\n",
      "Working on 765\n",
      "Working on 766\n",
      "Working on 767\n",
      "Working on 768\n",
      "Working on 769\n",
      "Working on 770\n",
      "Working on 771\n",
      "Working on 772\n",
      "Working on 773\n",
      "Working on 774\n",
      "Working on 775\n",
      "Working on 776\n",
      "Working on 777\n",
      "Working on 778\n",
      "Working on 779\n",
      "Working on 780\n",
      "Working on 781\n",
      "Working on 782\n",
      "Working on 783\n",
      "Working on 784\n",
      "Working on 785\n",
      "Working on 786\n",
      "Working on 787\n",
      "Working on 788\n",
      "Working on 789\n",
      "Working on 790\n",
      "Working on 791\n",
      "Working on 792\n",
      "Working on 793\n",
      "Working on 794\n",
      "Working on 795\n",
      "Working on 796\n",
      "Working on 797\n",
      "Working on 798\n",
      "Working on 799\n",
      "Working on 800\n",
      "Working on 801\n",
      "Working on 802\n",
      "Working on 803\n",
      "Working on 804\n",
      "Working on 805\n",
      "Working on 806\n",
      "Working on 807\n",
      "Working on 808\n",
      "Working on 809\n",
      "Working on 810\n",
      "Working on 811\n",
      "Working on 812\n",
      "Working on 813\n",
      "Working on 814\n",
      "Working on 815\n",
      "Working on 816\n",
      "Working on 817\n",
      "Working on 818\n",
      "Working on 819\n",
      "Working on 820\n",
      "Working on 821\n",
      "Working on 822\n",
      "Working on 823\n",
      "Working on 824\n",
      "Working on 825\n",
      "Working on 826\n",
      "Working on 827\n",
      "Working on 828\n",
      "Working on 829\n",
      "Working on 830\n",
      "Working on 831\n",
      "Working on 832\n",
      "Working on 833\n",
      "Working on 834\n",
      "Working on 835\n",
      "Working on 836\n",
      "Working on 837\n",
      "Working on 838\n",
      "Working on 839\n",
      "Working on 840\n",
      "Working on 841\n",
      "Working on 842\n",
      "Working on 843\n",
      "Working on 844\n",
      "Working on 845\n",
      "Working on 846\n",
      "Working on 847\n",
      "Working on 848\n",
      "Working on 849\n",
      "Working on 850\n",
      "Working on 851\n",
      "Working on 852\n",
      "Working on 853\n",
      "Working on 854\n",
      "Working on 855\n",
      "Working on 856\n",
      "Working on 857\n",
      "Working on 858\n",
      "Working on 859\n",
      "Working on 860\n",
      "Working on 861\n",
      "Working on 862\n",
      "Working on 863\n",
      "Working on 864\n",
      "Working on 865\n",
      "Working on 866\n",
      "Working on 867\n",
      "Working on 868\n",
      "Working on 869\n",
      "Working on 870\n",
      "Working on 871\n",
      "Working on 872\n",
      "Working on 873\n",
      "Working on 874\n",
      "Working on 875\n",
      "Working on 876\n",
      "Working on 877\n",
      "Working on 878\n",
      "Working on 879\n",
      "Working on 880\n",
      "Working on 881\n",
      "Working on 882\n",
      "Working on 883\n",
      "Working on 884\n",
      "Working on 885\n",
      "Working on 886\n",
      "Working on 887\n",
      "Working on 888\n",
      "Working on 889\n",
      "Working on 890\n",
      "Working on 891\n",
      "Working on 892\n",
      "Working on 893\n",
      "Working on 894\n",
      "Working on 895\n",
      "Working on 896\n",
      "Working on 897\n",
      "Working on 898\n",
      "Working on 899\n",
      "Working on 900\n",
      "Working on 901\n",
      "Working on 902\n",
      "Working on 903\n",
      "Working on 904\n",
      "Working on 905\n",
      "Working on 906\n",
      "Working on 907\n",
      "Working on 908\n",
      "Working on 909\n",
      "Working onWorking on 911\n",
      " 910\n",
      "Working on 912\n",
      "Working on 913\n",
      "Working on 914\n",
      "Working on 915\n",
      "Working on 916\n",
      "Working on 917\n",
      "Working on 918\n",
      "Working on 919\n",
      "Working on 920\n",
      "Working on 921\n",
      "Working on 922\n",
      "Working on 923\n",
      "Working on 924\n",
      "Working on 925\n",
      "Working on 926\n",
      "Working on 927\n",
      "Working on 928\n",
      "Working on 929\n",
      "Working on 930\n",
      "Working on 931\n",
      "Working on 932\n",
      "Working on 933\n",
      "Working on 934\n",
      "Working on 935\n",
      "Working on 936\n",
      "Working on 937\n",
      "Working on 938\n",
      "Working on 939\n",
      "Working on 940\n",
      "Working on 941\n",
      "Working on 942\n",
      "Working on 943\n",
      "Working on 944\n",
      "Working on 945\n",
      "Working on 946\n",
      "Working on 947\n",
      "Working onWorking on 949\n",
      " 948\n",
      "Working on 950\n",
      "Working on 951\n",
      "Working on 952\n",
      "Working on 953\n",
      "Working on 954\n",
      "Working on 955\n",
      "Working on 956\n",
      "Working on 957\n",
      "Working on 958\n",
      "Working on 959\n",
      "Working on 960\n",
      "Working on 961\n",
      "Working on 962\n",
      "Working on 963\n",
      "Working on 964\n",
      "Working on 965\n",
      "Working on 966\n",
      "Working on 967\n",
      "Working on 968\n",
      "Working on 969\n",
      "Working on 970\n",
      "Working on 971\n",
      "Working on 972\n",
      "Working on 973\n",
      "Working on 974\n",
      "Working on 975\n",
      "Working on 976\n",
      "Working on 977\n",
      "Working on 978\n",
      "Working on 979\n",
      "Working on 980\n",
      "Working on 981\n",
      "Working on 982\n",
      "Working on 983\n",
      "Working on 984\n",
      "Working on 985\n",
      "Working onWorking on 987\n",
      " 986\n",
      "Working on 988\n",
      "Working on 989\n",
      "Working on 990\n",
      "Working on 991\n",
      "Working on 992\n",
      "Working on 993\n",
      "Working on 994\n",
      "Working on 995\n",
      "Working on 996\n",
      "Working on 997\n",
      "Working on 998\n",
      "Working on 999\n",
      "Working on 1000\n",
      "Working on 1001\n",
      "Working on 1002\n",
      "Working on 1003\n",
      "Working on 1004\n",
      "Working on 1005\n",
      "Working on 1006\n",
      "Working on 1007\n",
      "Working on 1008\n",
      "Working on 1009\n",
      "Working on 1010\n",
      "Working on 1011\n",
      "Working on 1012\n",
      "Working on 1013\n",
      "Working on 1014\n",
      "Working on 1015\n",
      "Working on 1016\n",
      "Working on 1017\n",
      "Working on 1018\n",
      "Working on 1019\n",
      "Working on 1020\n",
      "Working on 1021\n",
      "Working on 1022\n",
      "Working on 1023\n",
      "Working on 1024\n",
      "Working on 1025\n",
      "Working on 1026\n",
      "Working on 1027\n",
      "Working on 1028\n",
      "Working on 1029\n",
      "Working on 1030\n",
      "Working on 1031\n",
      "Working on 1032\n",
      "Working on 1033\n",
      "Working on 1034\n",
      "Working on 1035\n",
      "Working on 1036\n",
      "Working on 1037\n",
      "Working on 1038\n",
      "Working on 1039\n",
      "Working on 1040\n",
      "Working on 1041\n",
      "Working on 1042\n",
      "Working on 1043\n",
      "Working on 1044\n",
      "Working on 1045\n",
      "Working on 1046\n",
      "Working on 1047\n",
      "Working on 1048\n",
      "Working on 1049\n",
      "Working on 1050\n",
      "Working on 1051\n",
      "Working on 1052\n",
      "Working on 1053\n",
      "Working on 1054\n",
      "Working on 1055\n",
      "Working on 1056\n",
      "Working on 1057\n",
      "Working on 1058\n",
      "Working on 1059\n",
      "Working on 1060\n",
      "Working on 1061\n",
      "Working on 1062\n",
      "Working on 1063\n",
      "Working on 1064\n",
      "Working on 1065\n",
      "Working on 1066\n",
      "Working on 1067\n",
      "Working on 1068\n",
      "Working on 1069\n",
      "Working on 1070\n",
      "Working on 1071\n",
      "Working on 1072\n",
      "Working on 1073\n",
      "Working on 1074\n",
      "Working on 1075\n",
      "Working on 1076\n",
      "Working on 1077\n",
      "Working on 1078\n",
      "Working on 1079\n",
      "Working on 1080\n",
      "Working on 1081\n",
      "Working on 1082\n",
      "Working on 1083\n",
      "Working on 1084\n",
      "Working on 1085\n",
      "Working on 1086\n",
      "Working on 1087\n",
      "Working on 1088\n",
      "Working on 1089\n",
      "Working on 1090\n",
      "Working on 1091\n",
      "Working on 1092\n",
      "Working on 1093\n",
      "Working on 1094\n",
      "Working on 1095\n",
      "Working on 1096\n",
      "Working on 1097\n",
      "Working on 1098\n",
      "Working on 1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1100\n",
      "Working on 1101\n",
      "Working on 1102\n",
      "Working on 1103\n",
      "Working on 1104\n",
      "Working on 1105\n",
      "Working on 1106\n",
      "Working on 1107\n",
      "Working on 1108\n",
      "Working on 1109\n",
      "Working on 1110\n",
      "Working on 1111\n",
      "Working on 1112\n",
      "Working on 1113\n",
      "Working on 1114\n",
      "Working on 1115\n",
      "Working on 1116\n",
      "Working on 1117\n",
      "Working on 1118\n",
      "Working on 1119\n",
      "Working on 1120\n",
      "Working on 1121\n",
      "Working on 1122\n",
      "Working on 1123\n",
      "Working on 1124\n",
      "Working on 1125\n",
      "Working on 1126\n",
      "Working on 1127\n",
      "Working on 1128\n",
      "Working on 1129\n",
      "Working on 1130\n",
      "Working on 1131\n",
      "Working on 1132\n",
      "Working on 1133\n",
      "Working on 1134\n",
      "Working on 1135\n",
      "Working on 1136\n",
      "Working on 1137\n",
      "Working on 1138\n",
      "Working on 1139\n",
      "Working on 1140\n",
      "Working on 1141\n",
      "Working on 1142\n",
      "Working on 1143\n",
      "Working on 1144\n",
      "Working on 1145\n",
      "Working on 1146\n",
      "Working on 1147\n",
      "Working on 1148\n",
      "Working on 1149\n",
      "Working on 1150\n",
      "Working on 1151\n",
      "Working on 1152\n",
      "Working on 1153\n",
      "Working on 1154\n",
      "Working on 1155\n",
      "Working on 1156\n",
      "Working on 1157\n",
      "Working on 1158\n",
      "Working on 1159\n",
      "Working on 1160\n",
      "Working on 1161\n",
      "Working on 1162\n",
      "Working on 1163\n",
      "Working on 1164\n",
      "Working on 1165\n",
      "Working on 1166\n",
      "Working on 1167\n",
      "Working on 1168\n",
      "Working on 1169\n",
      "Working on 1170\n",
      "Working on 1171\n",
      "Working on 1172\n",
      "Working on 1173\n",
      "Working on 1174\n",
      "Working on 1175\n",
      "Working onWorking on 1177\n",
      " 1176\n",
      "Working on 1178\n",
      "Working on 1179\n",
      "Working on 1180\n",
      "Working on 1181\n",
      "Working on 1182\n",
      "Working on 1183\n",
      "Working on 1184\n",
      "Working on 1185\n",
      "Working on 1186\n",
      "Working on 1187\n",
      "Working on 1188\n",
      "Working on 1189\n",
      "Working on 1190\n",
      "Working on 1191\n",
      "Working on 1192\n",
      "Working on 1193\n",
      "Working on 1194\n",
      "Working on 1195\n",
      "Working on 1196\n",
      "Working on 1197\n",
      "Working on 1198\n",
      "Working on 1199\n",
      "Working on 1200\n",
      "Working on 1201\n",
      "Working on 1202\n",
      "Working on 1203\n",
      "Working on 1204\n",
      "Working on 1205\n",
      "Working on 1206\n",
      "Working on 1207\n",
      "Working on 1208\n",
      "Working on 1209\n",
      "Working on 1210\n",
      "Working on 1211\n",
      "Working on 1212\n",
      "Working on 1213\n",
      "Working on 1214\n",
      "Working on 1215\n",
      "Working on 1216\n",
      "Working on 1217\n",
      "Working on 1218\n",
      "Working on 1219\n",
      "Working on 1220\n",
      "Working on 1221\n",
      "Working on 1222\n",
      "Working on 1223\n",
      "Working on 1224\n",
      "Working on 1225\n",
      "Working on 1226\n",
      "Working on 1227\n",
      "Working on 1228\n",
      "Working on 1229\n",
      "Working on 1230\n",
      "Working on 1231\n",
      "Working on 1232\n",
      "Working on 1233\n",
      "Working on 1234\n",
      "Working on 1235\n",
      "Working on 1236\n",
      "Working on 1237\n",
      "Working on 1238\n",
      "Working on 1239\n",
      "Working on 1240\n",
      "Working on 1241\n",
      "Working on 1242\n",
      "Working on 1243\n",
      "Working on 1244\n",
      "Working on 1245\n",
      "Working on 1246\n",
      "Working on 1247\n",
      "Working on 1248\n",
      "Working on 1249\n",
      "Working on 1250\n",
      "Working on 1251\n",
      "Working on 1252\n",
      "Working on 1253\n",
      "Working on 1254\n",
      "Working on 1255\n",
      "Working on 1256\n",
      "Working on 1257\n",
      "Working on 1258\n",
      "Working on 1259\n",
      "Working on 1260\n",
      "Working on 1261\n",
      "Working on 1262\n",
      "Working on 1263\n",
      "Working on 1264\n",
      "Working on 1265\n",
      "Working on 1266\n",
      "Working on 1267\n",
      "Working on 1268\n",
      "Working on 1269\n",
      "Working on 1270\n",
      "Working on 1271\n",
      "Working on 1272\n",
      "Working on 1273\n",
      "Working on 1274\n",
      "Working on 1275\n",
      "Working on 1276\n",
      "Working on 1277\n",
      "Working on 1278\n",
      "Working on 1279\n",
      "Working on 1280\n",
      "Working on 1281\n",
      "Working on 1282\n",
      "Working on 1283\n",
      "Working on 1284\n",
      "Working on 1285\n",
      "Working on 1286\n",
      "Working on 1287\n",
      "Working on 1288\n",
      "Working on 1289\n",
      "Working onWorking on 1291\n",
      " 1290\n",
      "Working on 1292\n",
      "Working on 1293\n",
      "Working on 1294\n",
      "Working on 1295\n",
      "Working on 1296\n",
      "Working on 1297\n",
      "Working onWorking on 1299\n",
      " 1298\n",
      "Working on 1300\n",
      "Working on 1301\n",
      "Working on 1302\n",
      "Working on 1303\n",
      "Working on 1304\n",
      "Working on 1305\n",
      "Working on 1306\n",
      "Working on 1307\n",
      "Working on 1308\n",
      "Working on 1309\n",
      "Working on 1310\n",
      "Working on 1311\n",
      "Working on 1312\n",
      "Working on 1313\n",
      "Working on 1314\n",
      "Working on 1315\n",
      "Working on 1316\n",
      "Working on 1317\n",
      "Working on 1318\n",
      "Working on 1319\n",
      "Working on 1320\n",
      "Working on 1321\n",
      "Working on 1322\n",
      "Working on 1323\n",
      "Working on 1324\n",
      "Working on 1325\n",
      "Working on 1326\n",
      "Working on 1327\n",
      "Working on 1328\n",
      "Working on 1329\n",
      "Working on 1330\n",
      "Working on 1331\n",
      "Working on 1332\n",
      "Working on 1333\n",
      "Working on 1334\n",
      "Working on 1335\n",
      "Working on 1336\n",
      "Working on 1337\n",
      "Working on 1338\n",
      "Working on 1339\n",
      "Working on 1340\n",
      "Working on 1341\n",
      "Working on 1342\n",
      "Working on 1343\n",
      "Working on 1344\n",
      "Working on 1345\n",
      "Working on 1346\n",
      "Working on 1347\n",
      "Working on 1348\n",
      "Working on 1349\n",
      "Working on 1350\n",
      "Working on 1351\n",
      "Working on 1352\n",
      "Working onWorking on 1354\n",
      " 1353\n",
      "Working on 1355\n",
      "Working on 1356\n",
      "Working on 1357\n",
      "Working on 1358\n",
      "Working on 1359\n",
      "Working on 1360\n",
      "Working on 1361\n",
      "Working on 1362\n",
      "Working on 1363\n",
      "Working on 1364\n",
      "Working on 1365\n",
      "Working on 1366\n",
      "Working on 1367\n",
      "Working on 1368\n",
      "Working on 1369\n",
      "Working on 1370\n",
      "Working on 1371\n",
      "Working on 1372\n",
      "Working on 1373\n",
      "Working on 1374\n",
      "Working on 1375\n",
      "Working on 1376\n",
      "Working on 1377\n",
      "Working on 1378\n",
      "Working on 1379\n",
      "Working on 1380\n",
      "Working on 1381\n",
      "Working on 1382\n",
      "Working onWorking on 1384\n",
      " 1383\n",
      "Working on 1385\n",
      "Working on 1386\n",
      "Working on 1387\n",
      "Working on 1388\n",
      "Working on 1389\n",
      "Working on 1390\n",
      "Working onWorking on 1392\n",
      " 1391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 25 Sep 2023 01:51:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80bf9e41f930e254-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1393\n",
      "Working on 1394\n",
      "Working on 1395\n",
      "Working on 1396\n",
      "Working on 1397\n",
      "Working on 1398\n",
      "Working on 1399\n",
      "Working on 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 25 Sep 2023 01:51:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80bf9f49ee1929e8-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1401\n",
      "Working on 1402\n",
      "Working on 1403\n",
      "Working on 1404\n",
      "Working on 1405\n",
      "Working on 1406\n",
      "Working on 1407\n",
      "Working on 1408\n",
      "Working on 1409\n",
      "Working on 1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 25 Sep 2023 01:52:02 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80bf9ff7dc6761e0-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1411\n",
      "Working on 1412\n",
      "Working on 1413\n",
      "Working on 1414\n",
      "Working on 1415\n",
      "Working on 1416\n",
      "Working on 1417\n",
      "Working on 1418\n",
      "Working on 1419\n",
      "Working on 1420\n",
      "Working on 1421\n",
      "Working on 1422\n",
      "Working on 1423\n",
      "Working on 1424\n",
      "Working on 1425\n",
      "Working on 1426\n",
      "Working on 1427\n",
      "Working on 1428\n",
      "Working on 1429\n",
      "Working on 1430\n",
      "Working on 1431\n",
      "Working on 1432\n",
      "Working on 1433\n",
      "Working on 1434\n",
      "Working on 1435\n",
      "Working on 1436\n",
      "Working on 1437\n",
      "Working on 1438\n",
      "Working on 1439\n",
      "Working on 1440\n",
      "Working on 1441\n",
      "Working on 1442\n",
      "Working on 1443\n",
      "Working onWorking on 1445\n",
      " 1444\n",
      "Working on 1446\n",
      "Working on 1447\n",
      "Working on 1448\n",
      "Working on 1449\n",
      "Working on 1450\n",
      "Working on 1451\n",
      "Working on 1452\n",
      "Working on 1453\n",
      "Working on 1454\n",
      "Working on 1455\n",
      "Working on 1456\n",
      "Working on 1457\n",
      "Working on 1458\n",
      "Working on 1459\n",
      "Working on 1460\n",
      "Working on 1461\n",
      "Working on 1462\n",
      "Working on 1463\n",
      "Working on 1464\n",
      "Working on 1465\n",
      "Working on 1466\n",
      "Working on 1467\n",
      "Working on 1468\n",
      "Working onWorking on 1470\n",
      " 1469\n",
      "Working on 1471\n",
      "Working on 1472\n",
      "Working on 1473\n",
      "Working on 1474\n",
      "Working on 1475\n",
      "Working on 1476\n",
      "Working on 1477\n",
      "Working on 1478\n",
      "Working on 1479\n",
      "Working on 1480\n",
      "Working on 1481\n",
      "Working on 1482\n",
      "Working on 1483\n",
      "Working on 1484\n",
      "Working on 1485\n",
      "Working on 1486\n",
      "Working on 1487\n",
      "Working on 1488\n",
      "Working on 1489\n",
      "Working on 1490\n",
      "Working on 1491\n",
      "Working on 1492\n",
      "Working onWorking on 1494\n",
      " 1493\n",
      "Working on 1495\n",
      "Working on 1496\n",
      "Working on 1497\n",
      "Working on 1498\n",
      "Working on 1499\n",
      "Working on 1500\n",
      "Working on 1501\n",
      "Working on 1502\n",
      "Working on 1503\n",
      "Working on 1504\n",
      "Working on 1505\n",
      "Working on 1506\n",
      "Working on 1507\n",
      "Working on 1508\n",
      "Working on 1509\n",
      "Working on 1510\n",
      "Working onWorking on 1512\n",
      "Working on 1513\n",
      "Working on 1514\n",
      " 1511\n",
      "Working on 1515\n",
      "Working on 1516\n",
      "Working on 1517\n",
      "Working on 1518\n",
      "Working on 1519\n",
      "Working on 1520\n",
      "Working on 1521\n",
      "Working on 1522\n",
      "Working on 1523\n",
      "Working on 1524\n",
      "Working on 1525\n",
      "Working on 1526\n",
      "Working on 1527\n",
      "Working on 1528\n",
      "Working on 1529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 25 Sep 2023 01:57:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '80bfa82a9c402c84-ORD', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 1530\n",
      "Working on 1531\n",
      "Working on 1532\n",
      "Working on 1533\n",
      "Working on 1534\n",
      "Working on 1535\n",
      "Working on 1536\n",
      "Working on 1537\n",
      "Working on 1538\n",
      "Working on 1539\n",
      "Working on 1540\n",
      "Working on 1541\n",
      "Working on 1542\n",
      "Working on 1543\n",
      "Working on 1544\n",
      "Working on 1545\n",
      "Working on 1546\n",
      "Working on 1547\n",
      "Working on 1548\n",
      "Working onWorking on 1550\n",
      " 1549\n",
      "Working on 1551\n",
      "Working on 1552\n",
      "Working on 1553\n",
      "Working on 1554\n",
      "Working on 1555\n",
      "Working on 1556\n",
      "Working on 1557\n",
      "Working on 1558\n",
      "Working on 1559\n",
      "Working on 1560\n",
      "Working onWorking on 1562\n",
      " 1561\n",
      "Working on 1563\n",
      "Working on 1564\n",
      "Working on 1565\n",
      "Working on 1566\n",
      "Working on 1567\n",
      "Working on 1568\n",
      "Working on 1569\n",
      "Working onWorking on 1571\n",
      " 1570\n",
      "Working on 1572\n",
      "Working on 1573\n",
      "Working on 1574\n",
      "Working on 1575\n",
      "Working on 1576\n",
      "Working on 1577\n",
      "Working on 1578\n",
      "Working on 1579\n",
      "Working on 1580\n",
      "Working on 1581\n",
      "Working on 1582\n",
      "Working on 1583\n",
      "Working on 1584\n",
      "Working on 1585\n",
      "Working on 1586\n",
      "Working on 1587\n",
      "Working on 1588\n",
      "Working on 1589\n",
      "Working on 1590\n",
      "Working on 1591\n",
      "Working on 1592\n",
      "Working on 1593\n",
      "Working on 1594\n",
      "Working on 1595\n",
      "Working on 1596\n",
      "Working on 1597\n",
      "Working on 1598\n",
      "Working on 1599\n",
      "Working on 1600\n",
      "Working on 1601\n",
      "Working on 1602\n",
      "Working on 1603\n",
      "Working on 1604\n",
      "Working on 1605\n",
      "Working on 1606\n",
      "Working on 1607\n",
      "Working on 1608\n",
      "Working on 1609\n",
      "Working on 1610\n",
      "Working on 1611\n",
      "Working on 1612\n",
      "Working on 1613\n",
      "Working on 1614\n",
      "Working on 1615\n",
      "Working on 1616\n",
      "Working on 1617\n",
      "Working on 1618\n",
      "Working on 1619\n",
      "Working on 1620\n",
      "Working on 1621\n",
      "Working on 1622\n",
      "Working on 1623\n",
      "Working on 1624\n",
      "Working on 1625\n",
      "Working on 1626\n",
      "Working on 1627\n",
      "Working on 1628\n",
      "Working on 1629\n",
      "Working on 1630\n",
      "Working on 1631\n",
      "Working on 1632\n",
      "Working on 1633\n",
      "Working on 1634\n",
      "Working on 1635\n",
      "Working on 1636\n",
      "Working on 1637\n",
      "Working on 1638\n",
      "Working on 1639\n",
      "Working on 1640\n",
      "Working on 1641\n",
      "Working on 1642\n",
      "Working on 1643\n",
      "Working on 1644\n",
      "Working on 1645\n",
      "Working on 1646\n",
      "Working on 1647\n",
      "Working on 1648\n",
      "Working on 1649\n",
      "Working on 1650\n",
      "Working on 1651\n",
      "Working on 1652\n",
      "Working on 1653\n",
      "Working on 1654\n",
      "Working on 1655\n",
      "Working on 1656\n",
      "Working on 1657\n",
      "Working on 1658\n",
      "Working on 1659\n",
      "Working on 1660\n",
      "Working on 1661\n",
      "Working on 1662\n",
      "Working on 1663\n",
      "Working on 1664\n",
      "Working on 1665\n",
      "Working on 1666\n",
      "Working on 1667\n",
      "Working on 1668\n",
      "Working on 1669\n",
      "Working on 1670\n",
      "Working on 1671\n",
      "Working on 1672\n",
      "Working on 1673\n",
      "Working on 1674\n",
      "Working on 1675\n",
      "Working on 1676\n",
      "Working on 1677\n",
      "Working on 1678\n",
      "Working on 1679\n",
      "Working on 1680\n",
      "Working on 1681\n",
      "Working on 1682\n",
      "Working onWorking on 1684\n",
      " 1683\n",
      "Working on 1685\n",
      "Working on 1686\n",
      "Working on 1687\n",
      "Working on 1688\n",
      "Working on 1689\n",
      "Working on 1690\n",
      "Working on 1691\n",
      "Working on 1692\n",
      "Working on 1693\n",
      "Working on 1694\n",
      "Working on 1695\n",
      "Working on 1696\n",
      "Working on 1697\n",
      "Working on 1698\n",
      "Working on 1699\n",
      "Working on 1700\n",
      "Working on 1701\n",
      "Working on 1702\n",
      "Working on 1703\n",
      "Working on 1704\n",
      "Working on 1705\n",
      "Working on 1706\n",
      "Working on 1707\n",
      "Working on 1708\n",
      "Working on 1709\n",
      "Working on 1710\n",
      "Working on 1711\n",
      "Working on 1712\n",
      "Working on 1713\n",
      "Working on 1714\n",
      "Working on 1715\n",
      "Working on 1716\n",
      "Working on 1717\n",
      "Working on 1718\n",
      "Working on 1719\n",
      "Working on 1720\n",
      "Working on 1721\n",
      "Working on 1722\n",
      "Working on 1723\n",
      "Working on 1724\n",
      "Working on 1725\n",
      "Working on 1726\n",
      "Working on 1727\n",
      "Working on 1728\n",
      "Working on 1729\n",
      "Working on 1730\n",
      "Working on 1731\n",
      "Working on 1732\n",
      "Working on 1733\n",
      "Working on 1734\n",
      "Working on 1735\n",
      "Working on 1736\n",
      "Working on 1737\n",
      "Working on 1738\n",
      "Working on 1739\n",
      "Working on 1740\n",
      "Working on 1741\n",
      "Working on 1742\n",
      "Working on 1743\n",
      "Working on 1744\n",
      "Working on 1745\n",
      "Working on 1746\n",
      "Working on 1747\n",
      "Working on 1748\n",
      "Working on 1749\n",
      "Working on 1750\n",
      "Working on 1751\n",
      "Working on 1752\n",
      "Working on 1753\n",
      "Working on 1754\n",
      "Working on 1755\n",
      "Working on 1756\n",
      "Working on 1757\n",
      "Working on 1758\n",
      "Working on 1759\n",
      "Working on 1760\n",
      "Working on 1761\n",
      "Working on 1762\n",
      "Working on 1763\n",
      "Working on 1764\n",
      "Working on 1765\n",
      "Working on 1766\n",
      "Working on 1767\n",
      "Working on 1768\n",
      "Working on 1769\n",
      "Working on 1770\n",
      "Working on 1771\n",
      "Working on 1772\n",
      "Working on 1773\n",
      "Working on 1774\n",
      "Working on 1775\n",
      "Working on 1776\n",
      "Working on 1777\n",
      "Working on 1778\n",
      "Working on 1779\n",
      "Working on 1780\n",
      "Working on 1781\n",
      "Working on 1782\n",
      "Working on 1783\n",
      "Working on 1784\n",
      "Working on 1785\n",
      "Working on 1786\n",
      "Working on 1787\n",
      "Working on 1788\n",
      "Working on 1789\n",
      "Working on 1790\n",
      "Working on 1791\n",
      "Working on 1792\n",
      "Working on 1793\n",
      "Working on 1794\n",
      "Working on 1795\n",
      "Working on 1796\n",
      "Working on 1797\n",
      "Working on 1798\n",
      "Working on 1799\n",
      "Working on 1800\n",
      "Working on 1801\n",
      "Working on 1802\n",
      "Working on 1803\n",
      "Working on 1804\n",
      "Working on 1805\n",
      "Working on 1806\n",
      "Working on 1807\n",
      "Working on 1808\n",
      "Working on 1809\n",
      "Working on 1810\n",
      "Working on 1811\n",
      "Working on 1812\n",
      "Working on 1813\n",
      "Working on 1814\n",
      "Working on 1815\n",
      "Working on 1816\n",
      "Working on 1817\n",
      "Working on 1818\n",
      "Working on 1819\n",
      "Working on 1820\n",
      "Working on 1821\n",
      "Working on 1822\n",
      "Working on 1823\n",
      "Working on 1824\n",
      "Working on 1825\n",
      "Working on 1826\n",
      "Working on 1827\n",
      "Working on 1828\n",
      "Working on 1829\n",
      "Working on 1830\n",
      "Working on 1831\n",
      "Working onWorking on 1833\n",
      " 1832\n",
      "Working on 1834\n",
      "Working on 1835\n",
      "Working on 1836\n",
      "Working on 1837\n",
      "Working on 1838\n",
      "Working on 1839\n",
      "Working on 1840\n",
      "Working on 1841\n",
      "Working on 1842\n",
      "Working on 1843\n",
      "Working on 1844\n",
      "Working on 1845\n",
      "Working on 1846\n",
      "Working on 1847\n",
      "Working on 1848\n",
      "Working on 1849\n",
      "Working on 1850\n",
      "Working on 1851\n",
      "Working on 1852\n",
      "Working on 1853\n",
      "Working on 1854\n",
      "Working on 1855\n",
      "Working on 1856\n",
      "Working on 1857\n",
      "Working on 1858\n",
      "Working on 1859\n",
      "Working on 1860\n",
      "Working on 1861\n",
      "Working on 1862\n",
      "Working on 1863\n",
      "Working on 1864\n",
      "Working on 1865\n",
      "Working on 1866\n",
      "Working on 13\n",
      "Working on 15\n",
      "Working on 14\n",
      "Working on 12\n",
      "Working on 10\n",
      "Working on 11\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# Define a function to process a group\n",
    "def process_group(x, groups, facts_db, model, q_a_enhance):\n",
    "    try:\n",
    "        print(\"Working on\", x)\n",
    "        group = groups[x]\n",
    "        q_a_string = \"\"\n",
    "        docs = get_at_index(group)\n",
    "        for x in docs:\n",
    "            q_a_string += \" \".join([\"Q:\", x.page_content.rstrip(\"\\n\"), \"\\n\" \"A:\", x.metadata['answer'], \"\\n\"])\n",
    "            q_a_string += \"\\n\"\n",
    "        facts_ = facts_db.similarity_search(docs[0].page_content, k=30)\n",
    "        facts_to_include = '\\n'.join([x.page_content for x in facts_])\n",
    "        prompt = q_a_enhance.format(q_a_string, facts_to_include)\n",
    "        #print(prompt)\n",
    "        \n",
    "        qs = model.predict_messages([HumanMessage(content=prompt)])\n",
    "        #print(qs.content)\n",
    "        return qs.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "new_qa_pairs = []\n",
    "remaining_groups = groups[900:]\n",
    "\n",
    "# Define the rate limit (in seconds per call)\n",
    "rate_limit = 1.0 / 100  # 100 calls per second\n",
    "\n",
    "# Define the maximum number of concurrent tasks\n",
    "max_concurrent_tasks = 10  # Adjust as needed\n",
    "\n",
    "# Create a bounded semaphore to limit concurrent tasks\n",
    "semaphore = threading.BoundedSemaphore(max_concurrent_tasks)\n",
    "\n",
    "def submit_task(x):\n",
    "    with semaphore:\n",
    "        return process_group(x, remaining_groups, facts_db, model, q_a_enhance)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the tasks to the thread pool\n",
    "    futures = [executor.submit(submit_task, x) for x in range(len(remaining_groups))]\n",
    "\n",
    "    # Collect the results as they become available\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            new_qa_pairs.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "2370a2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75905704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
