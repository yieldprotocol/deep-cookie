{"url": "https://docs.code4rena.com/awarding/judging-criteria", "md_content": "[Code4rena](/)\n\nSearch\n\n\u2303K\n\n[Code4rena](/)\n\nSearch\n\n\u2303K\n\n[Code4rena](/)\n\nRoles\n\n[Wardens](/roles/wardens)\n\n[Sponsors](/roles/sponsors)\n\n[Judges](/roles/judges)\n\n[Certified contributors](/roles/certified-contributors)\n\nAwarding\n\n[Incentive model and awards](/awarding/incentive-model-and-awards)\n\n[Judging criteria](/awarding/judging-criteria)\n\n[Severity Categorization](/awarding/judging-criteria/severity-categorization)\n\n[Fairness and validity](/awarding/fairness-and-validity)\n\nPhilosophy\n\n[Security is about people](/philosophy/security-is-about-people)\n\n[The culture we're building](/philosophy/how-we-work)\n\n[Intentionally structured](/philosophy/intentionally-structured)\n\nOther Details\n\n[FAQ](/structure/frequently-asked-questions)\n\n[Audit timeline](/structure/our-process)\n\n[Where can I find\u2026?](/structure/where-can-i-find...)\n\n[Powered By\nGitBook](https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=-MYGYvqTD29_fAaod9NJ)\n\n# Judging criteria\n\n###\n\nSubmission Review Process\n\nC4 strives to ensure a deliberate and transparent process for reviewing and\njudging submissions.\n\nAt the end of a given audit period, all reports will be reviewed and\ncategorized based on these criteria. Pending sponsor review, final reports\nwill be shared publicly on the [C4 Audit Report\npage](https://code4rena.com/reports). Audit results are shared on the C4\nDiscord and winners announced on the [C4\nTwitter](https://twitter.com/code423n4).\n\nReports are also judged based on grammar, conciseness, and formatting.\n\n###\n\nBest Current Practices\n\nThe [Code4rena org repo](https://github.com/code-423n4/org) documents open\ndiscussions and emergent best practices for judging C4 audits. Judges are\nencouraged to review [open issues in that\nrepo](https://github.com/code-423n4/org/issues) regularly.\n\n###\n\nDuplicate Submissions\n\nShould multiple submissions describing the same vulnerability be submitted,\nJudges have the discretion to place these bugs into the same bucket, in which\ncase, the award will be shared among those who submitted. However, multiple\nsubmissions from the same warden (or warden team), are treated as one by the\nawarding algorithm and do not split the pie into smaller pieces.\n\n###\n\nScope\n\nEach audit may include code that is explicitly in scope and out of scope, and\nspecific issues which also may be identified as out of scope.\n\nWardens who adhere to the audit guidelines and report valid low/medium/high\nseverity bugs which are not explicitly excluded from scope will earn a\nguaranteed payment.\n\nWardens _may_ elect to argue to bring things into scope--either by making the\ncase that an issue poses a more urgent threat than identified or by submitting\na medium or high severity finding in code which is out of scope. However, it\nis up to judges' absolute discretion whether to include these findings and\naward them, and these issues should include a clear argument as to why the\nitems merit being brought into scope.\n\nIn the interest of everyone's time, **please do not offer QA or gas reports on\nany code or known issues which are identified as out of scope.**\n\n###\n\nScoring\n\nThe scoring system has three primary goals:\n\n  * Rewarding Wardens for finding unique bugs\n\n  * Hardening C4 code audits to Sybil attacks\n\n  * Encouraging coordination by incentivizing Wardens to form teams.\n\n###\n\nAnalysis\n\nAn analysis is a written submission outlining:\n\n  * Wardens' analysis of the codebase as a whole and any observations or advice they have about architecture, mechanism, or approach\n\n  * Broader concerns like systemic risks or centralization risks\n\n  * The approach taken in reviewing the code\n\n  * New insights and learnings from the audit\n\nIf individual findings are trees, Analyses are the forest. They provide\nwardens with an opportunity to contribute value through high level insights\nand advice that aren't necessarily covered by specific bugs -- and a way to\nget credit for doing so.\n\nAnalyses are judged A/B/C, with the top Analysis selected for inclusion in the\naudit report, similarly to Gas and QA reports.\n\n###\n\nQA reports (low/non-critical)\n\nLow and non-critical findings must be submitted as a _single_ QA report per\nwarden. We allocate a **fixed 2.5% of prize pools toward QA reports.**\n\nYour QA report should include:\n\n  * all low severity findings; and\n\n  * all non-critical findings.\n\nEach QA report will be assessed based on report quality and thoroughness as\ncompared with other reports, with awards distributed on a curve. The top QA\nreport author will receive the top prize from the category.\n\nWardens overstating the severity of QA issues (submitting low/non-critical\nissues as med/high in order to angle for higher payouts) will have their\nscores reduced by judges.\n\nIn the unlikely event that zero high- or medium-risk vulnerabilities are\nfound, the full pool will be divided based on the QA Report curve.\n\n###\n\nGas reports\n\nGas reports should be submitted using the **same approach as the QA reports:**\na single submission per warden which includes all identified optimizations.\nThe gas pool will be allocated on a curve, and the top reporter will receive\nthe top prize in the category.\n\nThe gas pool varies from audit to audit, but typically it consists of 2.5% of\nthe total prize pool. The precise gas pool for each audit can be found in that\naudit's repo.\n\n##\n\nEstimating Risk\n\nSee [Severity Categorization](https://docs.code4rena.com/awarding/judging-\ncriteria/severity-categorization).\n\n[PreviousFAQ about QA and Gas Reports](/awarding/incentive-model-and-\nawards/qa-gas-report-faq)[NextSeverity Categorization](/awarding/judging-\ncriteria/severity-categorization)\n\nLast modified 10d ago\n\nOn this page\n\nSubmission Review Process\n\nBest Current Practices\n\nDuplicate Submissions\n\nScope\n\nScoring\n\nAnalysis\n\nQA reports (low/non-critical)\n\nGas reports\n\nEstimating Risk\n\n"}