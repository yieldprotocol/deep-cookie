{"url": "https://docs.code4rena.com/awarding/incentive-model-and-awards/qa-gas-report-faq", "md_content": "[Code4rena](/)\n\nSearch\n\n\u2303K\n\n[Code4rena](/)\n\nSearch\n\n\u2303K\n\n[Code4rena](/)\n\nRoles\n\n[Wardens](/roles/wardens)\n\n[Sponsors](/roles/sponsors)\n\n[Judges](/roles/judges)\n\n[Certified contributors](/roles/certified-contributors)\n\nAwarding\n\n[Incentive model and awards](/awarding/incentive-model-and-awards)\n\n[Awarding process](/awarding/incentive-model-and-awards/awarding-process)\n\n[Curve logic for QA and Gas optimization reports](/awarding/incentive-model-\nand-awards/curve-logic)\n\n[FAQ about QA and Gas Reports](/awarding/incentive-model-and-awards/qa-gas-\nreport-faq)\n\n[Judging criteria](/awarding/judging-criteria)\n\n[Fairness and validity](/awarding/fairness-and-validity)\n\nPhilosophy\n\n[Security is about people](/philosophy/security-is-about-people)\n\n[The culture we're building](/philosophy/how-we-work)\n\n[Intentionally structured](/philosophy/intentionally-structured)\n\nOther Details\n\n[FAQ](/structure/frequently-asked-questions)\n\n[Audit timeline](/structure/our-process)\n\n[Where can I find\u2026?](/structure/where-can-i-find...)\n\n[Powered By\nGitBook](https://www.gitbook.com/?utm_source=content&utm_medium=trademark&utm_campaign=-MYGYvqTD29_fAaod9NJ)\n\n# FAQ about QA and Gas Reports\n\nThis FAQ pertains to the award mechanism update that takes effect February 3,\n2022, which changes the submission guidelines for low-risk, non-critical, and\ngas optimization reports. For more details, see [Judging\nCriteria](https://docs.code4rena.com/roles/wardens/judging-criteria).\n\n###\n\nWhat happens to the award pool if no Med/High vulns are found?\n\nThe full pool would then be divided based on the QA Report curve.\n\n###\n\nWill non-critical findings hold some weight? Just want to know if it's worth\nspending a considerable amount of time writing this part of the report.\n\nThe full QA report will be graded on a curve against the other reports. We'll\nbe experimenting together as a community with this, but we think we'll learn a\nlot and it will be interesting to see the best practices emerge.\n\nWe are intentionally not providing an \"example,\" as we are eager to see what\napproaches folks take and to be able to learn from a variety of approaches.\n\n###\n\nWhat if a low-impact QA report turns out to be a high-impact report? How does\nthat work with the 10% prize pool? Would the report be upgraded?\n\nIt's conceivable it could be upgraded, though it's important to consider that\npart of auditing is demonstrating proper theory of how an issue could be\nexploited. If a warden notices something is \"off\" but is unable to articulate\nwhy it could lead to loss of funds, for example, the job is only half-done;\nwithout understanding the implications, a developer could very well overlook\nor deprioritize the issue.\n\nThe tl;dr for determining severity is relatively clear with regard to\nseparating by impact.\n\n###\n\nWhat happens when an issue submitted by the warden as part of their QA report\n(an L or N) _DOES_ get bumped up to Med/High by the judge after review?\n\nIf it seemed appropriate to do so based on a judge's assessment of the issue,\nthey could certainly choose to do this.\n\nThe judge could create a new separate Github issue in the findings repo that\ncontains the relevant portions of the warden's QA report, and add that to the\nrespective H or M level bucket.\n\nHowever, QA items may be marked as a duplicate of another finding _without_\nbeing granted an upgrade, since making the case for _how_ an issue can be\nexploited, and providing a thorough description and proof of concept, is part\nof what merits a finding properly earning medium or high severity.\n\n###\n\nConversely, in the reverse situation where an issue submitted by wardens as\nH/M level, is subsequently downgraded to QA level by the judge during their\nreview, would the penalty just be excluding the overrated warden submission\nfrom consideration in regards to the QA rewards?\n\nWe'll need to see how it works in reality, but our current assumption is that\n(a) low severity findings attempted to get pushed into med/high would\nessentially get zero (just logically so since they wouldn't be high or med),\nand then (b) their QA report would be lower quality as a result, and so they\nwouldn't score as highly as they could have. Judges could also decide to mark\noff points in someone's QA report if they saw behavior that seemed like it\nmight be trying to game for higher rewards by inflating severity, so it could\nhave a negative consequence as well.\n\n[PreviousCurve logic for QA and Gas optimization reports](/awarding/incentive-\nmodel-and-awards/curve-logic)[Next \\- AwardingJudging\ncriteria](/awarding/judging-criteria)\n\nLast modified 1yr ago\n\nOn this page\n\nWhat happens to the award pool if no Med/High vulns are found?\n\nWill non-critical findings hold some weight? Just want to know if it's worth\nspending a considerable amount of time writing this part of the report.\n\nWhat if a low-impact QA report turns out to be a high-impact report? How does\nthat work with the 10% prize pool? Would the report be upgraded?\n\nWhat happens when an issue submitted by the warden as part of their QA report\n(an L or N) DOES get bumped up to Med/High by the judge after review?\n\nConversely, in the reverse situation where an issue submitted by wardens as\nH/M level, is subsequently downgraded to QA level by the judge during their\nreview, would the penalty just be excluding the overrated warden submission\nfrom consideration in regards to the QA rewards?\n\n"}