{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeArena (C4) Question Answer bot\n",
    "\n",
    "### Objective\n",
    "- This notebook has the PoC work for a Question Answer bot using C4's knowledge bases.\n",
    "- The objective of the PoC is to prototype an LLM implementation that can accurately answer questions to their expectation and at the very least perform better than their current bot from [Mava](https://www.mava.app/)\n",
    "\n",
    "### Observations from the usage of Mava\n",
    "- The platform offers Discord support management with ticketing and AI help bot features\n",
    "- For the AI help bot, the user is able to specify links to multiple knowledge sources that can be used for answering questions.\n",
    "- Based on C4's testing of the Mava bot in the private channel, the following stats were observed:-\n",
    "    - Total questions asked: 29\n",
    "    - Total questions mis-answered based on emoji reactions: 13\n",
    "    - Accuracy - ~55%\n",
    "\n",
    "### Knowledge Bases\n",
    "Based on conversations with their team, the following knowledge bases were identified to be relevant and are the same ones that Mava is using:-\n",
    "- [Main Website](https://code4rena.com/)\n",
    "- [Docs](https://docs.code4rena.com/) \n",
    "\n",
    "\n",
    "### High-level Approach\n",
    "- Crawl and scrape C4â€™s website and docs using Scrapy lib\n",
    "- Convert the html content to markdown format so that the model can better understand the context\n",
    "- Use LangChain lib to do the following:-\n",
    "    - Split the markdown header-separated sections into semantic chunks\n",
    "    - Embed and store the semantic chunks in an in-memory vector db\n",
    "    - Use the retrieval augmented functionality to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the third-party packages\n",
    "\n",
    "!pip install 'langchain[llms]'\n",
    "!pip install Scrapy\n",
    "!pip install html2text\n",
    "!pip install lxml\n",
    "!pip install python-dotenv\n",
    "!pip install \"unstructured[all-docs]\"\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu \n",
    "!pip install GitPython\n",
    "!pip install notebook\n",
    "!pip install chromadb\n",
    "!pip install pandas\n",
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General setup - you can specify OPENAI_API_KEY in .env file\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') or getpass.getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "assert OPENAI_API_KEY, \"Please set OPENAI_API_KEY in your environment variables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "C4_WEBSITE_STORAGE_DIR = \"knowledge_base/c4/website\"\n",
    "C4_DOCS_STORAGE_DIR = \"knowledge_base/c4/docs\"\n",
    "C4_GH_DOCS_STORAGE_DIR = \"knowledge_base/c4/gh_docs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawling and Scraping using Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scrapy\n",
    "import html2text\n",
    "import lxml.html\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class GenericSpider(scrapy.Spider):\n",
    "    name = 'generic'\n",
    "\n",
    "    def __init__(self, domain='', storage_dir='.', *args, **kwargs):\n",
    "        super(GenericSpider, self).__init__(*args, **kwargs)\n",
    "        self.allowed_domains = [domain]\n",
    "        self.start_urls = [f'http://{domain}/']\n",
    "        self.storage_dir = storage_dir\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Remove unwanted elements using lxml\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        \n",
    "        # Remove non-text related tags\n",
    "        for unwanted in tree.xpath('//script|//img|//video|//audio|//iframe|//object|//embed|//canvas|//svg|//link|//source|//track|//map|//area'):\n",
    "            unwanted.drop_tree()\n",
    "\n",
    "        cleaned_html = lxml.html.tostring(tree).decode('utf-8')\n",
    "\n",
    "        # Convert HTML to Markdown\n",
    "        converter = html2text.HTML2Text()\n",
    "        markdown_text = converter.handle(cleaned_html)\n",
    "\n",
    "        # Save to a markdown file in the specified directory\n",
    "        if not os.path.exists(self.storage_dir):\n",
    "            os.makedirs(self.storage_dir)\n",
    "\n",
    "        url = response.url\n",
    "        page_name = response.url.split(\"/\")[-1] if response.url.split(\"/\")[-1] else \"index\"\n",
    "\n",
    "        filename = os.path.join(self.storage_dir, f'{page_name}.json')\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            # Store the URL and markdown text in JSON format\n",
    "            json.dump({'url': url, 'md_content': markdown_text}, f)\n",
    "\n",
    "        # Recursively follow relative links to other pages on the same domain\n",
    "        for href in response.css('a::attr(href)').getall():\n",
    "            url = response.urljoin(href)\n",
    "            if urlparse(url).netloc in self.allowed_domains:\n",
    "                yield scrapy.Request(url, self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Data has already been scraped and saved locally as JSON files in the 'knowledge_base/c4' directory. To re-run the scraping, uncomment the code in the cell below.\n",
    "\n",
    "On re-running the crawler, if you get 'ReactorNotRestartable' error, the notebook kernel would need to be restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scrapy.crawler import CrawlerRunner\n",
    "# from scrapy.utils.project import get_project_settings\n",
    "# from twisted.internet import reactor\n",
    "\n",
    "# settings = get_project_settings()\n",
    "\n",
    "# runner = CrawlerRunner(settings)\n",
    "# runner.crawl(GenericSpider, domain=\"code4rena.com\", storage_dir=C4_WEBSITE_STORAGE_DIR)\n",
    "# runner.crawl(GenericSpider, domain=\"docs.code4rena.com\", storage_dir=C4_DOCS_STORAGE_DIR)\n",
    "# d = runner.join()\n",
    "# d.addBoth(lambda _: reactor.stop())\n",
    "# reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get docs from Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from git import Repo\n",
    "\n",
    "# repo = Repo.clone_from(\n",
    "#     \"https://github.com/code-423n4/docs\", to_path=C4_GH_DOCS_STORAGE_DIR\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load locally saved scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_json_files(dir):\n",
    "    loader = DirectoryLoader(dir, loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    for d in documents:\n",
    "        page_content_dict = json.loads(d.page_content)\n",
    "        d.page_content = page_content_dict['md_content']\n",
    "        d.metadata['url'] = page_content_dict['url']\n",
    "    return documents\n",
    "\n",
    "c4_website_data_list = load_json_files(C4_WEBSITE_STORAGE_DIR)\n",
    "c4_docs_data_list = load_json_files(C4_DOCS_STORAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader(C4_GH_DOCS_STORAGE_DIR, loader_cls=TextLoader)\n",
    "c4_gh_docs_data_list = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the markdown content into semantic chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "\n",
    "website_chunks =  md_splitter.split_documents(c4_website_data_list)\n",
    "docs_chunks =  md_splitter.split_documents(c4_docs_data_list)\n",
    "gh_docs_chunks = md_splitter.split_documents(c4_gh_docs_data_list)\n",
    "\n",
    "print(len(website_chunks))\n",
    "print(len(docs_chunks))\n",
    "print(len(gh_docs_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embed the semantic chunks and store in an in-memory vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# NOTE: At times, OpenAI Embedding service can fail intermittently and return errorneous values such as [NaN], more info: https://github.com/langchain-ai/langchain/pull/7070\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "Chroma(\"vectorstore\").delete_collection()\n",
    "vectorstore = Chroma(\"vectorstore\", embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "vectorstore.add_documents(website_chunks)\n",
    "#vectorstore.add_documents(docs_chunks)\n",
    "vectorstore.add_documents(gh_docs_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation\n",
    "Workflow \n",
    "1. Use faster LLM (GPT-3.5) to generate 3 rephrased variants of the original user question to improve question quality which in-turn should improve retrieval\n",
    "2. Use the rephrased question to generate the final answer using RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate rephrased questions\n",
    "Use faster LLM (GPT-3.5) to generate 3 rephrased variants of the original user question to improve question quality which in-turn should improve retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"You are a teacher who is helping a student ask the right questions about a service so that they can look in the most relevant places to find the answer. \n",
    "# INSTRUCTIONS\n",
    "- You are given student's question below\n",
    "- Using the original question, generate 3 alternative questions that are rephrased to be not vague or ambiguous so as to clearly convey the same meaning and context as the original question\n",
    "- Return the final result as a JSON object containing a list of rephrased questions as \"new_questions\" field\n",
    "\n",
    "# QUESTION\n",
    "{question}\n",
    "\n",
    "# RESULT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_rephrased_questions(question):\n",
    "    chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "    llm_chain = LLMChain(llm=chat, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "    result = llm_chain(inputs={\"question\": question}, return_only_outputs=True)\n",
    "    result_dict = json.loads(result['text'])\n",
    "    new_questions = result_dict['new_questions']\n",
    "    return new_questions\n",
    "\n",
    "generate_rephrased_questions(\"What are scout awards?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate final answer using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result(question, result):\n",
    "    display(Markdown(f\"### Question\"))\n",
    "    display(Markdown(\"ORIGINAL: \" + question))\n",
    "    display(Markdown(\"REPHRASED: \" + f\"{result['rephrased_question'] if result['rephrased_question'] else 'None'}\"))\n",
    "\n",
    "    display(Markdown(f\"### Answer\"))\n",
    "    display(Markdown(result[\"result\"]))\n",
    "\n",
    "    display(Markdown(f\"### Sources\"))\n",
    "    sources = [r.metadata['url'] if 'url' in r.metadata else r.metadata['source'] for r in result[\"source_documents\"] ]\n",
    "    print(\", \".join(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0), chain_type=\"stuff\", retriever=vectorstore.as_retriever(), return_source_documents=True)\n",
    "\n",
    "\n",
    "def call_llm(question, use_rephrased_questions=True):\n",
    "    if not use_rephrased_questions:\n",
    "        result = qa({\"query\": question})\n",
    "        result['rephrased_question'] = None\n",
    "        return result\n",
    "\n",
    "\n",
    "    # Get rephrased questions\n",
    "    rephrased_questions = generate_rephrased_questions(question)\n",
    "\n",
    "    # Attempt each question until a valid result is found\n",
    "    for q in rephrased_questions:\n",
    "        result = qa({\"query\": q})\n",
    "        answer = result['result']\n",
    "        result['rephrased_question'] = None\n",
    "        \n",
    "        # If the model is unable to find an answer, it returns 'sorry' in the response, we try again with a different question\n",
    "        if 'sorry' in answer.lower():\n",
    "            continue\n",
    "        else:\n",
    "            result['rephrased_question'] = q\n",
    "            break\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEvaluator\n",
    "Using LangChain's [AutoEvaluator technique](https://autoevaluator.langchain.com/) to evaluate the bot's performance on the dataset of C4 questions correctly answered by Mava as per team feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# load yaml file\n",
    "with open('knowledge_base/c4/c4_mava_correct_ans_set.yaml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    yaml_data = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "mava_questions = [d['question'] for d in yaml_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\" \n",
    "    You are a grader trying to determine if a set of retrieved documents will help a student answer a question. \\n\n",
    "\n",
    "    Here is the question: \\n\n",
    "    {query}\n",
    "\n",
    "    Here are the documents retrieved to answer question: \\n\n",
    "    {result}\n",
    "    \n",
    "    Here is the correct answer to the question: \\n \n",
    "    {answer}\n",
    "   \n",
    "    Criteria: \n",
    "      relevance: Do all of the documents contain information that will help the student arrive that the correct answer to the question?\"\n",
    "\n",
    "    Your response should be as follows:\n",
    "\n",
    "    GRADE: (Correct or Incorrect, depending if all of the documents retrieved meet the criterion)\n",
    "    (line break)\n",
    "    JUSTIFICATION: (Write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Use three sentences maximum. Keep the answer as concise as possible.)\n",
    "    \"\"\"\n",
    "\n",
    "GRADE_DOCS_PROMPT = PromptTemplate(input_variables=['result', 'answer', 'query'], template=template)\n",
    "\n",
    "template = \"\"\"You are a teacher grading a quiz. \n",
    "You are given a question, the student's answer, and the true answer, and are asked to score the student answer as either Correct or Incorrect.\n",
    "\n",
    "Example Format:\n",
    "QUESTION: question here\n",
    "STUDENT ANSWER: student's answer here\n",
    "TRUE ANSWER: true answer here\n",
    "GRADE: Correct or Incorrect here\n",
    "\n",
    "Grade the student answers based ONLY on their factual accuracy. Ignore differences in punctuation and phrasing between the student answer and true answer. It is OK if the student answer contains more information than the true answer, as long as it does not contain any conflicting statements. If the student answers that there is no specific information provided in the context, then the answer is Incorrect. Begin! \n",
    "\n",
    "QUESTION: {query}\n",
    "STUDENT ANSWER: {result}\n",
    "TRUE ANSWER: {answer}\n",
    "GRADE:\n",
    "\n",
    "Your response should be as follows:\n",
    "\n",
    "GRADE: (Correct or Incorrect)\n",
    "(line break)\n",
    "JUSTIFICATION: (Without mentioning the student/teacher framing of this prompt, explain why the STUDENT ANSWER is Correct or Incorrect. Use one or two sentences maximum. Keep the answer as concise as possible.)\n",
    "\"\"\"\n",
    "\n",
    "GRADE_ANSWER_PROMPT = PromptTemplate(input_variables=[\"query\", \"result\", \"answer\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "def grade_model_answer(predicted_dataset, predictions):\n",
    "\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "        prompt=GRADE_ANSWER_PROMPT\n",
    "    )\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        predicted_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "\n",
    "    return graded_outputs\n",
    "\n",
    "\n",
    "def grade_model_retrieval(gt_dataset, predictions):\n",
    "    # Create an evaluation chain\n",
    "    eval_chain = QAEvalChain.from_llm(\n",
    "        llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0),\n",
    "        prompt=GRADE_DOCS_PROMPT\n",
    "    )\n",
    "\n",
    "    # Evaluate the predictions and ground truth using the evaluation chain\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        gt_dataset,\n",
    "        predictions,\n",
    "        question_key=\"question\",\n",
    "        prediction_key=\"result\"\n",
    "    )\n",
    "    return graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_answers = []\n",
    "source_docs = []\n",
    "for d in yaml_data:\n",
    "    result = call_llm(d['question'])\n",
    "    bot_answers.append(result['result'])\n",
    "    source_docs.append(result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [{'result': a} for a in bot_answers]\n",
    "\n",
    "answer_grades = grade_model_answer(yaml_data, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = []\n",
    "for i, d in enumerate(yaml_data):\n",
    "    retrieved_doc_text = \"\"\n",
    "    for j, doc in enumerate(source_docs[i]):\n",
    "        retrieved_doc_text += \"Doc %s: \" % str(j + 1) + doc.page_content + \" \"\n",
    "    retrieved = {\"question\": d[\"question\"], \"answer\": d[\"answer\"], \"result\": retrieved_doc_text}\n",
    "    retrieved_docs.append(retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_grades = grade_model_retrieval(yaml_data, retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"question\": [d['question'] for d in yaml_data],\n",
    "    \"Mava correct answer (True value)\": [d['answer'] for d in yaml_data],\n",
    "    \"Bot answers\": [p['result'] for p in predictions],\n",
    "    \"Retrieval relevancy score\": ['Incorrect' if 'Incorrect' in g['results'] else 'Correct' for g in retrieval_grades],\n",
    "    \"Answer similarity score\": ['Incorrect' if 'Incorrect' in g['results'] else 'Correct' for g in answer_grades]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE technique\n",
    "This technique can help improve information retrieval\n",
    "\n",
    "https://python.langchain.com/docs/use_cases/question_answering/how_to/hyde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"vectorstore_hyde\"\n",
    "Chroma(collection_name).delete_collection()\n",
    "\n",
    "vectorstore_hyde = Chroma(collection_name, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "vectorstore_hyde.add_documents(website_chunks)\n",
    "vectorstore_hyde.add_documents(gh_docs_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForRetrieverRun,\n",
    "    CallbackManagerForRetrieverRun,\n",
    ")\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List\n",
    "\n",
    "class HydeRetriever(VectorStoreRetriever):\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "        web_search_template = \"\"\"Please write a passage to answer the question \n",
    "        Question: {QUESTION}\n",
    "        Passage:\"\"\"\n",
    "\n",
    "        web_search = PromptTemplate(template=web_search_template, input_variables=[\"QUESTION\"])\n",
    "\n",
    "        llm_chain = LLMChain(llm=llm, prompt=web_search)\n",
    "\n",
    "        result = llm_chain(inputs={\"QUESTION\": query}, return_only_outputs=True)\n",
    "        hyquery = result['text']\n",
    "\n",
    "        return super()._get_relevant_documents(hyquery, run_manager=run_manager)\n",
    "\n",
    "\n",
    "hyde_retriever = HydeRetriever(vectorstore=vectorstore_hyde)\n",
    "\n",
    "hyde_retriever.get_relevant_documents(\"How can I access findings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-4\", temperature=0), chain_type=\"stuff\", retriever=hyde_retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "def call_hyde_llm(question):\n",
    "    result = qa({\"query\": question})\n",
    "    result['rephrased_question'] = None\n",
    "    return result\n",
    "\n",
    "def ask_hyde(question):\n",
    "    result = call_hyde_llm(question)\n",
    "    display_result(question, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Store with Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# NOTE: At times, OpenAI Embedding service can fail intermittently and return errorneous values such as [NaN], more info: https://github.com/langchain-ai/langchain/pull/7070\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"vectorstore_with_sources\"\n",
    "Chroma(collection_name).delete_collection()\n",
    "vectorstore_with_sources = Chroma(collection_name, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "for i, d in enumerate(website_chunks):\n",
    "    dd = d.copy()\n",
    "    dd.metadata['source'] = f\"w{i}-pl\"\n",
    "    vectorstore_with_sources.add_documents([dd])\n",
    "\n",
    "for i, d in enumerate(gh_docs_chunks):\n",
    "    dd = d.copy()\n",
    "    local_path = dd.metadata['source']\n",
    "    dd.metadata['source'] = f\"g{i}-pl\"\n",
    "    dd.metadata['url'] = f\"{local_path.replace(C4_GH_DOCS_STORAGE_DIR, 'https://github.com/code-423n4/docs/blob/main/')}\"\n",
    "    vectorstore_with_sources.add_documents([dd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiQuery approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "multiquery_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore_with_sources.as_retriever(), llm=llm\n",
    ")\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "lowercased_website_chunks = []\n",
    "for d in website_chunks:\n",
    "    dd = d.copy()\n",
    "    dd.page_content = d.page_content.lower()\n",
    "    lowercased_website_chunks.append(dd)\n",
    "\n",
    "\n",
    "lowercased_gh_docs_chunks = []\n",
    "for d in gh_docs_chunks:\n",
    "    dd = d.copy()\n",
    "    dd.page_content = d.page_content.lower()\n",
    "    lowercased_gh_docs_chunks.append(dd)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(lowercased_website_chunks + lowercased_gh_docs_chunks)\n",
    "bm25_retriever.k = 2\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, multiquery_retriever], weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(model, chain_type=\"stuff\", retriever=ensemble_retriever, return_source_documents=True)\n",
    "\n",
    "\n",
    "def run_qa_with_sources(question):\n",
    "    \n",
    "    # Santize the question by removing any trailing question marks\n",
    "    sanitized_question = question.rstrip(\"?\")\n",
    "\n",
    "    result = qa_with_sources({\"question\": sanitized_question}, return_only_outputs=True)\n",
    "\n",
    "    answer = result['answer']\n",
    "    source_ids = result['sources']\n",
    "    source_docs = result['source_documents']\n",
    "\n",
    "    source_urls = set()\n",
    "    for d in source_docs:\n",
    "        metadata = d.metadata\n",
    "        source_id = metadata['source']\n",
    "        url = metadata['url']\n",
    "        if source_id in source_ids:\n",
    "            source_urls.add(url)\n",
    "    return dict(answer=answer, source_urls=source_urls, source_docs=source_docs)\n",
    "\n",
    "def ask(question):\n",
    "    result = run_qa_with_sources(question)\n",
    "\n",
    "    display(Markdown(f\"### Question\"))\n",
    "    display(Markdown(\"ORIGINAL: \" + question))\n",
    "\n",
    "    display(Markdown(f\"### Answer\"))\n",
    "    display(Markdown(result[\"answer\"]))\n",
    "\n",
    "    display(Markdown(f\"### Sources\"))\n",
    "    print(\", \".join(result['source_urls']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_eval():\n",
    "    bot_answers = []\n",
    "    source_docs = []\n",
    "    for d in yaml_data:\n",
    "        result = run_qa_with_sources(d['question'])\n",
    "        bot_answers.append(result['answer'])\n",
    "        source_docs.append(result['source_docs'])\n",
    "    \n",
    "    predictions = [{'result': a} for a in bot_answers]\n",
    "\n",
    "    answer_grades = grade_model_answer(yaml_data, predictions)\n",
    "\n",
    "    retrieved_docs = []\n",
    "    for i, d in enumerate(yaml_data):\n",
    "        retrieved_doc_text = \"\"\n",
    "        for j, doc in enumerate(source_docs[i]):\n",
    "            retrieved_doc_text += \"Doc %s: \" % str(j + 1) + doc.page_content + \" \"\n",
    "        retrieved = {\"question\": d[\"question\"], \"answer\": d[\"answer\"], \"result\": retrieved_doc_text}\n",
    "        retrieved_docs.append(retrieved)\n",
    "\n",
    "    retrieval_grades = grade_model_retrieval(yaml_data, retrieved_docs)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"question\": [d['question'] for d in yaml_data],\n",
    "        \"Mava correct answer (True value)\": [d['answer'] for d in yaml_data],\n",
    "        \"Bot answers\": [p['result'] for p in predictions],\n",
    "        \"Retrieval relevancy score\": ['Incorrect' if 'Incorrect' in g['results'] else 'Correct' for g in retrieval_grades],\n",
    "        \"Answer similarity score\": ['Incorrect' if 'Incorrect' in g['results'] else 'Correct' for g in answer_grades]\n",
    "    })\n",
    "    print(f\"Bot Accuracy: {df['Answer similarity score'].value_counts()['Correct'] / len(df['Answer similarity score'])}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions that were answered incorrectly by the Mava bot as per emoji reaction in the test channel\n",
    "MAVA_MISANSWERED_QUES = [\n",
    "    \"Am I allowed to use AI in an audit?\",\n",
    "    \"Can I change my Code4rena username?\",\n",
    "    \"How do I book a solo audit?\",\n",
    "    \"Do I need to be certified to participate in an audit?\",\n",
    "    \"How do bot races work?\",\n",
    "    \"Can I change my Code4rena profile name?\",\n",
    "    \"What are scout awards?\",\n",
    "    \"What are analysis reports?\",\n",
    "    \"what is an analysis finding?\",\n",
    "    \"My name wasn't in the award announcements. When can I check on my results?\",\n",
    "    \"How long does the certification process take?\",\n",
    "    \"How can I access findings.csv?\",\n",
    "    \"Can I use chatgpt?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ques = [d['question'] for d in yaml_data]\n",
    "eval_set = labeled_ques + MAVA_MISANSWERED_QUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "for q in eval_set:\n",
    "    result = run_qa_with_sources(q)\n",
    "    eval_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"question\": [q for q in eval_set],\n",
    "    \"Bot answers\": [r['answer'] for r in eval_results],\n",
    "    \"Sources\": [ \", \".join(r['source_urls']) for r in eval_results],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"./outputs/eval_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"My wallet was hacked. What do I do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c4-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
